Self-Play Preference Optimization for Language Model
Alignment
Yue Wu∗†Zhiqing Sun∗‡Huizhuo Yuan∗§Kaixuan Ji¶Yiming Yang‖Quanquan Gu∗∗
Abstract
Traditional reinforcement learning from human feedback (RLHF) approaches relying on
parametric models like the Bradley-Terry model fall short in capturing the intransitivity and
irrationality in human preferences. Recent advancements suggest that directly working with
preference probabilities can yield a more accurate reflection of human preferences, enabling
more flexible and accurate language model alignment. In this paper, we propose a self-play-
based method for language model alignment, which treats the problem as a constant-sum
two-player game aimed at identifying the Nash equilibrium policy. Our approach, dubbed
Self-Play Preference Optimization (SPPO), approximates the Nash equilibrium through iterative
policy updates and enjoys theoretical convergence guarantee. Our method can effectively increase
the log-likelihood of the chosen response and decrease that of the rejected response, which cannot
be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO)
and Identity Preference Optimization (IPO). In our experiments, using only 60k prompts (without
responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging
a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model
from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled
win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative)
DPO and IPO on MT-Bench and the Open LLM Leaderboard. Notably, the strong performance
of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.)
from GPT-4 or other stronger language models.
1 Introduction
Large Language Models (LLMs) (e.g., Ouyang et al., 2022; OpenAI et al., 2023), have shown
remarkable capabilities in producing human-like text, fielding questions, and coding. Despite
∗Equal contribution
†Department of Computer Science, University of California, Los Angeles, Los Angeles, CA 90095; e-mail:
ywu@cs.ucla.edu
‡Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213; e-mail: zhiqings@cs.cmu.edu
§Department of Computer Science, University of California, Los Angeles, Los Angeles, CA 90095; e-mail:
hzyuan@cs.ucla.edu
¶Department of Computer Science, University of California, Los Angeles, Los Angeles, CA 90095; e-mail:
kauxuanji@cs.ucla.edu
‖Language Technologies Institute & Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA
15213; e-mail: yiming@cs.cmu.edu
∗∗Department of Computer Science, University of California, Los Angeles, Los Angeles, CA 90095; e-mail:
qgu@cs.ucla.edu
1arXiv:2405.00675v1  [cs.LG]  1 May 2024
their advancements, these models encounter challenges in tasks requiring high levels of reliability,
safety, and ethical alignment. To address these challenges, Reinforcement Learning from Human
Feedback (RLHF), also known as Preference-based Reinforcement Learning (PbRL), presents a
promising solution. This framework for policy optimization, highlighted in works by Christiano et al.
(2017) and recently in Ouyang et al. (2022), has led to significant empirical success in fine-tuning
instruction-following LLMs, making them more aligned with human preferences and thus more
helpful.
Most existing approaches to RLHF rely on either explicit or implicit reward models. Taking
InstructGPT (Ouyang et al., 2022) as an example, a reference policy πrefis first established,
typically from supervised pre-training or instruction-based (supervised) fine-tuning. An explicit
reward function is obtained by training a reward model based on human preference feedback data,
employing the Bradley-Terry (BT) model (Bradley and Terry, 1952). Subsequently, reinforcement
learning algorithms such as Proximal Policy Optimization (Schulman et al., 2017, PPO) are used to
fine-tune the reference LLM πrefby maximizing the expected reward function. The reward model
provides a “reward score” r(y;x) for the given response yand prompt x, approximately reflecting
how humans value these responses. More recently, methods like Direct Preference Optimization
(Rafailov et al., 2024, DPO) have been introduced. These methods forgo the training of a separate
reward model, instead using the log-likelihood ratio to implicitly represent the reward score, which
is then integrated into the same Bradley-Terry model to directly optimize the LLM. Nonetheless,
both the two-step RLHF algorithms and the one-step direct preference fundamentally adhere to the
reward maximization objective and are determined by parametric models such as the BT model.
Parametric preference models such as the Bradley-Terry model (Bradley and Terry, 1952) and
the Thurstone model (Thurstone, 1927) provide reasonable approximations of human preferences,
yet they fall short of fully capturing the complexity of human behavior. These models presuppose a
monotonous and transitive relationship among preferences for different choices. However, empirical
evidence suggests otherwise. For instance, Tversky (1969) observed human decisions can be
influenced by different factors and exhibit inconsistency. Such observations indicate that human
preferences do not always adhere to a single, value-based hierarchy and can even appear irrational,
such as exhibiting loops in preference relations. For LLMs, another motivating evidence is that
Munos et al. (2023) has empirically shown that directly predicting the pairwise preference can
achieve higher accuracy than predicting the preference via a BT-based reward model.
To address the inconsistency in human preference, researchers have proposed to work directly
with the preference probability and design algorithms that can more flexibly represent human
preferences (Lou et al., 2022; Wu et al., 2023) in the ranking or bandit setting. Recently, an
emerging line of work (Munos et al., 2023; Swamy et al., 2024) also proposed to study RLHF
for LLMs under such general preference P(y≻y′|x), where yandy′are two different responses
andxis the prompt. Munos et al. (2023) formulated RLHF as finding the Nash equilibrium of a
(Kullback–Leibler (KL) divergence-regularized) two-player constant-sum game where each player
is an LLM that outputs responses and aims to maximize its probability of being preferred over
its opponent, where the preference P(y≻y′|x) is assumed given by an external source, such as
human annotators or a strong language model. They proposed to approximate the Nash equilibrium
using an on-policy mirror descent algorithm. Very recently, Swamy et al. (2024) proposed Self-play
Preference Optimization (SPO) for the same (unregularized) two-player constant-sum game. Their
algorithm is designed to identify the mini-maximal optimal policy (i.e., the Nash equilibrium policy)
by iteratively fine-tuning the policy based on the data generated from the policy of the last iteration.
2
Different from our work, they focus on the Markov decision process (MDP) in simple robotic or
game tasks and employ typical policy optimization algorithms such as Proximal Policy Optimization
(PPO) (Schulman et al., 2017) or Soft Actor-Critic (SAC) (Haarnoja et al., 2018). It remains
unclear how their self-play framework can be applied to LLM alignment. More recently, Rosset et al.
(2024) proposed the Direct Nash Optimization (DNO) algorithm based on the cross-entropy between
true and predicted win rate gaps. However, their practical version still utilizes the iterative-DPO
framework as in Xu et al. (2023) which is not covered in their theoretical analysis.
In this paper, motivated by these observations, we propose a new self-play framework that (1)
enjoys provable guarantees to solve the two-player constant-sum game; and (2) can scale up to
large-scale efficient fine-tuning of large language models. In detail, we formulate the RLHF problem
as a constant-sum two-player game. Our objective is to identify the Nash equilibrium policy, which
consistently provides preferred responses over any other policy on average. To identify the Nash
equilibrium policy approximately, we adopt the classic online adaptive algorithm with multiplicative
weights (Freund and Schapire, 1999) as a high-level framework that solves the two-player game.
Further, each step of the high-level framework can be approximated by a self-play mechanism, where
in each round the policy is playing against itself in the previous round by fine-tuning it on synthetic
data that are generated by the policy and annotated by the preference model.
Our contributions are highlighted as follows:
•Starting from the exponential weight update algorithm which provably converges to the
Nash equilibrium of the two-player constant-sum game, we propose the Self-Play Preference
Optimization (SPPO) algorithm for large language model alignment. The algorithm converges
to an approximate Nash equilibrium provably and admits a simple form of loss function for
easy optimization.
•We compare our method with the state-of-the-art method including DPO, Identity Preference
Optimization (IPO) (Azar et al., 2023), and Kahneman-Tversky Optimization (KTO) (Etha-
yarajh et al., 2024), and show that our SPPO loss function can effectively increase the
log-likelihood of the chosen response and decrease that of the rejected response, which cannot
be trivially achieved by symmetric pairwise loss such as DPO and IPO. Our experiments also
confirm that SPPO outperforms iterative DPO and IPO on various benchmarks.
•Empirically, SPPO significantly enhances the well-aligned Mistral-7B-Instruct-v0.2 model,
achieving an increase of over 11% on the length-controlled win rate against GPT-4-Turbo
on the AlpacaEval 2.0 (Dubois et al., 2024a) test set. Additionally, SPPO exhibits strong
generalist abilities across different tasks, including MT-Bench, the Open LLM Leaderboard,
and the PairRM score (Jiang et al., 2023b). Unlike iterative DPO/IPO, which tends to show
performance decay on other benchmarks when optimized towards the PairRM score, SPPO’s
performance gain is consistent. Notably, all the strong performances are achieved without
external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language
models. Despite using only the 60k prompts (without responses) from the UltraFeedback
dataset (Cui et al., 2023) and forgoing any prompt augmentation, our method achieves
performance comparable to GPT-4 on the AlpacaEval 2.0 win-rate.
3
2 Related Work
RLHF with Explicit/Implicit Reward Model Originally, reinforcement learning from human
feedback (RLHF) was proposed by Christiano et al. (2017) as a methodology that first learns a reward
model reflecting human preferences and then uses reinforcement learning algorithms to maximize
the reward. This methodology is applied by Ouyang et al. (2022) to fine-tune instruction-following
large language models and leads to the popular ChatGPT.
The reward model in the works mentioned above assumes a parametric model such as the
Bradley-Terry model (Bradley and Terry, 1952), which assigns a “score” representing how preferred
a given response is. More recently, Rafailov et al. (2024) proposed to instead directly solve the closed-
form solution of such a score implied by the Bradley-Terry model. The Direct Policy Optimization
(DPO) method is claimed to be more efficient and stable, yet, still implicitly assumes such a reward
model that specifies the “score”. In a similar spirit, Zhao et al. (2023) proposed to calibrate the
score so that the score of the winner in comparison has a margin over the score of the loser, and
induces a different SLic loss. Similarly, Ethayarajh et al. (2024) derived a different loss function
(called KTO) from the Kahneman-Tversky human utility function, which implicitly denotes a score
of the given response. Liu et al. (2023) proposed Rejection Sampling Optimization (RSO) which
utilizes a preference model to generate preference pairs with candidates sampled from the optimal
policy; then preference optimization is applied on the sampled preference pairs. Hong et al. (2024)
proposed Odds Ratio Preference Optimization (ORPO) algorithm that can perform supervised
fine-tuning and preference alignment in one training session without maintaining an intermediate
reference policy.
RLHF with General Preference Model Often, the human preference is not strictly transitive,
and cannot be sufficiently represented by a single numerical score. Azar et al. (2023) proposed a
general preference optimization objective based on the preference probability between a pair of
responses instead of a score of a single response. They further propose a learning objective based on
identity mapping of the preference probability called IPO (Preference Optimization with Identity
mapping), which aims to maximize the current policy’s expected winning probability over a given
reference policy. Munos et al. (2023) formulated the RLHF problem with general preference as a
two-player, constant-sum game, where each player is one policy that aims to maximize the probability
of its response being preferred against its opponent. They aim to identify the Nash equilibrium policy
of this game and propose a mirror-descent algorithm that guarantees the last-iterate convergence of
a policy with tabular representations1. Swamy et al. (2024) studied the two-player constant-sum
game without KL regularization and proposed Self-Play Preference Optimization (SPO), an RLHF
framework similar to our framework in this paper. The framework is the same as ours in the idealized
case where the exponential weight update is utilized. Different from our work, they focus on the
multi-round Markov decision process (MDP) in robotic or game tasks rather than in fine-tuning
large language models. More recently, Rosset et al. (2024) proposed the Direct Nash Optimization
(DNO) algorithm based on the cross-entropy between true and predicted win rate gaps, and provided
theoretical guarantees on the error of finite-sample approximations. However, their practical version
still utilizes the iterative-DPO framework as in Xu et al. (2023) with the DPO loss instead of their
own DNO loss. Notably, in their experiments, they added the GPT-4 generated responses as their
1Due to the tabular representation, computing the normalizing factor is prohibitive and the algorithm is approxi-
mately executed by sampling one token instead of a full response.
4
“gold sample” into their fine-tuning data, and used GPT-4 as a judge to assign a numerical score to
each response for preference pair construction. In sharp contrast, our work does not require use any
strong external supervision besides a small-sized reward model.
Self-Play Fine-Tuning Most works mentioned above (Rafailov et al., 2024; Zhao et al., 2023;
Azar et al., 2023; Ethayarajh et al., 2024) consider one single optimization procedure starting from
some reference policy. The same procedure may be applied repeatedly for multiple rounds in a
self-play manner. In each round, new data are generated by the policy obtained in the last round;
these new data are then used for training a new policy that can outperform the old policy.
The self-play fine-tuning can be applied to both scenarios with or without human preference
data. For example, Singh et al. (2023) proposed an Expectation-Maximization (EM) framework
where in each round, new data are generated and annotated with a reward score; the new policy is
obtained by fine-tuning the policy on the data with a high reward. Chen et al. (2024) proposed
a self-play framework to fine-tune the model in a supervised way. In each round, new preference
pairs are synthesized by labeling the policy-generated responses as losers and the human-generated
responses as winners. Then DPO is applied in each round to fine-tune another policy based on
these synthesized preference data. Yuan et al. (2024) proposed Self-Rewarding Language Models,
where the language model itself is used to annotate preference on its own responses. Iterative
DPO is applied to fine-tune language models on these annotated data. These works show iterative
fine-tuning can significantly improve the performance.
Theory of RLHF There is also a line of research to analyze RLHF and provide its theoretical
guarantees. Zhu et al. (2023) studied the standard RLHF with separate reward-learning and
model-tuning and proposed a pessimistic reward-learning process that provably learns a linear
reward model. Wang et al. (2024) proposed a framework to reduce any RLHF problem with a
reward model to a reward-based standard RL problem. Additionally, they proposed to identify the
Nash equilibrium policy when a general preference model is present and show that the problem
can be reduced to a two-player zero-sum Markov game. Xiong et al. (2023) studied the reverse-KL
regularized contextual bandit for RLHF in different settings and proposed efficient algorithms with
finite-sample theoretical guarantees. Ye et al. (2024) studied the theoretical learnability of the
KL-regularized Nash-Learning from Human Feedback (NLHF) by considering both offline and online
settings and proposed provably efficient algorithms. Ji et al. (2024) proposed an active-query-based
proximal policy optimization algorithm with regret bounds and query complexity based on the
problem dimension and the sub-optimality gap.
3 Preliminaries
We consider the preference learning scenario as follows. Given a text sequence (commonly referred
to as prompt) x= [x1, x2, . . .], two text sequences y= [y1, y2, . . .] and y′are generated as responses
to the prompt x. An autoregressive language model πgiven the prompt xcan generate responses y
following the probability decomposition
π(y|x) =NY
i=1π(yi|x,y<i).
5
Given the prompt xand two responses yandy′, a preference oracle (either a human annotator or
a language model) will provide preference feedback o(y≻y′|x)∈ {0,1}indicating whether yis
preferred over y′. We denote P(y≻y′|x) =E[o(y≻y′|x)] as the probability of y“winning the
duel” over y′. The KL divergence of two probability distributions of density pandqis defined as
KL(p∥q) =Ey∼p(y)h
logp(y)
q(y)i
.
3.1 RLHF with Reward Models
Christiano et al. (2017) first learn a reward function r(y;x) following the Bradley-Terry model (Bradley
and Terry, 1952). For a prompt-response-response triplet ( x,y,y′), the Bradley-Terry model specifies
the probability of ybeing chosen over yas
P(y≻y′|x) =exp(r(y;x))
exp(r(y;x)) + exp( r(y′;x))=σ 
r(y;x)−r(y′;x)
, (3.1)
where σ(x) =ex/(ex+ 1) is the logistic function. The reward function associated with the Bradley-
Terry model can be estimated by maximizing the log-likelihood logP(y≻y′|x). Suppose the
true reward function r(y;x)) is available, Christiano et al. (2017) proposed to solve the following
optimization problem with policy optimization algorithms in RL such as PPO (Schulman et al.,
2017):
max
θEx∼X,y∼πθ(·|x)[r(y;x)]−η−1Ex∼X[KL(πθ(·|x)∥πref(·|x))], (3.2)
where Xis the prompt distribution.
Rafailov et al. (2024) identified that the optimization problem above has a closed-form solution
such that for any y,
π∗(y|x)∝πref(y|x) exp( ηr(y;x)),
which can be further converted to the DPO loss for any triplet ( x,yw,yl) where the winner ywis
chosen over the loser yl:
ℓDPO(x,yw,yl;θ;πref) :=−logσ 
η−1
logπθ(yw|x)
πref(yw|x)
−logπθ(yl|x)
πref(yl|x)!
.
3.2 RLHF with General Preference
Following Wang et al. (2024); Munos et al. (2023), we aim to establish RLHF methods without
a reward/utility model, as the human preference can be non-transitive (Tversky, 1969). Under
a general preference oracle P(y≻y′|x), we follow Dud´ ık et al. (2015) and aim to identify the
von Neumann winner . More specifically, the von Neumann winner π∗is the (symmetric) Nash
equilibrium of the following two-player constant-sum game:
(π∗, π∗) = arg max
πmin
π′Ex∼Xh
Ey∼π(·|x),y′∼π(·|x)
P(y≻y′|x)i
. (3.3)
In addition, we define the winning probability of one response yagainst a distribution of responses
πas
P(y≻π|x) =Ey′∼π(·|x)[P(y≻y′|x)],
6
and the winning probability of one policy πagainst another policy π′as
P(π≻π′|x) =Ey∼π(·|x)Ey′∼π′(·|x)[P(y≻y′|x)].
Furthermore, we define P(π≻π′) =Ex∼X[P(π≻π′|x)], where xis a prompt drawn from the
prompt distribution X. One may notice that P(y′≻y|x) = 1−P(y≻y′|x) due to the symmetry
of the preference oracle and thus P(π≻π′) +P(π′≻π) = 1. The two-player constant-sum game
(3.3) can be simplified as
(π∗, π∗) = arg max
πmin
π′P(π≻π′).
4 Self-Play Preference Optimization (SPPO)
In this section, we introduce the Self-Play Preference Optimization (SPPO) algorithm, derived from
the following theoretical framework.
4.1 Theoretical Framework
There are well-known algorithms to approximately solve the Nash equilibrium in a constant-sum
two-player game. In this work, we follow Freund and Schapire (1999) to establish an iterative
framework that can asymptotically converge to the optimal policy on average. We start with a
theoretical framework that conceptually solves the two-player game as follows:
πt+1(y|x)∝πt(y|x) exp( ηP(y≻πt|x)),fort= 1,2, . . . . (4.1)
(4.1) is an iterative framework that relies on the multiplicative weight update in each round tand
enjoys a clear structure. Initially, we have a base policy π1usually from some supervised fine-tuned
model. In each round, the updated policy πt+1is obtained from the reference policy πtfollowing
the multiplicative weight update. More specifically, a response yshould have a higher probability
weight if it has a higher average advantage over the current policy πt.
Equivalently, (4.1) can be written as
πt+1(y|x) =πt(y|x) exp 
ηP(y≻πt|x)
Zπt(x), (4.2)
where Zπt(x) =P
yπt(y|x)exp 
ηP(y≻πt|x)
is the normalizing factor (a.k.a., the partition
function). For any fixed xandy, the ideal update policy πt+1should satisfy the following equation:
logπt+1(y|x)
πt(y|x)
=η·P(y≻πt|x)−logZπt(x). (4.3)
Unlike the pair-wise design in DPO or IPO that cancels the log normalizing factor logZπt(x) by
differentiating (4.3) between yandy′, we choose to approximate (4.3) directly in terms of L2
distance:
πt+1= argmin
πEx∼X,y∼πt(·|x)
logπ(y|x)
πt(y|x)
−
ηP(y≻πt|x)−logZπt(x)2
. (4.4)
7
Estimation of the Probability The optimization objective (4.4) can be approximated with finite
samples. We choose to sample Kresponses y1,y2, . . . ,yK∼πt(·|x) for each prompt x, and denote
the empirical distribution by bπK
t. The finite-sample optimization problem can be approximated as
πt+1= argmin
πEx∼X,y∼πt(·|x)
logπ(y|x)
πt(y|x)
−
ηP(y≻bπK
t|x)−logZbπK
t(x)2
. (4.5)
Specifically, P(y≻bπK
t|x) =PK
k=1P(y≻yk|x)/KandZbπK
t(x) =Ey∼πt(·|x)[exp(ηP(y≻bπK
t|x))].
ZbπK
t(x), treated as an expectation, can be further estimated by Bnew samples with in total O(KB)
queries of the preference oracle P.(4.5) is an efficiently tractable optimization problem. Informally
speaking, when K→ ∞ ,(4.5) will recover (4.4). We have the following guarantee on the convergence
of (4.4):
Theorem 4.1. Assume the optimization problem (4.4) is realizable. Denote πtas the policy
obtained via (4.4) and the mixture policy ¯πT=1
TPT
t=1πt. By setting η= Θ(1 /√
T), we have that
max
π
P(π≻¯πT)
−min
π
P(π≺¯πT)
=O(1/√
T).
Theorem 4.1 characterizes the convergence rate of the average policy across the time horizon T
towards the Nash equilibrium, in terms of the duality gap. The proof is based on Theorem 1 in
Freund and Schapire (1999) with slight modification. For completeness, we include the proof in
Appendix A.
Alternatively, we can avoid estimating logZbπK
t(x) by replacing it simply with η/22in(4.5) to
obtain a more clear objective:
πt+1= argmin
πEx∼X,y∼πt(·|x)
logπ(y|x)
πt(y|x)
−η
P(y≻bπK
t|x)−1
22
. (4.6)
Intuitively, if a tie occurs (i.e., P(y≻bπK
t|x) = 1 /2), we prefer the model does not update weight
aty. Ifywins over bπK
ton average (i.e., P(y≻bπK
t|x)>1/2), then we increase the probability
density at yto employ the advantage of yoverbπK
t. In our experiments, we choose to minimize the
objective (4.6).
4.2 The SPPO Algorithm
Based on the aformentioned theoretical framework, we propose the Self-Play Preference Optimization
algorithm in Algorithm 1. In each round t, Algorithm 1 will first generate Kresponses y1,y2, . . . ,yK
according to πt(·|x) for each prompt x(Line 3). Then, the preference oracle Pwill be queried
to calculate the win rate among the Kresponses (Line 4). At Line 5, certain criteria can be
applied to determine which response should be kept in the constructed dataset Dtand construct
the prompt-response-probability triplet ( x,y,bP(y≻πt|x)). We will discuss the design choices later
in Section 5. One straightforward design choice is to include all Kresponses into Dtand each
bP(yi≻πt|x) is estimated by comparing yito all Kresponses. In total, O(K2) queries will be made.
Then the algorithm will optimize (4.6) on the dataset Dt(Line 6).
2Assuming the winning probability between any pair is a fair coin toss, when K→ ∞ , we can show that indeed
ZbπK
t(x)→eη/2.
8
Algorithm 1 Self-Play Preference Optimization (SPPO)
1:input : base policy πθ1, preference oracle P, learning rate η, number of generated samples K.
2:fort= 1,2, . . .do
3: Generate synthetic responses by sampling x∼ X andy1:K∼πt(·|x).
4: Annotate the win-rate P(yk≻yk′|x),∀k, k′∈[K].
5: Select responses from y1:Kto form dataset Dt={(xi,yi,bP(yi≻πt|xi))}i∈[N].
6: Optimize πθt+1according to (4.6):
θt+1←argmin
θE(x,y,bP(y≻πt|x))∼Dt
logπθ(y|x)
πt(y|x)
−η
bP(y≻πt|x)−1
22
. (4.7)
7:end for
Comparison with DPO, IPO, and KTO In practice, we utilize mini-batches of more than
2 responses to estimate the win rate of a given response, while the DPO and IPO loss focus on a
single pair of responses. When only a pair of responses ywandylis available, we have the pair-wise
symmetric loss based on the preference triplet ( x,yw,yl) defined as:
ℓSPPO (x,yw,yl;θ;πref) :=
logπθ(yw|x)
πref(yw|x)
−η
P(yw≻yl|x)−1
22
+
logπθ(yl|x)
πref(yl|x)
−η
P(yw≺yl|x)−1
22
, (4.8)
where P(yw≻yl|x) can be either a soft probability within [0 ,1] or a hard label 1 indicating yw≻yl.
We now compare the SPPO loss to other baselines. For the ease of comparison, let
a=βlogπθ(yw|x)
πref(yw|x)
, b=βlogπθ(yl|x)
πref(yl|x)
, c=βKL(πθ∥πref),
then we have
ℓDPO(yw,yl,x) =−logσ(a−b), (4.9)
ℓIPO(yw,yl,x) = [( a−b)−1]2, (4.10)
ℓKTO(yw,yl,x) =σ(−a+c) +σ(b−c) (simplified) , (4.11)
where σ(x) =ex/(ex+ 1) and the SPPO loss can be written as
ℓSPPO(yw,yl,x) = (a−1/2)2+ (b+ 1/2)2.
It can be seen that SPPO not only pushes the gap between aandbto be 1, but also attempts to push
value of ato be close to 1 /2 and the value of bto be close to −1/2 such that πθ(yw|x)> π ref(yw|x)
andπθ(yl|x)< π ref(yl|x). We believe this is particularly important: when there are plenty of
preference pairs, DPO and IPO can ensure the policy will converge to the target policy, but when
the preference pairs are scarce (e.g., one pair for each prompt), there is no guarantee that the
estimated reward of the winner awill increase and the estimated reward of the loser bwill decrease.
Instead, only the reward gap between the winner and the loser (i.e., a−b) will increase. This
9
phenomenon is observed by Pal et al. (2024) that DPO only drives the loser’s likelihood to be small,
but the winner’s likelihood barely changes. We believe that fitting βlog
πt+1(y|x)
πt(y|x)
directly to
P(y≻πt|x)−1/2 is more direct than IPO which attempts to fit βlog
πt+1(yw|x)
πt(yw|x)
−βlog
πt+1(yl|x)
πt(yl|x)
toP(yw≻πt|x)−P(yl≻πt|x). In addition, SPPO shares a similar spirit as KTO. The KTO loss
pushes ato be large by minimizing σ(−a+c) and pushes bto be small by minimizing σ(b−c). In
contrast, SPPO pushes ato be as large as 1 /2 and bto be as small as −1/2.
On the other hand, we would like to comment that although DPO and KTO can be extended
to their iterative variants, they are not by nature iterative algorithms and do not have provable
guarantees that they can reach the Nash equilibrium. In contrast, SPPO and IPO are by design
capable to solve the Nash equilibrium iteratively. SPPO is superior to IPO because its design
explicitly alleviates the data sparsity issue, as discussed above.
5 Experiments
We conduct extensive experiments to show the performance of our method and compare it with
other baselines.
5.1 Experiment Setup
Base Model and Datasets We follow the experimental setup of Snorkel3, a model that utilizes
iterative DPO to achieve state-of-the-art performance on AlpacaEval benchmarks. Specifically,
we use Mistral-7B-Instruct-v0.2 as our base model4. Mistral-7B-Instruct-v0.2 is an instruction
fine-tuned version of Mistral-7B-v0.2 model (Jiang et al., 2023a). We also adopt Ultrafeedback (Cui
et al., 2023) as our source of prompts which includes around 60k prompts from diverse resources.
During generation, we follow the standard chat template of Mistral-7B. In order to avoid overfitting
during the fine-tuning, we split the dataset into three portions and use only one portion per iteration.
These settings were also adopted by training the model Snorkel-Mistral-PairRM-DPO5(Snorkel).
We follow the splitting in Snorkel for a fair comparison.
Preference Model We employ PairRM (Jiang et al., 2023b), an efficient pair-wise preference
model of size 0.4B. PairRM is based on DeBERTA-V3 (He et al., 2021) and trained on high-quality
human-preference datasets. Results on benchmarks like Auto-J Pairwise dataset (Li et al., 2023a)
show that it outperforms most of the language-model-based reward models and performs comparably
with larger reward models like UltraRM-13B (Cui et al., 2023). We refer the readers to the homepage
on Huggingface6for detailed benchmark results. We therefore keep PairRM as our ranking model
following Snorkel for a balance between accuracy and efficiency.
Specifically, PairRM will output a “relative reward” s(y,y′;x) that reflects the strength difference
between yandy′, i.e.,
P(y≻y′|x) =exp(s(y,y′;x))
1 + exp( s(y,y′;x)).
3https://huggingface.co/snorkelai/Snorkel-Mistral-PairRM-DPO
4https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
5https://huggingface.co/snorkelai/Snorkel-Mistral-PairRM-DPO
6https://huggingface.co/llm-blender/PairRM
10
Unlike the Bradley-Terry-based reward model, PairRM only assigns the relative reward which is not
guaranteed to be transitive (i.e., s(y1,y2;x) +s(y2,y3;x)̸=s(y1,y3;x)). So it indeed models the
general preference.
Response Generation and Selection During the generation phase in each iteration, we use top
p= 1.0 and temperature 1 .0 to sample from the current policy. We sample with different random
seeds to get K= 5 different responses for each prompt. Previous works utilizing Iterative DPO
choose 2 responses to form a pair for each prompt. For a fair comparison, we do not include all
K= 5 responses in the preference data but choose two responses among them. Following Snorkel,
we choose the winner ywand loser ylto be the response with the highest andlowest PairRM score,
which is defined for each response yias:
sPairRM (yi;x) :=1
KKX
k=1s(yi,yk;x).
Probability Estimation We then estimate the win rate over the distribution by the average win
rate over all the sampled responses as explained in (4.5):
bP(yi≻πt|xi) =1
KKX
k=1P(yi≻yk|x),∀i∈[K].
Hyperparameter Tuning The experiments are conducted on 8 ×Nvidia A100 GPUs. For
SPPO, we trained three iterations in total. In each iteration, we selected the model that was trained
on the first epoch of the 20k prompts from UltraFeedback to proceed to the next iteration. The
global training batch size is set to 64 and ηis set to 1 e3. The learning rate schedule is determined by
the following hyperparameters: learning rate=5.0e-7, number of total training epochs=18, warmup
ratio=0.1, linear schedule. The best hyper-parameters for each model is selected by the average
win-rate (judged by PairRM-0.4B) on a hold-out subset of Ultrafeedback as the metric. For more
details on the win-rate comparison using PairRM as a judge, please refer to Section 5.2 and Figure 3.
Baselines We evaluate the following base models as well as baseline methods for fine-tuning
LLMs:
•Mistral-7B-Instruct-v0.2: Mistral-7B-Instruct-v0.2 is an instruction fine-tuned version of
Mistral-7B-v0.2 model (Jiang et al., 2023a). It is the starting point of our algorithm.
•Snorkel (Mistral-PairRM-DPO): We directly evaluate the uploaded checkpoint on Hugging-
Face7. This model is obtained by three rounds of iterative DPO from Mistral-7B-Instruct-v0.2.
•(Iterative) DPO: We also implement the iterative DPO algorithm by ourselves. The experi-
mental settings and model selection schemes align with those used for SPPO, except for the
adoption of the DPO loss function as defined in (4.9). Hyperparameters are optimized to
maximize the average win-rate assessed by PairRM at each iteration. Note that the practical
algorithm in Rosset et al. (2024) is essentially the same as iterative DPO.
7https://huggingface.co/snorkelai/Snorkel-Mistral-PairRM-DPO
11
•(Iterative) IPO: We implement the iterative IPO algorithm by ourselves. The experimental
setting and the model selection scheme is the same as iterative DPO, except that the loss
function is the IPO loss (4.10) . For fair comparison, hyperparameters for IPO is also selected
by evaluation using the average PairRM win-rate on the hold-out subset of Ultrafeedback.
•Self-rewarding LM: Yuan et al. (2024) proposed to prompt the LLM itself as a preference
judge to construct new preference pairs and iteratively fine-tune the LLM with the DPO
algorithm. We use the AlpacaEval 2.0 win rate reported by Yuan et al. (2024) for comparison.
Note that Self-rewarding LM is a trained from Llama 2 70B.
Benchmarks Following previous works, we use AlpacaEval 2.0 (Dubois et al., 2024a), MT-Bench
(Zheng et al., 2024), and Open LLM Leaderboard (Beeching et al., 2023a) as our evaluation
benchmarks.
•AlpacaEval 2.0 is an LLM-based automatic evaluation benchmark. It employs AlpacaFarm
(Dubois et al., 2024b) as its prompts set composed of general human instructions. The model
responses and the reference response generated by GPT-4-Turbo are fed into a GPT-4-Turbo-
based annotator to be judged. We follow the standard approach and report the win rate over
the reference responses.
•MT-Bench (Zheng et al., 2024) is a collection of 80 high-quality multi-turn open-ended
questions. The questions cover topics like writing, role-playing, math, coding, etc.. The
generated answer is judged by GPT-4 and given a score directly without pairwise comparison.
•Open LLM Leaderboard (Beeching et al., 2023a) consists of six datasets, each of which
focuses on a facet of language model evaluation. In detail, the evaluation rubric includes math
problem-solving, language understanding, human falsehood mimicking, and reasoning. We
follow the standard evaluation process and use in-context learning to prompt the language
model and compute the average score over six datasets to measure the performance.
5.2 Experimental Results
We evaluate the models on the three benchmarks described above. We also compare models based
on the pre-trained preference model PairRM.
Evaluation using GPT-4 as a judge In the assessment of AI chatbots, human evaluation
remains the benchmark for quality and accuracy (Askell et al., 2021; Ouyang et al., 2022). However,
due to its limitations in scalability and reproducibility, we explore the alternative approach of using
the advanced capabilities of GPT-4 (OpenAI et al., 2023) as an automatic evaluation tool. We
conduct GPT-4-based automatic evaluation on AlpacaEval 2.0 (Li et al., 2023b) and MT-Bench
(Zheng et al., 2023) to measure the chatbot capability of our model. The results can be found
in Table 1 for AlpacaEval 2.0 and Figure 2 (left) for MT-Bench. We also provide a radar chart
analyzing the MT-Bench results in Figure 2 (right). We found that the performance of SPPO
models consistently improve along with the iterative alignment iterations.
Table 1 (AlpacaEval 2.0) shows the win rate over the GPT-4-Turbo baseline of different models
on 805 prompts. We also include one column indicating the length-controlled win rate, and one
12
Table 1: AlpacaEval 2.0 evaluation of various models (detailed in Baselines) in terms of both
normal and length-controlled (LC) win rates in percentage (%). SPPO Iter3 model achieves the
highest LC win rate of 28.53% and a normal win rate of 31.02%. SPPO demonstrates steady
performance gains across iterations and outperforms other baselines which show a tendency to
produce longer responses. Additionally, re-ranking with the PairRM reward model (best-of-16) at
test time consistently enhances the performance across all models and SPPO (best-of-16) achieves
high win rate without strong external supervision like GPT-4 .
ModelAlpacaEval 2.0
LC Win Rate Win Rate Avg. Len
Mistral-7B-Instruct-v0.2 17.11 14.72 1676
Mistral-7B-Instruct-v0.2 (best-of-16) 22.45 17.94 1529
Snorkel (Mistral-PairRM-DPO) 26.39 30.22 2736
Snorkel (Mistral-PairRM-DPO best-of-16) 29.97 34.86 2616
Self-Rewarding 70B Iter1 - 9.94 1092
Self-Rewarding 70B Iter2 - 15.38 1552
Self-Rewarding 70B Iter3 - 20.44 2552
DPO Iter1 23.81 20.44 1723
DPO Iter2 24.23 24.46 2028
DPO Iter3 22.30 23.39 2189
IPO Iter1 23.78 20.77 1693
IPO Iter2 21.08 23.38 2660
IPO Iter3 20.06 22.47 2760
SPPO Iter1 24.79(+7.69) 23.51(+8.79) 1855
SPPO Iter2 26.89(+2.10) 27.62(+4.11) 2019
SPPO Iter3 28.53 (+1.64) 31.02 (+3.40) 2163
SPPO Iter1 (best-of-16) 28.71(+6.26) 27.77(+9.83) 1901
SPPO Iter2 (best-of-16) 31.23(+2.52) 32.12(+4.35) 2035
SPPO Iter3 (best-of-16) 32.13 (+0.9) 34.94 (+2.82) 2174
column on the average length of each model, to account for the tendency of the LLM-based judge
to favor longer sequence outputs — an issue colloquially termed the ”reward hacking” phenomenon.
According to the table, SPPO Iter3 has the highest win rate, 28.52% for the length-controlled
version, and 31.02% for the overall win rate. The performance gains over previous iterations are
7.69% (Mistral-7B-Instruct →Iter1), 2.10% (Iter1 →Iter2), and 1.64% (Iter2 →Iter3), respectively,
indicating steady improvements across iterations, as illustrated in Figure 1. Additionally, the data
indicates that SPPO achieves superior performance compared to the iterative variants of DPO and
IPO. The length-controlled win rate for SPPO reaches 28.53%, outperforming the DPO’s best rate
of 26.39% (by Snorkel) and IPO’s rate of 25.45% . Notably, while DPO and IPO training tend
to significantly increase the average output length—2736 and 2654, respectively—SPPO shows a
more moderate length increase, moving from 1676 in the base model to 2163 at the third iteration.
This suggests that SPPO improves the performance while more effectively controlling the tendency
13
Table 2: AlpacaEval 2.0 leaderboard results of both normal and length-controlled (LC) win rates
in percentage (%). SPPO can outperform larger models and SPPO (best-of-16) can outperform
proprietary models such as GPT-4(6/13).
ModelAlpacaEval 2.0
LC. Win Rate Win Rate
GPT-4 Turbo 50.0 50.0
Claude 3 Opus 40.5 29.1
GPT-4 0314 35.3 22.1
Llama 3 70B Instruct 34.4 33.2
SPPO Iter3 (best-of-16) 32.1 34.9
GPT-4 0613 30.2 15.8
Snorkel (Mistral-PairRM-DPO best-of-16) 30.0 34.9
Mistral Medium 28.6 21.9
SPPO Iter3 28.5 31.0
Claude 2 28.2 17.2
Snorkel (Mistral-PairRM-DPO) 26.4 30.2
Gemini Pro 24.4 18.2
Mistral 8 ×7B v0.1 23.7 18.1
Llama 3 8B Instruct 22.9 22.6
GPT-3.5 Turbo 0613 22.7 14.1
Vicuna 33B v1.3 17.6 12.7
towards longer output lengths compared to DPO and IPO. Finally, we present the best-of-16 results
for each model, selected using the PairRM reward model. We find that re-ranking with the preference
model at test time can consistently improve the performance of base model (Mistral-7B-Instruct-
v0.2), DPO (Snorkel), and SPPO (Iter3) by 5.34%, 3.57%, and 3.6%, respectively. Notably, this
shows that while SPPO significantly enhances model alignment using PairRM-0.4B as the sole
external supervision, it has not resulted in over-optimization against the preference model (Gao
et al., 2023). Future work will explore further improvements in model alignment, potentially through
additional iterations beyond the current three (following Snorkel’s methodology).
In Table 2, we compare SPPO on the AlpacaEval 2.0 leaderboard with other state-of-the-art AI
chatbots. We found our SPPO model outperforms many competing models trained on proprietary
alignment data (e.g., Claude 2, Gemini Pro, & Llama 3 8B Instruct). With test-time reranking,
SPPO Iter3 (best-of-16) is even competitive to GPT-4 0613 and Llama 3 70B Instruct.
In Figure 2 (left), we evaluate the performance of SPPO on MT-Bench. We can see that SPPO
Iter3 outperforms all baseline models, achieving an average score of 7.59. While we are not certain
why the MT-Bench performance drops at the first two iterations, the performance of SPPO at the
final iteration still improves over the base model. Since the length-controlled AlpacaEval 2.0 has
a 98% Pearson correlation with human evaluations and 10 ×more evaluation prompts, it likely
provides a more reliable evaluation than MT-Bench. To gain deeper understanding on MT-Bench
performance, we plot the improvement in Figure 2 (right), broken down by question prompt category.
14
Iter1 Iter2 Iter320253035LC. Win Rate (%)
GPT-4 0314
Snorkel (Mistral-PairRM-DPO)
Mistral-7B-Instruct-v0.2
DPO
IPO
SPPO(a)
Iter1 Iter2 Iter31015202530Win Rate (%)
GPT-4 0314
Snorkel (Mistral-PairRM-DPO)
Mistral-7B-Instruct-v0.2
Self-Rewarding 70B
DPO
IPO
SPPO-7B (b)
Figure 1: Win Rate against GPT-4-Turbo with (a) and without (b) Length Controlling (LC) on
AlpacaEval 2.0. SPPO demonstrates steady improvements on both LC and raw win rates.
Table 3: Open LLM Leaderboard Evaluation . SPPO fine-tuning improves the base model’s
performance on Arc, TruthfulQA, and GSM8k, reaching a state-of-the-art average score of 66.75.
However, subsequent iterations of DPO, IPO, and SPPO see a decline in performance. It is possible
that aligning with human preferences (simulated by the PairRM preference model in our study)
may not always enhance, and can even detract from, overall performance.
Models Arc TruthfulQA WinoGrande GSM8k HellaSwag MMLU Average
Mistral-7B-Instruct-v0.2 63.65 66.85 77.98 41.93 84.89 59.15 65.74
Snorkel 66.04 70.86 77.74 36.77 85.64 60.83 66.31
DPO Iter1 63.14 68.39 77.19 40.33 85.25 59.41 65.62
DPO Iter2 64.16 67.84 76.09 39.95 85.23 59.03 65.38
DPO Iter3 65.19 67.89 77.27 32.30 85.49 59.00 64.52
IPO Iter1 64.68 68.60 77.98 43.75 85.08 59.04 66.52
IPO Iter2 62.12 66.30 77.51 39.20 83.15 59.70 64.66
IPO Iter3 62.97 67.12 77.51 37.45 83.69 59.57 64.72
SPPO Iter1 65.02 69.40 77.82 43.82 85.11 58.84 66.67
SPPO Iter2 65.53 69.55 77.03 44.35 85.29 58.72 66.75
SPPO Iter3 65.36 69.97 76.80 42.68 85.16 58.45 66.40
SPPO Iter3 demonstrates notable gains in RolePlay, Reasoning, Math, and Coding tasks.
Open LLM Leaderboard We further evaluate the capabilities of SPPO models using Huggingface
Open LLM Leaderboard (Beeching et al., 2023b). This leaderboard encompasses 6 different datasets,
each focusing on a a specific capability of LLMs: Arc (Clark et al., 2018), HellaSwag (Zellers et al.,
2019), Winogrande (Sakaguchi et al., 2021), MMLU (Hendrycks et al., 2020), TruthfulQA (Lin et al.,
2021), and GSM8k (Cobbe et al., 2021). The models are prompted with zero or few-shot exemplars.
The results, presented in Table 3, demonstrate that SPPO can enhance the performance of the
base model on Arc, TruthfulQA, and GSM8k, and achieve the state-of-the-art performance with an
15
ModelMT-Bench
1st Turn 2nd Turn Average
Mistral-7B-Instruct-v0.2 7.78 7.25 7.51
Snorkel (Mistral-PairRM-DPO) 7.83 7.33 7.58
DPO Iter1 7.45 6.58 7.02
DPO Iter2 7.57 6.56 7.06
DPO Iter3 7.49 6.69 7.09
SPPO Iter1 7.63 6.79 7.21
SPPO Iter2 7.90 7.08 7.49
SPPO Iter3 7.84 7.34 7.59
Writing
Roleplay
Reasoning
Math
CodingExtractionSTEMHumanities
0 2 4 6 8 10model
Mistral-7B-Instruct-v0.2
SPPO Iter1
SPPO Iter2
SPPO Iter3
Figure 2: MT-Bench Evaluation. Left: SPPO Iter3 outperforms all baseline models by achieving
an average score of 7.59. Despite initial drops in performance in the first two iterations, SPPO
Iter3 improves upon the base model by the final iteration. Right: Radar chart of MT-Bench results.
SPPO Iter3’s improves across different MT-Bench categories, showing significant gains in RolePlay,
Reasoning, Math, and Coding tasks.
averagte score of 66.75. However, these improvements do not hold in subsequent alignment iterations:
DPO, IPO, and SPPO’s performance declines after the first or second iterations. This limitation
may be attributed to the “alignment tax” phenomenon (Askell et al., 2021), which suggests that
aligning with human preferences (simulated by PairRM preference in our study) might not improve
or even hurt the general performance. Improving language model capabilities through alignment
iterations remains a topic for future research, and we posit that incorporating high-quality SFT
annotations (Chen et al., 2024) could play a significant role in this endeavor.
Evaluation using PairRM as a judge As SPPO identifies the von Neumann winner (see (3.3))
in a two-player constant-sum game, we examine the pairwise preferences among SPPO models and
other baselines. The pairwise win rates, measured by PairRM, are depicted in Figure 3. We observe
that in all algorithms—namely DPO, IPO, and SPPO—the newer model iterations surpass the
previous ones. For example, SPPO Iteration 3 outperforms SPPO Iteration 2. Both SPPO and IPO
consistently outperform DPO across all iterations. While SPPO is superior to IPO in the first two
iterations, IPO exceeds SPPO in performance during the final iteration. Considering the superior
performance of SPPO in standard benchmarks evaluated by GPT-4 or against ground-truth answers
(e.g., AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard), along with IPO’s tendency to
produce longer sequence outputs (see Avg. Len in Table 1), we believe this is due to IPO exploiting
the length bias in PairRM that favors longer sequences. Conversely, SPPO models benefit from a
more robust regularization within a multiplicative weight update framework.
5.3 Ablation Study
We study the effect of mini-batch size when estimating the win rate P(y≻πt|x). Specifically, for
each prompt, we still generate 5 responses and choose the winner ywand loser ylaccording to the
16
0.500 0.552 0.572 0.577 0.631 0.656 0.664 0.719 0.713 0.741 0.809
0.448 0.500 0.520 0.523 0.577 0.601 0.627 0.676 0.682 0.706 0.781
0.428 0.480 0.500 0.496 0.568 0.580 0.609 0.659 0.652 0.673 0.758
0.423 0.477 0.504 0.500 0.552 0.577 0.608 0.636 0.631 0.667 0.750
0.369 0.423 0.432 0.448 0.500 0.517 0.551 0.595 0.598 0.631 0.722
0.344 0.399 0.420 0.423 0.483 0.500 0.530 0.574 0.577 0.613 0.707
0.336 0.373 0.391 0.392 0.449 0.470 0.500 0.550 0.562 0.587 0.670
0.281 0.324 0.341 0.364 0.405 0.426 0.450 0.500 0.514 0.540 0.651
0.287 0.318 0.348 0.369 0.402 0.423 0.438 0.486 0.500 0.526 0.633
0.259 0.294 0.327 0.333 0.369 0.387 0.413 0.460 0.474 0.500 0.613
0.191 0.219 0.242 0.250 0.278 0.293 0.330 0.349 0.367 0.387 0.500
IPO Iter3 SPPO Iter3 Snork
el (Mistral-P
airRM-DPO)DPO Iter3 SPPO Iter2 IPO Iter2 DPO Iter2 SPPO Iter1 DPO Iter1 IPO Iter1 Mistral-7B-Instruct-
v0.2IPO Iter3
SPPO Iter3
Snork el (Mistral-P airRM-DPO)
DPO Iter3
SPPO Iter2
IPO Iter2
DPO Iter2
SPPO Iter1
DPO Iter1
IPO Iter1
Mistral-7B-Instruct- v0.2
0.20.30.40.50.60.70.8Figure 3: Pairwise win rates among base model (Mistral-7B-Instruct-v0.2), DPO models, IPO
models, and SPPO models using PairRM-0.4B as a judge, which may favor models with longer
outputs. On benchmarks with more powerful judge models (e.g., GPT-4), such as AlpacaEval 2.0
and MT-Bench, SPPO outperforms other baseline algorithms by a large margin.
PairRM score. When estimating the probability, we varies the batch size to be K= 2,3,5. For
K= 2, we estimate P(y≻πt|x) with only 2 samples ywandyl:
bP(yw≻πt|x) =P(yw≻yw|x) +P(yw≻yl|x)
2=1/2 +P(yw≻yl|x)
2,
andbP(yl≻πt|x) similarly. K= 5 indicates the original setting we use.
We compare the results on AlpacaEval 2.0, as shown in Figure 4. We find that the performance
of SPPO is robust to the noise in estimating P(y≻πt|x). While K= 5 initially outperforms
K= 2 in the first iteration, the difference in their performance diminishes in subsequent iterations.
Additionally, we observe that K= 2 exhibits a reduced tendency to increase output length.
6 Conclusions
This paper introduced Self-Play Preference Optimization (SPPO), an innovative approach to fine-
tuning Large Language Models (LLMs) from Human/AI Feedback. Our method performs self-play
17
Mini-Batch
SizeIterationAlpacaEval 2.0
Win Rate Avg. Len
(chars) LC. Raw
K= 2Iter1 23.85 23.53 1948
Iter2 26.91 27.24 1999
Iter3 28.26 28.22 1961
Iter1 24.79 23.51 1855
Iter2 26.89 27.62 2019 K= 5
Iter3 28.53 31.02 2163
Iter1 Iter2 Iter3182022242628LC. Win Rate (%)
Snorkel (Mistral-PairRM-DPO)
Mistral-7B-Instruct-v0.2
SPPO (K=2)
SPPO (K=5)
Figure 4: AlpacaEval 2.0 evaluation on SPPO of different mini-batch size in terms of both normal
and length-controlled (LC) win rates in percentage (%). K= 2,5 denote different mini-batch sizes
when estimating the win rate P(y≻πt|x).
within a two-player game to iteratively refine models towards the Nash equilibrium, guided by a
preference-based learning objective. SPPO has demonstrated significant improvements over existing
methods such as DPO and IPO across multiple benchmarks, including AlpacaEval 2.0, MT-Bench,
and the Open LLM Leaderboard. By integrating a preference model and employing a batched
estimation process, SPPO aligns LLMs more closely with human preferences and avoids common
pitfalls such as “length bias” reward hacking. These results underscore the potential of SPPO to
enhance the alignment of generative AI systems, making a compelling case for its broader application
in the field of LLMs and beyond.
A Proof of Theorem 4.1
Proof of Theorem 4.1. Suppose the optimization problem is realizable, we have exactly that
πt+1(y|x)∝πt(y|x) exp( ηP(y≻πt|x)),fort= 1,2, . . . . (A.1)
To prove that the exponential weight update can induce the optimal policy, we directly invoke a
restated version of Theorem 1 in Freund and Schapire (1999):
Lemma A.1 (Theorem 1 in Freund and Schapire (1999), restated) .For any oracle Pand for any
sequence of mixed policies µ1, µ2, . . . , µ T, the sequence of policies π1, π2, . . . , π Tproduced by (A.1)
satisfies:
TX
t=1P(πt≺µt)≤min
πη
1−e−ηTX
t=1P(π≺µt) +KL(π∥π0)
1−e−η
.
By setting µt=πt, we have that
T
2≤min
πηT
1−e−ηP(π≺¯πT) +KL(π∥π0)
1−e−η
,
18
where the LHS comes from that P(πt≺πt) = 1 /2 and the RHS comes from that1
TPT
t=1P(π≺
πt) =P(π≺¯πt). Now rearranging terms gives
1−e−η
2η≤min
π
P(π≺¯πT) +KL(π∥π0)
ηT
.
We can naively bound the KL-divergence KL(π∥π0)≤ ∥logπ0(·)∥∞, which can be seen as a (large)
constant.
By choosing η=∥logπ0(·)∥∞√
T, we have
1
2−s
∥logπ0(·)∥∞
4√
T+O(T−1)≤min
π
P(π≺¯πT)
+s
∥logπ0(·)∥∞√
T,
where the LHS comes from Taylor’s expansion1−e−η
2η=1
2−η
4+o(η). Notice that 1 /2 at the LHS is
already the value of the symmetric two-player constant-sum game. This shows that for appropriately
chosen ηandT, the mixture policy ¯ πTis close to the minimax optimal policy (Nash equilibrium).
The optimality gap is thus bounded by
max
π
P(π≻¯πT)
−min
π
P(π≺¯πT)
= max
π
1−P(π≺¯πT)
−min
π
P(π≺¯πT)
= 21
2−min
π
P(π≺¯πT)
=O1√
T
.
B Response Example in different iterations
19
Table 4: Generation example of our fine-tuned model by SPPO at different iterations.
Prompt You will be given a definition of a task first, then some input of the task. You are
given a dialog between 2 or more individuals. The dialog will include information
as to the relationship between 2 of the participants. At the end, you will be asked
to provide an answer as to the relationship of those 2 participants. In this scenario,
’0’ will represent a spousal relationship, ’1’ will represent siblings, and ’2’ will
represent a child/parent relationship. Speaker 1: I’ll be right with you. Okay?
Thanks, Wendy. Speaker 2: Oh my God! How cute is the on-call doctor? Speaker
3: Ooh, so cute, that I’m thinking about jamming this pen in my eye. Speaker 4:
Dr. Burke will see you know. Speaker 2: Oh no-no-no, not Dr. Burke. Dr. Burke
is out of town. The-the on-call doctor will see me now. Speaker 4: Dr. Richard
Burke is out of town. Dr. Timothy Burke, his son, will see you now. Speaker 5:
Ready? What is the relationship between Speaker 5 and Richard Burke? Output:
Mistral-7B The relationship between Speaker 5 and Richard Burke is that of a parent and
child, as Richard Burke is identified as the son of another character in the dialog.
Iteration 1 The relationship between Speaker 5 and Richard Burke is that of a father and
child. Based on the dialogue, it was revealed that Dr. Timothy Burke is the on-call
doctor and the son of Dr. Richard Burke.
Iteration 2 The relationship between Speaker 5 and Richard Burke is that of a father and
child. The conversation reveals that Dr. Timothy Burke is the on-call doctor and
he is identified as the son of Dr. Richard Burke.
Iteration 3 Speaker 5 is the individual asking about the relationship, so they are not directly
related to Dr. Richard Burke. The information provided in the dialog reveals
that Dr. Timothy Burke is the on-call doctor and the son of Dr. Richard Burke.
Therefore, the answer is: The relationship between Speaker 5 and Dr. Richard
Burke is that of a child (Dr. Timothy Burke) and parent. Speaker 5 is inquiring
about the relationship, but they are not themselves a part of it. The specified
relationship codes (0 for spousal, 1 for sibling, 2 for child/parent) align with
this description: Speaker 5 (0, 1, or 2) and Dr. Richard Burke (2) connect as a
parent-child duo. Output: 2 (child/parent relationship) In this scenario, Speaker
5’s role is to determine the relationship between herself and Dr. Richard Burke,
but since they are not related (she is an outside inquirer), the correct answer
pertains only to the relationship identified within the dialog: Dr. Richard Burke
(father) and Dr. Timothy Burke (son).
20
Table 5: Another generation example of our fine-tuned model by SPPO at different iterations.
Prompt Q: Context: Roman Brady is a fictional character from the long running NBC soap
opera, Days of Our Lives. The role was originated in 1981 by Wayne Northrop
who played the role until 1984, was filled by Drake Hogestyn from 1986 – 1991,
again by Northrop from 1991 – 1994, and is currently being portrayed by series
veteran Josh Taylor, who stepped into the role in 1997. Question: who is roman
brady on days of our lives? Let’s give stream of consciousness below
Mistral-7B Roman Brady is a fictional character on the NBC soap opera ”Days of Our Lives.”
He was first portrayed by Wayne Northrop from 1981 to 1984. Later, Drake
Hogestyn took over the role from 1986 to 1991. Northrop returned to the role from
1991 to 1994. Since 1997, the character is being portrayed by Josh Taylor. Roman
Brady is a significant character in the show’s long-running storylines.
Iteration 1 Roman Brady is a fictional character on the NBC soap opera ”Days of Our Lives.”
He was first portrayed by Wayne Northrop from 1981 to 1984. Drake Hogestyn
took over the role from 1986 to 1991. Northrop returned to play Roman again
from 1991 to 1994. Since 1997, the character has been portrayed by Josh Taylor.
So, in summary, Roman Brady is a long-standing character on ”Days of Our Lives”
who has been played by three different actors over the years: Wayne Northrop
(1981-1984, 1991-1994), Drake Hogestyn (1986-1991), and currently by Josh Taylor
(1997-present).
Iteration 2 Roman Brady is a fictional character on the NBC soap opera ”Days of Our Lives.”
He was first portrayed by Wayne Northrop from 1981 to 1984. Drake Hogestyn
took over the role from 1986 to 1991. Northrop returned to play Roman again
from 1991 to 1994. Since 1997, the character has been portrayed by Josh Taylor.
Overall, Roman Brady is a long-standing and complex figure in the show’s history,
with multiple actors bringing their unique interpretations to the role.
Iteration 3 Roman Brady is a fictional character on the NBC soap opera ”Days of Our Lives.”
He was originally played by Wayne Northrop from 1981 to 1984. Drake Hogestyn
took over the role from 1986 to 1991. Northrop returned to portray Roman
once more from 1991 to 1994. Since 1997, the character has been portrayed by
Josh Taylor.Roman Brady is a complex and prominent figure in the long-running
daytime drama. Initially played by Northrop in the early 1980s, his tenure included
involvement in various storylines, such as romantic relationships and business
dealings. Hogestyn’s portrayal during the late 1980s and early 1990s introduced
new dimensions to the character. Northrop’s second stint, from 1991 to 1994,
further developed Roman’s narrative. Since 1997, Josh Taylor has assumed the
role, continuing to contribute to the evolving story of Roman Brady on ”Days of
Our Lives.” Throughout these different portrayals, Roman has remained a central
character, known for his intricate relationships, business ventures, and personal
struggles.
21
References
Askell, A. ,Bai, Y. ,Chen, A. ,Drain, D. ,Ganguli, D. ,Henighan, T. ,Jones, A. ,Joseph,
N.,Mann, B. ,DasSarma, N. et al. (2021). A general language assistant as a laboratory for
alignment. arXiv preprint arXiv:2112.00861 .
Azar, M. G. ,Rowland, M. ,Piot, B. ,Guo, D. ,Calandriello, D. ,Valko, M. andMunos,
R.(2023). A general theoretical paradigm to understand learning from human preferences. arXiv
preprint arXiv:2310.12036 .
Beeching, E. ,Fourrier, C. ,Habib, N. ,Han, S. ,Lambert, N. ,Rajani, N. ,Sanseviero,
O.,Tunstall, L. andWolf, T. (2023a). Open llm leaderboard. https://huggingface.co/
spaces/HuggingFaceH4/open_llm_leaderboard .
Beeching, E. ,Fourrier, C. ,Habib, N. ,Han, S. ,Lambert, N. ,Rajani, N. ,Sanseviero, O. ,
Tunstall, L. andWolf, T. (2023b). Open llm leaderboard. Hugging Face .
Bradley, R. A. andTerry, M. E. (1952). Rank Analysis of Incomplete Block Designs: I. The
Method of Paired Comparisons. Biometrika 39324–345.
Chen, Z. ,Deng, Y. ,Yuan, H. ,Ji, K. andGu, Q. (2024). Self-play fine-tuning converts weak
language models to strong language models. arXiv preprint arXiv:2401.01335 .
Christiano, P. F. ,Leike, J. ,Brown, T. ,Martic, M. ,Legg, S. andAmodei, D. (2017).
Deep reinforcement learning from human preferences. Advances in neural information processing
systems 30.
Clark, P. ,Cowhey, I. ,Etzioni, O. ,Khot, T. ,Sabharwal, A. ,Schoenick, C. andTafjord,
O.(2018). Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv
preprint arXiv:1803.05457 .
Cobbe, K. ,Kosaraju, V. ,Bavarian, M. ,Chen, M. ,Jun, H. ,Kaiser, L. ,Plappert, M. ,
Tworek, J. ,Hilton, J. ,Nakano, R. et al. (2021). Training verifiers to solve math word
problems. arXiv preprint arXiv:2110.14168 .
Cui, G. ,Yuan, L. ,Ding, N. ,Yao, G. ,Zhu, W. ,Ni, Y. ,Xie, G. ,Liu, Z. andSun, M.
(2023). Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint
arXiv:2310.01377 .
Dubois, Y. ,Galambosi, B. ,Liang, P. andHashimoto, T. B. (2024a). Length-controlled
alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475 .
Dubois, Y. ,Li, C. X. ,Taori, R. ,Zhang, T. ,Gulrajani, I. ,Ba, J. ,Guestrin, C. ,Liang,
P. S. andHashimoto, T. B. (2024b). Alpacafarm: A simulation framework for methods that
learn from human feedback. Advances in Neural Information Processing Systems 36.
Dud´ık, M. ,Hofmann, K. ,Schapire, R. E. ,Slivkins, A. andZoghi, M. (2015). Contextual
dueling bandits. In Conference on Learning Theory . PMLR.
Ethayarajh, K. ,Xu, W. ,Muennighoff, N. ,Jurafsky, D. andKiela, D. (2024). Kto: Model
alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306 .
22
Freund, Y. andSchapire, R. E. (1999). Adaptive game playing using multiplicative weights.
Games and Economic Behavior 2979–103.
Gao, L. ,Schulman, J. andHilton, J. (2023). Scaling laws for reward model overoptimization.
InInternational Conference on Machine Learning . PMLR.
Haarnoja, T. ,Zhou, A. ,Abbeel, P. andLevine, S. (2018). Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International conference
on machine learning . PMLR.
He, P. ,Gao, J. andChen, W. (2021). Debertav3: Improving deberta using electra-style
pre-training with gradient-disentangled embedding sharing.
Hendrycks, D. ,Burns, C. ,Basart, S. ,Zou, A. ,Mazeika, M. ,Song, D. andSteinhardt, J.
(2020). Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 .
Hong, J. ,Lee, N. andThorne, J. (2024). Reference-free monolithic preference optimization with
odds ratio. arXiv preprint arXiv:2403.07691 .
Ji, K. ,He, J. andGu, Q. (2024). Reinforcement learning from human feedback with active queries.
arXiv preprint arXiv:2402.09401 .
Jiang, A. Q. ,Sablayrolles, A. ,Mensch, A. ,Bamford, C. ,Chaplot, D. S. ,Casas, D.
d. l. ,Bressand, F. ,Lengyel, G. ,Lample, G. ,Saulnier, L. et al. (2023a). Mistral 7b.
arXiv preprint arXiv:2310.06825 .
Jiang, D. ,Ren, X. andLin, B. Y. (2023b). Llm-blender: Ensembling large language models with
pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561 .
Li, J. ,Sun, S. ,Yuan, W. ,Fan, R.-Z. ,Zhao, H. andLiu, P. (2023a). Generative judge for
evaluating alignment. arXiv preprint arXiv:2310.05470 .
Li, X. ,Zhang, T. ,Dubois, Y. ,Taori, R. ,Gulrajani, I. ,Guestrin, C. ,Liang, P. and
Hashimoto, T. B. (2023b). Alpacaeval: An automatic evaluator of instruction-following models.
https://github.com/tatsu-lab/alpaca_eval .
Lin, S. ,Hilton, J. andEvans, O. (2021). Truthfulqa: Measuring how models mimic human
falsehoods. arXiv preprint arXiv:2109.07958 .
Liu, T. ,Zhao, Y. ,Joshi, R. ,Khalman, M. ,Saleh, M. ,Liu, P. J. andLiu, J. (2023).
Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657
.
Lou, H. ,Jin, T. ,Wu, Y. ,Xu, P. ,Gu, Q. andFarnoud, F. (2022). Active ranking without
strong stochastic transitivity. Advances in neural information processing systems .
Munos, R. ,Valko, M. ,Calandriello, D. ,Azar, M. G. ,Rowland, M. ,Guo, Z. D. ,Tang,
Y.,Geist, M. ,Mesnard, T. ,Michi, A. et al. (2023). Nash learning from human feedback.
arXiv preprint arXiv:2312.00886 .
23
OpenAI, J., Achiam ,Adler, S. ,Agarwal, S. ,Ahmad, L. ,Akkaya, I. ,Aleman, F. L. ,
Almeida, D. ,Altenschmidt, J. ,Altman, S. ,Anadkat, S. et al. (2023). Gpt-4 technical
report. arXiv preprint arXiv:2303.08774 .
Ouyang, L. ,Wu, J. ,Jiang, X. ,Almeida, D. ,Wainwright, C. ,Mishkin, P. ,Zhang, C. ,
Agarwal, S. ,Slama, K. ,Ray, A. et al. (2022). Training language models to follow instructions
with human feedback. Advances in Neural Information Processing Systems 3527730–27744.
Pal, A. ,Karkhanis, D. ,Dooley, S. ,Roberts, M. ,Naidu, S. andWhite, C. (2024). Smaug:
Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228
.
Rafailov, R. ,Sharma, A. ,Mitchell, E. ,Manning, C. D. ,Ermon, S. andFinn, C. (2024).
Direct preference optimization: Your language model is secretly a reward model. Advances in
Neural Information Processing Systems 36.
Rosset, C. ,Cheng, C.-A. ,Mitra, A. ,Santacroce, M. ,Awadallah, A. andXie, T. (2024).
Direct nash optimization: Teaching language models to self-improve with general preferences.
arXiv preprint arXiv:2404.03715 .
Sakaguchi, K. ,Bras, R. L. ,Bhagavatula, C. andChoi, Y. (2021). Winogrande: An
adversarial winograd schema challenge at scale. Communications of the ACM 6499–106.
Schulman, J. ,Wolski, F. ,Dhariwal, P. ,Radford, A. andKlimov, O. (2017). Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347 .
Singh, A. ,Co-Reyes, J. D. ,Agarwal, R. ,Anand, A. ,Patil, P. ,Liu, P. J. ,Harrison,
J.,Lee, J. ,Xu, K. ,Parisi, A. et al. (2023). Beyond human data: Scaling self-training for
problem-solving with language models. arXiv preprint arXiv:2312.06585 .
Swamy, G. ,Dann, C. ,Kidambi, R. ,Wu, Z. S. andAgarwal, A. (2024). A minimaximalist
approach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056 .
Thurstone, L. (1927). A law of comparative judgment. Psychological Review 34273.
Tversky, A. (1969). Intransitivity of preferences. Psychological review 7631.
Wang, Y. ,Liu, Q. andJin, C. (2024). Is rlhf more difficult than standard rl? a theoretical
perspective. Advances in Neural Information Processing Systems 36.
Wu, Y. ,Jin, T. ,Di, Q. ,Lou, H. ,Farnoud, F. andGu, Q. (2023). Borda regret minimization for
generalized linear dueling bandits. In ICML 2023 Workshop The Many Facets of Preference-Based
Learning .
Xiong, W. ,Dong, H. ,Ye, C. ,Zhong, H. ,Jiang, N. andZhang, T. (2023). Gibbs sampling from
human feedback: A provable kl-constrained framework for rlhf. arXiv preprint arXiv:2312.11456 .
Xu, J. ,Lee, A. ,Sukhbaatar, S. andWeston, J. (2023). Some things are more cringe than
others: Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682 .
24
Ye, C. ,Xiong, W. ,Zhang, Y. ,Jiang, N. andZhang, T. (2024). A theoretical analysis
of nash learning from human feedback under general kl-regularized preference. arXiv preprint
arXiv:2402.07314 .
Yuan, W. ,Pang, R. Y. ,Cho, K. ,Sukhbaatar, S. ,Xu, J. andWeston, J. (2024). Self-
rewarding language models. arXiv preprint arXiv:2401.10020 .
Zellers, R. ,Holtzman, A. ,Bisk, Y. ,Farhadi, A. andChoi, Y. (2019). Hellaswag: Can a
machine really finish your sentence? arXiv preprint arXiv:1905.07830 .
Zhao, Y. ,Joshi, R. ,Liu, T. ,Khalman, M. ,Saleh, M. andLiu, P. J. (2023). Slic-hf: Sequence
likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425 .
Zheng, L. ,Chiang, W.-L. ,Sheng, Y. ,Zhuang, S. ,Wu, Z. ,Zhuang, Y. ,Lin, Z. ,Li, Z. ,Li,
D.,Xing, E. et al. (2023). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances
in Neural Information Processing Systems 36.
Zheng, L. ,Chiang, W.-L. ,Sheng, Y. ,Zhuang, S. ,Wu, Z. ,Zhuang, Y. ,Lin, Z. ,Li, Z. ,Li,
D.,Xing, E. et al. (2024). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances
in Neural Information Processing Systems 36.
Zhu, B. ,Jiao, J. andJordan, M. I. (2023). Principled reinforcement learning with human
feedback from pairwise or k-wise comparisons. arXiv preprint arXiv:2301.11270 .
25