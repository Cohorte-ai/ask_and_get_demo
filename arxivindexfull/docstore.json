{"docstore/metadata": {"phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone": {"doc_hash": "1d73b83bbcb01ac20098e99e810ac3db9ac314ae75d7d3091e1f04330fc1cf6f"}, "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention": {"doc_hash": "2e0f0a65137957aa87b09912b7e4beee6c570c85d000c2836c629c317c737404"}, "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models": {"doc_hash": "ae4b74afb321dead2220dbf2ce32c7f9aca8886d0cfeea9547667981469ffd05"}, "attention_is_all_you_need": {"doc_hash": "b772de61c571b5c2b1faf24bb4153e24ceecb37d9c22944aee6be95e4b266c23"}, "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis": {"doc_hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2"}, "a302a713-ac42-4509-a9a2-f2bd388f1af8": {"doc_hash": "361783914fee3d238ef37d1a31293eb940cbf87a6f2c7f6be777c8496ca3d448", "ref_doc_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone"}, "4a0cd810-3214-4b9e-ae98-b09055cfd986": {"doc_hash": "5b79401d64308c3e34ac440ea11ecc452d4078cae1b076e3e121c3f0269119a8", "ref_doc_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone"}, "32379a21-97eb-4b9c-8ac0-7c21302a7315": {"doc_hash": "e48c7e414f4921199143fd10ce81c4752762a97f61e92039a642de2adab47fb2", "ref_doc_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone"}, "5874e3c8-e266-4293-8921-4d32ad651c11": {"doc_hash": "2e651b0a061a4eca7914ff3f37aaf30ed5aeb82218eafbeb5e2adf5558a048d1", "ref_doc_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone"}, "71ad8bf0-092e-40e2-a87f-87d6554e3432": {"doc_hash": "2d65f6d2e640bedce09a2e569d745b928e1ec6850d8bf12856db9c1b9253cd60", "ref_doc_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone"}, "e3a2b94c-ecc3-40fd-aac9-17f6dff3a2fe": {"doc_hash": "a1243e05bd919e898cef498d67177009d771c10aa5842cf1841cef7d6231f64f", "ref_doc_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone"}, "5e03aabd-d0dd-4efa-bab5-94c9b8d518c1": {"doc_hash": "a29d6dc061bb2d0c2c7558506761c8a932afa2c46f9562a835db593966a41216", "ref_doc_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone"}, "c2ab354c-d7f3-4668-83e7-455a01bdb1ea": {"doc_hash": "bb37ed5377745ee1c752ba1d28adb6ff4a83284174c585df6438363561a354d5", "ref_doc_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone"}, "902caf10-2e3b-4105-895e-8d080ceb145c": {"doc_hash": "ed0cd43682a37e3c08b2d1610ad51d2d17d0385ed0409494aa011d5e0d98e934", "ref_doc_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone"}, "ae0591ff-ca0a-4c02-966e-017fe1fef75c": {"doc_hash": "f23b054bb2d15e9ace99c95834936e2fd1ddf33c1ea53678dcdbb94234271f7c", "ref_doc_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone"}, "f9606dfa-42bd-48f3-bb6a-771d9c74274f": {"doc_hash": "5ca1638423184f57a5d2bd0a1941b3a204ea454ac39775d08be35de8508beeee", "ref_doc_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone"}, "f98839d3-cec4-4fe6-a6d9-04a5e7f25875": {"doc_hash": "f47e301d8cd58758a109449698350f162234cd411c7ea4e4533bcf40e53e7680", "ref_doc_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention"}, "34264931-bbb0-479c-9b81-e643fdeabce6": {"doc_hash": "2d52c7da11b18162d485b7a4131356403d9bdf95719008bfe1d10b63b7edd66c", "ref_doc_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention"}, "088f9899-8dba-4ada-bee4-7c8fc4ccb19f": {"doc_hash": "fd42aa5db32ac559c466632278354e5ba9f1be5bd6207f3a950aae9e0183adb1", "ref_doc_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention"}, "fce8519d-3e34-4885-a7a6-c4b2514bab06": {"doc_hash": "18c679e3ef971589fbbec7d0e7f0a087554b150a892c66325dea0cc405edd2de", "ref_doc_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention"}, "0f0476ca-26bc-4dc0-ac95-b80d959e0b60": {"doc_hash": "61a2361bda78926e2fca1f4671e3808e8da175c46eafe18d6137579f75b10b90", "ref_doc_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention"}, "f3f9f3fb-b8e0-471b-9430-886ad9ddfa16": {"doc_hash": "30cf349df77caed4dbf9aa7dfb1b3a2e568ac601ccc5e7d2bec41c5621707216", "ref_doc_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention"}, "a2d44ff4-3e3c-4523-9f9d-fe6a3d692572": {"doc_hash": "fa1a4a57cbf2436a2eba74ae8542e796bf1a55749e17c48f6153bff004db9559", "ref_doc_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention"}, "3a96d7f4-1a22-4c2f-9cea-7b46be1a7674": {"doc_hash": "3a199742379cfa9cd18c9d187a83595d50f2b4b1f702830708896238514b492d", "ref_doc_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention"}, "e5f96973-ca2b-4ace-b207-ee5e6c7b4a9d": {"doc_hash": "7d7a190f8f5f9b227a554e6c3b637b9dfcc51804fcc87f736e79e82e2af75ae7", "ref_doc_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention"}, "ea090aea-2a3c-408b-8190-945e25e739c6": {"doc_hash": "3ba6fe6a79ed019feb7a29d020d897b1066d134a36774cad0d3c267caf2d52ea", "ref_doc_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention"}, "5250accc-3425-42a3-906f-902217ca1440": {"doc_hash": "40b6ad7b4e183b08fe020026fb8418f69f46671d46f750a725fc0a5d792ebadb", "ref_doc_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention"}, "55e2c282-a495-4863-8042-234f76ab1f2e": {"doc_hash": "86fd17a0e10ea72a47bc1ad2c328c30253abb614d9447d51abbcd1576d8ef009", "ref_doc_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention"}, "cd94529f-38e1-4590-b98c-29b46afc1e3e": {"doc_hash": "83d316095ad3a6b0f8c2f6e7ad747212beefb79153dc365e5ddaaea162e0bde0", "ref_doc_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention"}, "cb44d17a-a68d-40e0-8ef7-d555bea3b23c": {"doc_hash": "bda92c04458ff581ce2c72c0c871a1a1c9284948eb761741a7e9d171ba73b554", "ref_doc_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention"}, "313cf5ae-84a6-4bb7-8779-e640ba4f8bde": {"doc_hash": "3f749da4522dc609b566d1d89c39bd5b151fe46ea4538721f25854e891e24051", "ref_doc_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models"}, "ab935868-42a1-451c-aae5-ea359f68442b": {"doc_hash": "f24ab077f5ff0b6cdb12ce07a7a3076358ce9900a9e2bce25c4b9574ded2e067", "ref_doc_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models"}, "f9daad84-c0c2-43c7-a8ec-a9bcb4d18198": {"doc_hash": "43dd7709f4b0979c7a003e53d697aa9f2342326a11ec595c611bbecacb5912cb", "ref_doc_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models"}, "68df46e3-3932-4d86-b538-cf3fa32c80b1": {"doc_hash": "266fd9986c1f91983543874f0c0b10698960aa0e4cc327bd7629dd97fd0988a7", "ref_doc_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models"}, "abe64ea2-e9c7-4afe-89a7-0224c679db57": {"doc_hash": "bffd8458b931a104fc40993ef8632c92e81f41dc4373d1d3a4fdf18f9ccbbb05", "ref_doc_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models"}, "d247bf45-ed8f-4e83-98d4-8f5cbb4af2b6": {"doc_hash": "f8edd89538686e21fbef0f8a29baf4225962d8944382379cc376af46abd4b0ec", "ref_doc_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models"}, "8f3aff55-9de9-4d75-9a37-2df328337463": {"doc_hash": "e240452c382b979a6355cbc29dc52cec4d9008759ce1c45bb6edce75fb1819b9", "ref_doc_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models"}, "775b688a-1f18-4cc4-bb47-acc5492557e2": {"doc_hash": "b2aa7a1ba3e7e377e386f19de291206ffc4cd429e260833c51030a2a20d4a027", "ref_doc_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models"}, "cc093955-24e9-48e7-91bd-134fc32fdf10": {"doc_hash": "bbe9e44bd4dc29001a0ddf7d9a067105b6da8ea4285931a60c25bd24ae79142f", "ref_doc_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models"}, "bdf9cd3e-f264-477b-8b6d-722054c50b74": {"doc_hash": "f091d7a36da781012e86509246f2ccb95cb68ffbfdf3dfa9e141403f600f1324", "ref_doc_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models"}, "e3af10ba-05e8-4289-b39f-9c0b5a85bafa": {"doc_hash": "1c7499a0c18ac1d6ad86ea4f66b8ca419d1f419f837cc53731c6e96e1b61d5f6", "ref_doc_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models"}, "71b3447b-d1c9-4ebe-bda1-5927740a15e2": {"doc_hash": "81415e4179ff4ddeca96ecc4ef3e616868fa33fecbb735b3f89640b7466f7a0d", "ref_doc_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models"}, "62116594-a9ba-407a-bfb9-6339b47cddc3": {"doc_hash": "6a1470c8d31c39ca25688c8a98bab04801ace7a6c1b046a9c4464be468c5fc9f", "ref_doc_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models"}, "17656648-675c-4e4c-9f74-f8a1898a0e13": {"doc_hash": "b540b7618b72c2d5c2e9828b3c7d348a015d0b77a38f23bfb25dde1d05342a4b", "ref_doc_id": "attention_is_all_you_need"}, "46161918-fde5-404b-a93f-cd7f392f1a8f": {"doc_hash": "c4dcc0c89fcd37396d8af135a4d0e20ca519c58b8bef488bfe9a0be7993e90d2", "ref_doc_id": "attention_is_all_you_need"}, "591ef5c2-b978-4a27-91a1-c337d874dd70": {"doc_hash": "4073babf1adfd6e21c694aac2763b3e846092bd3ff78be579d9b41eb62eb6ba1", "ref_doc_id": "attention_is_all_you_need"}, "76890a72-4f06-4667-acc0-76843cc527a1": {"doc_hash": "6607e6097cb3be5a37ca3d59ca9a55c5f1ed73ebd953d581d9aa9802c14d84e8", "ref_doc_id": "attention_is_all_you_need"}, "f24a72f4-a72a-45c6-9a50-8a8d05513a0b": {"doc_hash": "a4ef224a9a8a7b1841d0c3e7698a4cf8457f44de2662402d286c3089038e7c79", "ref_doc_id": "attention_is_all_you_need"}, "c5caa6f1-858b-4b0e-9d43-50307d4c33a0": {"doc_hash": "5a3cc399b273c4527d48d1922cd01ad1262abe4a93d5dc246fed4f30989e5005", "ref_doc_id": "attention_is_all_you_need"}, "3d82c73e-304a-4f3a-80c2-3e64487ad704": {"doc_hash": "1c94958f343223867aeaa953fd5f298b0aaf001cbaa2076932ef9c30c1242161", "ref_doc_id": "attention_is_all_you_need"}, "2b5a090b-5488-4693-b018-fc85db5e5597": {"doc_hash": "7566ece69353ec42975976dbf40d9d4d8476880a1b31a9aba24e71a1190660ec", "ref_doc_id": "attention_is_all_you_need"}, "a6ae20ed-1773-4a80-bc6e-bcc837a7ea87": {"doc_hash": "a22111e1713eddae4fd60bca8e57fd6ad4c4322e0d5a8d588bda37548492de45", "ref_doc_id": "attention_is_all_you_need"}, "6fe8244d-d01a-4367-bef8-6bf6b29496ee": {"doc_hash": "2bff995149fdd1f0594964f25cc71dfa14fa9ac0c6b993e755abd456a24b0a8d", "ref_doc_id": "attention_is_all_you_need"}, "0296c0e0-2a66-4aed-986c-a3fa62c6a5fd": {"doc_hash": "77527360d848306d181c6d45667d8d666e73f49ac5bb376cd3757524706caca2", "ref_doc_id": "attention_is_all_you_need"}, "437d6809-94e8-4638-82ec-dadd0ac2e706": {"doc_hash": "996d68c0cd6e7bfc99fdc4875800c93cd352c25affd6b8372239cabb3b2c2052", "ref_doc_id": "attention_is_all_you_need"}, "010b42cb-6cde-4e2e-b2c1-87a26bfd755d": {"doc_hash": "f2a4276202495a83d892f88ab10a8ac0966005449e5217eceb1c46f961e34916", "ref_doc_id": "attention_is_all_you_need"}, "5ca433ae-99b9-4a01-9613-f422ed83191c": {"doc_hash": "01131f84fa2c6b97b9ec3e27ee036f47c4b5f70b3076c9535a9c0a3de79e5373", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "45092251-c78b-4dfb-9f4a-4aea28b74238": {"doc_hash": "24c65485a3001c67cda002588026e62ce874a0a976af234515c888feb54d194d", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "abfc959e-5184-4b04-acdb-d1672f33d50a": {"doc_hash": "249e92cd649ae6fc64897b4321eb6327cb58442de9b70a0377bbb664feb25b48", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "07734fec-4445-4ee1-a66a-55c466de346a": {"doc_hash": "b9a52080f3f3901b557cc553d44456c7fe5e7cc9bd4c7f9fd97f2058af3b72df", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "d0111e1a-f9e8-4076-8946-f00e4ccf83ab": {"doc_hash": "c2687d907680e761b36accb8f77f4ea9ed6f0afc569d05308760dfd7abe0030d", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "09a30549-613c-4f86-96a2-0e62e5467107": {"doc_hash": "603ac9fd30bb224d726042afc0262d807bea02aa4e1b89e5586f61b07558551b", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "9e994813-dccd-4900-a4a1-62866edcb020": {"doc_hash": "487faef632419ae531efefbb94273d4a394c09a8ce2bda314ac14afd9bcf6d7b", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "7c8b2255-e058-4df9-92e0-dad2ddea4bb9": {"doc_hash": "f7168d67e205fe4385f883fe0f4e35bef0687aa7b168ce3cd17ec99a5d339cea", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "fc800e59-e0fb-4a83-a51c-395bc166a0e1": {"doc_hash": "ec70e209ded7de388da962482f6564c027f5ca78a4b47e584dca720d774f85b6", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "1fe02bce-2d9b-4b42-9962-5b96a155f989": {"doc_hash": "67172051b31db45c80620bdd3ee3cc0a6ee83ce6925e4d189c2687da1986d6a3", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "9f277803-acb7-4984-b710-677e4dbbd209": {"doc_hash": "daebe9ca6bba8f85c0d7890a50bced1676e5df6f0e05700e101fd918a76c0582", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "06df1f32-e75d-4d2e-b0d2-d9aed2263ec6": {"doc_hash": "cb57a3fe0713aa062e8fe1cc7ef8505d0ce58e3be189ff8caf33b8fe57d09edb", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "fabc6f2b-20bf-4367-a65f-63e6abe5e6ff": {"doc_hash": "abd19473266f7fd041d384abd699eab006be2d98bd5115be74f4b5eba0c06c4c", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "0cec7fa9-c358-476f-8495-f98cc293777e": {"doc_hash": "7c59a724ea4d8f696dcdb39f04a039daef7d54412aad903d3df5235ee983d94d", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "1d0e5eaf-c9ea-43a3-a15b-2ee649a044d5": {"doc_hash": "b8083d7ad52ee4ba55ea4e65ce4413e271a0851f4677a44dd75e7b6de6d57e10", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "a81bd2d8-339d-4124-b17e-96cfd43be695": {"doc_hash": "e72bcab966d31bcb6c2beb9c292fbb3f5ec05cb72e261f93a4993f602dbebdc2", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "80998fca-7d7f-4e33-bc97-02456a449d2c": {"doc_hash": "57eaf9c1586749287b735611f9d7cad4dad6e71b44c72ee4d60acb1acc9c61b8", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "8071f620-4c3b-41e9-ae64-a5a953a57d3e": {"doc_hash": "b0426196872a6ce0115342c5fa25e32d63264de6f3e0603aaeb0e72b2059f4eb", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "704047b1-a36a-4da0-a97c-513c568cc5ca": {"doc_hash": "27b1a99349e83786ab54c0e3b3feca46ff82756fb84d4eaa915b40c6cf4a7296", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "e8da4888-0c49-4b85-8b23-57e608cf4c58": {"doc_hash": "fa046ea3da1039a388e2b2ea65fe9a07946373f9256e3c90a952b80431f6acee", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "78b30729-344e-4468-b07a-ee14486603ca": {"doc_hash": "4b807d0d5a907eb81a749347b10b2adfcb0c222d6512568f67c747be3c2d7280", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "c4ac1324-33eb-4290-b37e-3ec45952c912": {"doc_hash": "7fa4601a5b34aec34e95b93fe9a3ec4add4c9d0b9100cd45cde1dc91d65198f8", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "f02a42c7-5a6f-4088-bff2-f8b2bd12ccb5": {"doc_hash": "140165ba7d8fb2f89445d7901ad0634c570e6a46e1dc8807201b110bfaecfc4c", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "a6cec9fc-118a-4b28-a706-fa7355bfdc54": {"doc_hash": "4f2947a68f35f603b593de26099569b20689c6973ffc68dc5b2eb5fbaab05768", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "39a40bea-67f5-48e9-8b8a-575e98e7563c": {"doc_hash": "61df971fd15235671f988dc103cbca3bd9f16efa3d3de655ba9967b72e07bb1e", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "60ab30f6-200c-444a-98fc-3f616dc39add": {"doc_hash": "7a0871bf792d728b39cc2e625ffb9f36d7a46cc9efdd3f1ba9b6bc890b42beaa", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "886891de-9050-4153-aafe-558e6c4c9775": {"doc_hash": "8874be322d969f7a34980861cdfbc06d356b86826d539bb2913d2e7882282188", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "faa4de4b-018e-40ef-9903-1bf60f3e1bc2": {"doc_hash": "cfb45ea9fd3ce3f9ba8fea6b0a25c5605677673d1fa5142e6356a4a9a2d3d34b", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "e873e7b1-a0f9-49ba-9e24-7a06cb02b613": {"doc_hash": "ffd1a38db7b978664a8c530dfe5a8613528875deada4caeb99fedda504451ae0", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "f594f6e8-e041-4263-85de-82bce159084b": {"doc_hash": "1264b52bc3d0e9a44da2e9f06bc56c3580863fbaf7dff738537b9ba034a2f410", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "59ba10f2-f256-4413-995b-5450da642bff": {"doc_hash": "d8a2f4cbae3e99c00cdbdc349eaa1c9eda276b25679ea5e0b2069e6b4242cfed", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "264c2ab9-95f1-4ad7-a464-05d748cb6e21": {"doc_hash": "5fcc261b7d5b573c0cbf9b24ce6a7c77bd4b84e6ca3f8f443a90821c03d22ab4", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "62f0efd7-058f-4834-8c4d-9354cb911292": {"doc_hash": "a6550c3908ce77fe4ab81391796f1ba3a143b27be96cd1608ffb507cb1cb1465", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "c418a385-7d69-425d-84ff-b3b083c362b8": {"doc_hash": "9f9454b27b92d52eb9e66cc0c67c0bec689251697ac3623af7e1548fdc47f972", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}, "b00647ef-3325-460c-ba49-6f9721f9549b": {"doc_hash": "aaee28bed02b94843bf4f04c5fa980626c5f824e76cfdf93e2b461a0191e2468", "ref_doc_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone"}, "c623a196-b15f-4618-831b-a4d7d42cbf5b": {"doc_hash": "c7ed1771c4823b6b56360e43e7779191945919fa093e8bcc7519cca7e9329d44", "ref_doc_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention"}, "600d818b-aa3e-465d-9e14-1aa896fd173d": {"doc_hash": "3a0a54e317a910ae9d6d84c2d47d096a5ef52153d0d1a2b7f43dc9ef0df120ae", "ref_doc_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models"}, "57e6e1e6-8244-4476-a9d2-91bf029192ce": {"doc_hash": "255eebcade556acca8b547284672cc2870e172dfaf067ad0dd7be069804aea63", "ref_doc_id": "attention_is_all_you_need"}, "71b6e97e-ce67-4bbd-a1a6-04735127a41c": {"doc_hash": "8a753d11bd32cf6f38a5d0942e643f223421bb1a95741cde4c8943db591c3027", "ref_doc_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis"}}, "docstore/data": {"a302a713-ac42-4509-a9a2-f2bd388f1af8": {"__data__": {"id_": "a302a713-ac42-4509-a9a2-f2bd388f1af8", "embedding": null, "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone", "node_type": "4", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "1d73b83bbcb01ac20098e99e810ac3db9ac314ae75d7d3091e1f04330fc1cf6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a0cd810-3214-4b9e-ae98-b09055cfd986", "node_type": "1", "metadata": {}, "hash": "74b4e7a982b50bb00de009074f51cf91fcbf382721505952cba599bf1b7da472", "class_name": "RelatedNodeInfo"}}, "text": "Phi-3 Technical Report:\nA Highly Capable Language Model Locally on Your Phone\nMicrosoft\nAbstract\nWe introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens,\nwhose overall performance, as measured by both academic benchmarks and internal testing, rivals\nthat of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38\non MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in\nour dataset for training, a scaled-up version of the one used for phi-2 , composed of heavily filtered\nweb data and synthetic data. The model is also further aligned for robustness, safety, and chat format.\nWe also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T\ntokens, called phi-3-small andphi-3-medium , both significantly more capable than phi-3-mini\n(e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).\n1 Introduction\nThe striking progress of AI in the last few years can be largely attributed to major efforts through-\nout the world towards scaling-up to ever-larger models and datasets. Large Language Models (LLMs)\nhave steadily increased in size from a mere billion parameters just five years ago (GPT-2 had 1.5 bil-\nlion parameters [RWC+19]) to trillion parameters today. The impetus for this effort originates in the\nseemingly predictable improvement one obtains by training large models, the so-called scaling laws\n[KMH+20, HBM+22, MRB+23]. However these laws assume a \u201cfixed\u201d data source. This assumption\nis now significantly disrupted by the existence of frontier LLMs themselves, which allow us to interact\nwith data in novel ways. In our previous works on the phi models [GZA+23, LBE+23, JBA+23] it was\nshown that a combination of LLM-based filtering of web data, and LLM-created synthetic data, enable\nperformance in smaller language models that were typically seen only in much larger models. For exam-\nple our previous model trained on this data recipe, phi-2 (2.7B parameters), matched the performance\nof models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini\n(3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets used in\nphi-2 . With its small size, phi-3-mini can easily be inferenced locally on a modern phone (see Figure\n1), yet it achieves a quality that seems on-par with models such as Mixtral 8x7B [JSR+24] and GPT-3.5.\nUser: Explain why it is surprising that one can build a language model small enough to\nfit on a phone, yet almost as powerful as ChatGPT. Just use one funny sentence.\nphi-3-mini: It\u2019s like fitting a supercomputer in a flip phone, but instead of breaking\nthe phone, it just breaks the internet with its tiny, yet mighty, linguistic prowess!\nUser: Okay now more serious answer, and note that this was achieved solely by changing\nthe training data.\nphi-3-mini: The development of a compact language model that rivals the capabilities\nof ChatGPT, while fitting on a phone, is a testament to the power of data-driven\nmachine learning. By meticulously curating and optimizing the training dataset,\nresearchers can significantly reduce the model\u2019s size without compromising its\nperformance. [...]\n1arXiv:2404.14219v1  [cs.CL]  22 Apr 2024\n2 Technical Specifications\nThephi-3-mini model is a transformer decoder architecture [VSP+17], with default context length 4 K.\nWe also introduce a long context version via LongRope [DZZ+24] that extends the context length to\n128K, called phi-3-mini-128K .\nTo best benefit the open source community, phi-3-mini is built upon a similar block structure as\nLlama-2 [TLI+23] and uses the same tokenizer with vocabulary size of 320641. This means that all\npackages developed for Llama-2 family of models can be directly adapted to phi-3-mini . The model\nuses 3072 hidden dimension, 32 heads and 32 layers.", "start_char_idx": 0, "end_char_idx": 3911, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a0cd810-3214-4b9e-ae98-b09055cfd986": {"__data__": {"id_": "4a0cd810-3214-4b9e-ae98-b09055cfd986", "embedding": null, "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone", "node_type": "4", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "1d73b83bbcb01ac20098e99e810ac3db9ac314ae75d7d3091e1f04330fc1cf6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a302a713-ac42-4509-a9a2-f2bd388f1af8", "node_type": "1", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "361783914fee3d238ef37d1a31293eb940cbf87a6f2c7f6be777c8496ca3d448", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32379a21-97eb-4b9c-8ac0-7c21302a7315", "node_type": "1", "metadata": {}, "hash": "5dae993ca6c56dbe4f79cc39d910e8b56badebeaf699a761efc3a0f13391f8f4", "class_name": "RelatedNodeInfo"}}, "text": "[...]\n1arXiv:2404.14219v1  [cs.CL]  22 Apr 2024\n2 Technical Specifications\nThephi-3-mini model is a transformer decoder architecture [VSP+17], with default context length 4 K.\nWe also introduce a long context version via LongRope [DZZ+24] that extends the context length to\n128K, called phi-3-mini-128K .\nTo best benefit the open source community, phi-3-mini is built upon a similar block structure as\nLlama-2 [TLI+23] and uses the same tokenizer with vocabulary size of 320641. This means that all\npackages developed for Llama-2 family of models can be directly adapted to phi-3-mini . The model\nuses 3072 hidden dimension, 32 heads and 32 layers. We trained using bfloat16 for a total of 3.3T tokens.\nThe model is already chat-finetuned, and the chat template is as follows:\n<|user|> /n Question <|end|> /n <|assistant|>\nThephi-3-small model (7B parameters) leverages the tiktoken tokenizer (for better multilingual to-\nkenization) with a vocabulary size of 100352 and has default context length 8 K. It follows the standard\ndecoder architecture of a 7B model class, having 32 layers and a hidden size of 4096. To minimize KV\ncache footprint, the model also leverages a grouped-query attention, with 4 queries sharing 1 key. More-\noverphi-3-small uses alternative layers of dense attention and a novel blocksparse attention to further\noptimize on KV cache savings while maintaining long context retrieval performance. An additional 10%\nmultilingual data was also used for this model.\nHighly capable language model running locally on a cell-phone. Thanks to its small size, phi-\n3-mini can be quantized to 4-bits so that it only occupies \u22481.8GB of memory. We tested the quantized\nmodel by deploying phi-3-mini on iPhone 14 with A16 Bionic chip running natively on-device and fully\noffline achieving more than 12 tokens per second.\nTraining Methodology. We follow the sequence of works initiated in \u201cTextbooks Are All You\nNeed\u201d [GZA+23], which utilize high quality training data to improve the performance of small language\nmodels and deviate from the standard scaling-laws . In this work we show that such method allows to\nreach the level of highly capable models such as GPT-3.5 or Mixtral with only 3.8B total parameters\n(while Mixtral has 45B total parameters for example). Our training data of consists of heavily filtered\nweb data (according to the \u201ceducational level\u201d) from various open internet sources, as well as synthetic\nLLM-generated data. Pre-training is performed in two disjoint and sequential phases; phase-1 comprises\nmostly of web sources aimed at teaching the model general knowledge and language understanding.\nPhase-2 merges even more heavily filtered webdata (a subset used in Phase-1) with some synthetic data\nthat teach the model logical reasoning and various niche skills.\nData Optimal Regime. Unlike prior works that train language models in either \u201ccompute optimal\nregime\u201d [HBM+22] or \u201cover-train regime\u201d, we mainly focus on the quality of data for a given scale .2\nWe try to calibrate the training data to be closer to the \u201cdata optimal\u201d regime for small models. In\nparticular, we filter the web data to contain the correct level of \u201cknowledge\u201d and keep more web pages\nthat could potentially improve the \u201creasoning ability\u201d for the model. As an example, the result of a\ngame in premier league in a particular day might be good training data for frontier models, but we need\nto remove such information to leave more model capacity for \u201creasoning\u201d for the mini size models. We\ncompare our approach with Llama-2 in Figure 2.\n1We remove BoS tokens and add some additional tokens for chat template.\n2Just like for \u201ccompute optimal regime\u201d, we use the term \u201coptimal\u201d in an aspirational sense for \u201cdata optimal regime\u201d.\nWe are not implying that we actually found the provably \u201coptimal\u201d data mixture for a given scale.\n2\nFigure 1: 4-bit quantized phi-3-mini running natively on an iPhone with A16 Bionic chip, generating over 12\ntokens per second.", "start_char_idx": 3263, "end_char_idx": 7231, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32379a21-97eb-4b9c-8ac0-7c21302a7315": {"__data__": {"id_": "32379a21-97eb-4b9c-8ac0-7c21302a7315", "embedding": null, "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone", "node_type": "4", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "1d73b83bbcb01ac20098e99e810ac3db9ac314ae75d7d3091e1f04330fc1cf6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a0cd810-3214-4b9e-ae98-b09055cfd986", "node_type": "1", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "5b79401d64308c3e34ac440ea11ecc452d4078cae1b076e3e121c3f0269119a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5874e3c8-e266-4293-8921-4d32ad651c11", "node_type": "1", "metadata": {}, "hash": "40e5de5668283189b4a5612d7361ddc4d7f951cf2b933828b06ea9d01608f21a", "class_name": "RelatedNodeInfo"}}, "text": "As an example, the result of a\ngame in premier league in a particular day might be good training data for frontier models, but we need\nto remove such information to leave more model capacity for \u201creasoning\u201d for the mini size models. We\ncompare our approach with Llama-2 in Figure 2.\n1We remove BoS tokens and add some additional tokens for chat template.\n2Just like for \u201ccompute optimal regime\u201d, we use the term \u201coptimal\u201d in an aspirational sense for \u201cdata optimal regime\u201d.\nWe are not implying that we actually found the provably \u201coptimal\u201d data mixture for a given scale.\n2\nFigure 1: 4-bit quantized phi-3-mini running natively on an iPhone with A16 Bionic chip, generating over 12\ntokens per second.\nFigure 2: Scaling law close to the \u201cData Optimal Regime\u201d (from left to right: phi-1.5, phi-2, phi-3-mini, phi-3-\nsmall) versus Llama-2 family of models (7B, 13B, 34B, 70B) that were trained on the same fixed data. We plot\nthe log of MMLU error versus the log of model size.\n3\nTo test our data on larger size of models, we also trained phi-3-medium , a model with 14B pa-\nrameters using the same tokenizer and architecture of phi-3-mini , and trained on the same data for\nslightly more epochs (4.8T tokens total as for phi-3-small ). The model has 40 heads and 40 layers,\nwith embedding dimension 5120. We observe that some benchmarks improve much less from 7B to 14B\nthan they do from 3.8B to 7B, perhaps indicating that our data mixture needs further work to be in\nthe \u201cdata optimal regime\u201d for 14B parameters model. We are still actively investigating some of those\nbenchmarks (including a regression on HumanEval), hence the numbers for phi-3-medium should be\nconsidered as a \u201cpreview\u201d.\nPost-training. Our models went through post-training with both supervised instruction fine-tuning,\nand preference tuning with DPO. We have worked on generating and curating various instruction and\npreference data. This has improved the model chat capabilities, robustness, as well as its safety.\n3 Academic benchmarks\nOn the next page we report the results for phi-3-mini on standard open-source benchmarks measuring\nthe model\u2019s reasoning ability (both common sense reasoning and logical reasoning). We compare to phi-2\n[JBA+23], Mistral-7b-v0.1 [JSM+23], Mixtral-8x7b [JSR+24], Gemma 7B [TMH+24], Llama-3-instruct-\n8b [AI23], and GPT-3.5. All the reported numbers are produced with the exact same pipeline to ensure\nthat the numbers are comparable. These numbers might differ from other published numbers due to\nslightly different choices in the evaluation. As is now standard, we use few-shot prompts to evaluate\nthe models, at temperature 0. The prompts and number of shots are part of a Microsoft internal tool\nto evaluate language models, and in particular we did no optimization to the pipeline for the phi-3\nmodels.3The number of k\u2013shot examples is listed per-benchmark. An example of a 2-shot prompt is\ndescribed in Appendix A.\n3For example, we found that using ## before the Question can lead to a noticeable improvement to phi-3-mini \u2019s\nresults across many benchmarks, but we did not do such changes in the prompts.\n4\nPhi-3-mini\n3.8bPhi-3-small\n7bPhi-3-medium\n14b (preview)Phi-2\n2.7bMistral\n7bGemma\n7bLlama-3-In\n8bMixtral\n8x7bGPT-3.5\nversion 1106\nMMLU\n(5-Shot) [HBK+21]68.8 75.3 78.2 56.3 61.7 63.6 66.0 68.4 71.4\nHellaSwag\n(5-Shot) [ZHB+19]76.7 78.7 83.0 53.6 58.5 49.8 69.5 70.4 78.8\nANLI\n(7-Shot) [NWD+20]52.8 55.0 58.", "start_char_idx": 6531, "end_char_idx": 9953, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5874e3c8-e266-4293-8921-4d32ad651c11": {"__data__": {"id_": "5874e3c8-e266-4293-8921-4d32ad651c11", "embedding": null, "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone", "node_type": "4", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "1d73b83bbcb01ac20098e99e810ac3db9ac314ae75d7d3091e1f04330fc1cf6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32379a21-97eb-4b9c-8ac0-7c21302a7315", "node_type": "1", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "e48c7e414f4921199143fd10ce81c4752762a97f61e92039a642de2adab47fb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71ad8bf0-092e-40e2-a87f-87d6554e3432", "node_type": "1", "metadata": {}, "hash": "dbfffc2102212d126142c3153f2e3d8d13963b97920f93494c3aac5ae44a7276", "class_name": "RelatedNodeInfo"}}, "text": "8bPhi-3-small\n7bPhi-3-medium\n14b (preview)Phi-2\n2.7bMistral\n7bGemma\n7bLlama-3-In\n8bMixtral\n8x7bGPT-3.5\nversion 1106\nMMLU\n(5-Shot) [HBK+21]68.8 75.3 78.2 56.3 61.7 63.6 66.0 68.4 71.4\nHellaSwag\n(5-Shot) [ZHB+19]76.7 78.7 83.0 53.6 58.5 49.8 69.5 70.4 78.8\nANLI\n(7-Shot) [NWD+20]52.8 55.0 58.7 42.5 47.1 48.7 54.8 55.2 58.1\nGSM-8K\n(0-Shot; CoT) [CKB+21]82.5 88.9 90.3 61.1 46.4 59.8 77.4 64.7 78.1\nMedQA\n(2-Shot) [JPO+20]53.8 58.2 69.4 40.9 49.6 50.0 58.9 62.2 63.4\nAGIEval\n(0-Shot) [ZCG+23]37.5 45.0 48.4 29.8 35.1 42.1 42.0 45.2 48.4\nTriviaQA\n(5-Shot) [JCWZ17]64.0 59.1 75.6 45.2 72.3 75.2 73.6 82.2 85.8\nArc-C\n(10-Shot) [CCE+18]84.9 90.7 91.0 75.9 78.6 78.3 80.5 87.3 87.4\nArc-E\n(10-Shot) [CCE+18]94.6 97.1 97.8 88.5 90.6 91.4 92.3 95.6 96.3\nPIQA\n(5-Shot) [BZGC19]84.2 87.8 87.7 60.2 77.7 78.1 77.1 86.0 86.6\nSociQA\n(5-Shot) [BZGC19]76.6 79.0 80.2 68.3 74.6 65.5 73.2 75.9 68.3\nBigBench-Hard\n(0-Shot) [SRR+22, SSS+22]71.7 75.0 81.3 59.4 57.3 59.6 68.9 69.7 68.32\nWinoGrande\n(5-Shot) [SLBBC19]70.8 82.5 81.4 54.7 54.2 55.6 58.0 62.0 68.8\nOpenBookQA\n(10-Shot) [MCKS18]83.2 88.4 87.2 73.6 79.8 78.6 81.6 85.8 86.0\nBoolQ\n(0-Shot) [CLC+19]77.2 82.9 86.6 \u2013 72.2 66.0 78.3 77.6 79.1\nCommonSenseQA\n(10-Shot) [THLB19]80.2 80.3 82.6 69.3 72.6 76.2 73.6 78.1 79.6\nTruthfulQA\n(10-Shot) [LHE22]65.0 68.7 75.7 \u2013 52.1 53.0 62.0 60.1 85.8\nHumanEval\n(0-Shot) [CTJ+21]58.5 59.1 55.5 59.0 28.0 34.1 38.4 37.8 62.2\nMBPP\n(3-Shot) [AON+21]70.", "start_char_idx": 9663, "end_char_idx": 11084, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71ad8bf0-092e-40e2-a87f-87d6554e3432": {"__data__": {"id_": "71ad8bf0-092e-40e2-a87f-87d6554e3432", "embedding": null, "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone", "node_type": "4", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "1d73b83bbcb01ac20098e99e810ac3db9ac314ae75d7d3091e1f04330fc1cf6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5874e3c8-e266-4293-8921-4d32ad651c11", "node_type": "1", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2e651b0a061a4eca7914ff3f37aaf30ed5aeb82218eafbeb5e2adf5558a048d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3a2b94c-ecc3-40fd-aac9-17f6dff3a2fe", "node_type": "1", "metadata": {}, "hash": "0f99ca9d1307e5a43cbb0599d33c7baf29653cff2a272e8714d67faaf657a5b9", "class_name": "RelatedNodeInfo"}}, "text": "2 82.9 86.6 \u2013 72.2 66.0 78.3 77.6 79.1\nCommonSenseQA\n(10-Shot) [THLB19]80.2 80.3 82.6 69.3 72.6 76.2 73.6 78.1 79.6\nTruthfulQA\n(10-Shot) [LHE22]65.0 68.7 75.7 \u2013 52.1 53.0 62.0 60.1 85.8\nHumanEval\n(0-Shot) [CTJ+21]58.5 59.1 55.5 59.0 28.0 34.1 38.4 37.8 62.2\nMBPP\n(3-Shot) [AON+21]70.0 71.4 74.5 60.6 50.8 51.5 65.3 60.2 77.8\nAverage 71.2 74.9 78.2 \u2013 61.0 62.0 68.0 69.9 75.3\nGPQA\n(2-Shot; CoT) [RHS+23]32.8 34.3 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 29.0\nMT Bench\n(2 round ave.) [ZCS+23]8.38 8.70 8.91 \u2013 \u2013 \u2013 \u2013 \u2013 8.35\n4 Safety\nPhi-3-mini was developed in accordance with Microsoft\u2019s responsible AI principles. The overall ap-\nproach consisted of safety alignment in post-training, red-teaming, automated testing and evaluations\nacross dozens of RAI harm categories. Helpfulness and harmlessness preference datasets [BJN+22,\nJLD+23] with modifications inspired by [BSA+24] and multiple in-house generated datasets were lever-\naged to address the RAI harm categories in safety post-training. An independent red team at Microsoft\niteratively examined phi-3-mini to further identify areas of improvement during the post-training pro-\ncess. Based on their feedback, we curated additional datasets tailored to address their insights, thereby\nrefining the post-training dataset. This process resulted in significant decrease of harmful response rates,\n5\nPhi-3-Mini-4k\n3.8bPhi-3-Mini-128k\n3.8bPhi-2\n2.7bMistral\n7bGemma\n7bLlama-3-In\n8b\nUngroundedness 0.603 0.637 1.481 0.935 0.679 0.328\nIntellectual Property (DR-1) 23.95% 21.50% 24.00% 56.20% 38.33% 37.30%\nHarmful Content Continuation (DR-3) 0.75% 1.08% 2.93% 2.58% 1.28% 1.30%\nHarmful Content Summarization (DR-3) 10.00% 10.20% 14.35% 22.33% 10.33% 8.20%\nJailbreak (DR-1) 12.29% 12.57% 15.00% 15.57% 11.43% 13.00%\nTable 1: Comparison of Microsoft internal multi-turn conversation RAI benchmark results of phi-3-mini and\nother models. Note that a lower value indicates a better performance for all metrics in the table.\nas shown in Figure 3.\nFigure 3: Comparison of harmful response percentages by Microsoft AI Red Team between phi-3-mini before\nand after the safety alignment. Note that the harmful response percentages in this chart are inflated numbers as\nthe red team tried to induce phi-3-mini in an adversarial way to generate harmful responses through multi-turn\nconversations.\nTable 1 shows the results of in-house RAI benchmarks for phi-3-mini-4k andphi-3-mini-128k\ncompared to phi-2 [JBA+23], Mistral-7b-v0.1 [JSM+23], Gemma 7b [TMH+24]. This benchmark utilized\nGPT-4 to simulate multi-turn conversations in five different categories and to evaluate the model re-\nsponses. Ungroundedness between 0 (fully grounded) and 4 (not grounded) measures if the information\nin a response is based on a given prompt.", "start_char_idx": 10801, "end_char_idx": 13532, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3a2b94c-ecc3-40fd-aac9-17f6dff3a2fe": {"__data__": {"id_": "e3a2b94c-ecc3-40fd-aac9-17f6dff3a2fe", "embedding": null, "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone", "node_type": "4", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "1d73b83bbcb01ac20098e99e810ac3db9ac314ae75d7d3091e1f04330fc1cf6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71ad8bf0-092e-40e2-a87f-87d6554e3432", "node_type": "1", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2d65f6d2e640bedce09a2e569d745b928e1ec6850d8bf12856db9c1b9253cd60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e03aabd-d0dd-4efa-bab5-94c9b8d518c1", "node_type": "1", "metadata": {}, "hash": "64e4a3d2431ab4db9db8a159ec17442b8ecd5685f43f49331e806ca804bcb2a3", "class_name": "RelatedNodeInfo"}}, "text": "Figure 3: Comparison of harmful response percentages by Microsoft AI Red Team between phi-3-mini before\nand after the safety alignment. Note that the harmful response percentages in this chart are inflated numbers as\nthe red team tried to induce phi-3-mini in an adversarial way to generate harmful responses through multi-turn\nconversations.\nTable 1 shows the results of in-house RAI benchmarks for phi-3-mini-4k andphi-3-mini-128k\ncompared to phi-2 [JBA+23], Mistral-7b-v0.1 [JSM+23], Gemma 7b [TMH+24]. This benchmark utilized\nGPT-4 to simulate multi-turn conversations in five different categories and to evaluate the model re-\nsponses. Ungroundedness between 0 (fully grounded) and 4 (not grounded) measures if the information\nin a response is based on a given prompt. In other categories, responses were evaluated in terms of\nthe severity of harmfulness from 0 (no harm) to 7 (extreme harm) and the defect rates (DR- x) were\ncomputed as the percentage of samples with the severity score being greater than or equal to x.\n6\nFigure 4: Left: phi-3-mini \u2019s completion without search. Right: phi-3-mini \u2019s completion with search, using the\ndefault HuggingFace Chat-UI search ability.\n5 Weakness\nIn terms of LLM capabilities, while phi-3-mini model achieves similar level of language understanding\nand reasoning ability as much larger models, it is still fundamentally limited by its size for certain tasks.\nThe model simply does not have the capacity to store too much \u201cfactual knowledge\u201d, which can be seen\nfor example with low performance on TriviaQA. However, we believe such weakness can be resolved by\naugmentation with a search engine. We show an example using the HuggingFace default Chat-UI with\nphi-3-mini in Figure 4. Another weakness related to model\u2019s capacity is that we mostly restricted the\nlanguage to English. Exploring multilingual capabilities for Small Language Models is an important\nnext step, with some initial promising results on phi-3-small by including more multilingual data.\nDespite our diligent RAI efforts, as with most LLMs, there remains challenges around factual inaccu-\nracies (or hallucinations), reproduction or amplification of biases, inappropriate content generation, and\nsafety issues. The use of carefully curated training data, and targeted post-training, and improvements\nfrom red-teaming insights significantly mitigates these issues across all dimensions. However, there is\nsignificant work ahead to fully address these challenges.\n7\nReferences\n[AI23] Meta AI. Introducing meta llama 3: The most capable openly available llm to date, 2023.\n[AON+21] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program\nsynthesis with large language models. arXiv preprint arXiv:2108.07732 , 2021.\n[BJN+22] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,\nDawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kada-\nvath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds,\nDanny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda,\nCatherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah,\nBen Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement\nlearning from human feedback, 2022.\n[BSA+24] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul R\u00a8 ottger, Dan Jurafsky, Tatsunori\nHashimoto, and James Zou. Safety-tuned llamas: Lessons from improving the safety of large\nlanguage models that follow instructions, 2024.\n[BZGC19] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical\ncommonsense in natural language. arXiv preprint arXiv:1911.11641 , 2019.", "start_char_idx": 12759, "end_char_idx": 16569, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e03aabd-d0dd-4efa-bab5-94c9b8d518c1": {"__data__": {"id_": "5e03aabd-d0dd-4efa-bab5-94c9b8d518c1", "embedding": null, "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone", "node_type": "4", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "1d73b83bbcb01ac20098e99e810ac3db9ac314ae75d7d3091e1f04330fc1cf6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3a2b94c-ecc3-40fd-aac9-17f6dff3a2fe", "node_type": "1", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "a1243e05bd919e898cef498d67177009d771c10aa5842cf1841cef7d6231f64f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2ab354c-d7f3-4668-83e7-455a01bdb1ea", "node_type": "1", "metadata": {}, "hash": "fec1008e1c1fe5ed7622a71a8cba9c5e80f0ba6a0398a8824e570036afaafdbe", "class_name": "RelatedNodeInfo"}}, "text": "Training a helpful and harmless assistant with reinforcement\nlearning from human feedback, 2022.\n[BSA+24] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul R\u00a8 ottger, Dan Jurafsky, Tatsunori\nHashimoto, and James Zou. Safety-tuned llamas: Lessons from improving the safety of large\nlanguage models that follow instructions, 2024.\n[BZGC19] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical\ncommonsense in natural language. arXiv preprint arXiv:1911.11641 , 2019.\n[CCE+18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\nchallenge, 2018.\n[CKB+21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz\nKaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168 , 2021.\n[CLC+19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.\nInProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers) , pages 2924\u20132936, 2019.\n[CTJ+21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\nBrooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo-\nhammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings,\nMatthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,\nShantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage,\nMira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code,\n2021.\n8\n[DZZ+24] Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu,\nFan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens,\n2024.\n[GZA+23] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00b4 esar Teodoro Mendes, Allie Del Giorno,\nSivakanth Gopi, Mojan Javaheripi, Gustavo de Rosa Piero Kauffmann, Olli Saarikivia,\nAdil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00b4 ebastien Bubeck, Ronen El-\ndan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need. arXiv\npreprint arXiv:2306.11644 , 2023.\n[HBK+21] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,\nDawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH\ndataset, 2021.", "start_char_idx": 16062, "end_char_idx": 19184, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2ab354c-d7f3-4668-83e7-455a01bdb1ea": {"__data__": {"id_": "c2ab354c-d7f3-4668-83e7-455a01bdb1ea", "embedding": null, "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone", "node_type": "4", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "1d73b83bbcb01ac20098e99e810ac3db9ac314ae75d7d3091e1f04330fc1cf6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e03aabd-d0dd-4efa-bab5-94c9b8d518c1", "node_type": "1", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "a29d6dc061bb2d0c2c7558506761c8a932afa2c46f9562a835db593966a41216", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "902caf10-2e3b-4105-895e-8d080ceb145c", "node_type": "1", "metadata": {}, "hash": "d7c8a10468e6350a2692797cacc5088edcef2113d4f0bc6a778657fdf00dd47d", "class_name": "RelatedNodeInfo"}}, "text": "Textbooks are all you need. arXiv\npreprint arXiv:2306.11644 , 2023.\n[HBK+21] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,\nDawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH\ndataset, 2021.\n[HBM+22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Eliza Ruther-\nford Trevor Cai, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark,\nTom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Au-\nrelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. Training compute-optimal large language models. arXiv preprint\narXiv:2203.15556 , 2022.\n[JBA+23] Mojan Javaheripi, S\u00b4 ebastien Bubeck, Marah Abdin, Jyoti Aneja, Caio C\u00b4 esar\nTeodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, Suriya\nGunasekar, Piero Kauffmann, Yin Tat Lee, Yuanzhi Li, Anh Nguyen, Gustavo de Rosa, Olli\nSaarikivi, Adil Salim, Shital Shah, Michael Santacroce, Harkirat Singh Behl, Adam Tau-\nmann Kalai, Xin Wang, Rachel Ward, Philipp Witte, Cyril Zhang, and Yi Zhang. Phi-2:\nThe surprising power of small language models. Microsoft Research Blog , 2023.\n[JCWZ17] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale\ndistantly supervised challenge dataset for reading comprehension, 2017.\n[JLD+23] Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang\nSun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of\nllm via a human-preference dataset, 2023.\n[JPO+20] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits.\nWhat disease does this patient have? a large-scale open domain question answering dataset\nfrom medical exams, 2020.\n[JSM+23] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, L\u00b4 elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timoth\u00b4 ee Lacroix, and William El Sayed. Mistral 7b, 2023.\n[JSR+24] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary,\nChris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian\nBressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00b4 elio Renard Lavaud, Lu-\ncile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szy-\nmon Antoniak, Teven Le Scao, Th\u00b4 eophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00b4 ee\nLacroix, and William El Sayed. Mixtral of experts, 2024.\n9\n[KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural\nlanguage models. arXiv preprint arXiv:2001.08361 , 2020.", "start_char_idx": 18923, "end_char_idx": 21841, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "902caf10-2e3b-4105-895e-8d080ceb145c": {"__data__": {"id_": "902caf10-2e3b-4105-895e-8d080ceb145c", "embedding": null, "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone", "node_type": "4", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "1d73b83bbcb01ac20098e99e810ac3db9ac314ae75d7d3091e1f04330fc1cf6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2ab354c-d7f3-4668-83e7-455a01bdb1ea", "node_type": "1", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "bb37ed5377745ee1c752ba1d28adb6ff4a83284174c585df6438363561a354d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae0591ff-ca0a-4c02-966e-017fe1fef75c", "node_type": "1", "metadata": {}, "hash": "6ba6988f9eaf04c84e32d5043dd9f8514bffa185076325532cc722b6f8601dde", "class_name": "RelatedNodeInfo"}}, "text": "Mixtral of experts, 2024.\n9\n[KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural\nlanguage models. arXiv preprint arXiv:2001.08361 , 2020.\n[LBE+23] Yuanzhi Li, S\u00b4 ebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and\nYin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint\narXiv:2309.05463 , 2023.\n[LHE22] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic\nhuman falsehoods, 2022.\n[MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor\nconduct electricity? a new dataset for open book question answering, 2018.\n[MRB+23] Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus,\nNouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained\nlanguage models. arXiv preprint arXiv:2305.16264 , 2023.\n[NWD+20] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.\nAdversarial nli: A new benchmark for natural language understanding, 2020.\n[RHS+23] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang,\nJulien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof\nq&a benchmark, 2023.\n[RWC+19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\n[SLBBC19] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande:\nAn adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641 , 2019.\n[SRR+22] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,\nAdam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri` a Garriga-Alonso, et al.\nBeyond the imitation game: Quantifying and extrapolating the capabilities of language\nmodels. arXiv preprint arXiv:2206.04615 , 2022.\n[SSS+22] Mirac Suzgun, Nathan Scales, Nathanael Sch\u00a8 arli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Chal-\nlenging big-bench tasks and whether chain-of-thought can solve them, 2022.\n[THLB19] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A\nquestion answering challenge targeting commonsense knowledge, 2019.\n[TLI+23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth\u00b4 ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n[TMH+24] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,\nShreya Pathak, Laurent Sifre, Morgane Rivi` ere, Mihir Sanjay Kale, Juliette Love, et al.", "start_char_idx": 21582, "end_char_idx": 24545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae0591ff-ca0a-4c02-966e-017fe1fef75c": {"__data__": {"id_": "ae0591ff-ca0a-4c02-966e-017fe1fef75c", "embedding": null, "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone", "node_type": "4", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "1d73b83bbcb01ac20098e99e810ac3db9ac314ae75d7d3091e1f04330fc1cf6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "902caf10-2e3b-4105-895e-8d080ceb145c", "node_type": "1", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "ed0cd43682a37e3c08b2d1610ad51d2d17d0385ed0409494aa011d5e0d98e934", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9606dfa-42bd-48f3-bb6a-771d9c74274f", "node_type": "1", "metadata": {}, "hash": "83919021c8c5f045b45ed453d095b5c81b1d96e501acfe105a05d6231a9b3deb", "class_name": "RelatedNodeInfo"}}, "text": "Commonsenseqa: A\nquestion answering challenge targeting commonsense knowledge, 2019.\n[TLI+23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth\u00b4 ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n[TMH+24] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,\nShreya Pathak, Laurent Sifre, Morgane Rivi` ere, Mihir Sanjay Kale, Juliette Love, et al.\nGemma: Open models based on gemini research and technology, 2024.\n10\n[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems , volume 30, 2017.\n[ZCG+23] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin\nSaied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating\nfoundation models, 2023.\n[ZCS+23] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with\nmt-bench and chatbot arena. arXiv preprint arXiv:2306.05685 , 2023.\n[ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can\na machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages 4791\u20134800, 2019.\nA Example prompt for benchmarks\nQuestion:\nSolve for x:(\u22121\n3)(\u22124\u22123x)=1\n2\nOptions:\nA.\u22125\n6\nB.7\n6\nC.5\n3\nD.1\n6\nAnswer: A\nQuestion:\nWhich of the following is the body cavity that contains the pituitary gland?\nOptions:\nA. Abdominal\nB. Cranial\nC. Pleural\nD. Spinal\nAnswer: B\nQuestion:\nWhere was the most famous site of the mystery cults in Greece?", "start_char_idx": 23944, "end_char_idx": 25880, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9606dfa-42bd-48f3-bb6a-771d9c74274f": {"__data__": {"id_": "f9606dfa-42bd-48f3-bb6a-771d9c74274f", "embedding": null, "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone", "node_type": "4", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "1d73b83bbcb01ac20098e99e810ac3db9ac314ae75d7d3091e1f04330fc1cf6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae0591ff-ca0a-4c02-966e-017fe1fef75c", "node_type": "1", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "f23b054bb2d15e9ace99c95834936e2fd1ddf33c1ea53678dcdbb94234271f7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f98839d3-cec4-4fe6-a6d9-04a5e7f25875", "node_type": "1", "metadata": {}, "hash": "dd5ab6fce2e125edfa980a8a012dc8e78b8337cbf4dfcc036d8c6ded9053b387", "class_name": "RelatedNodeInfo"}}, "text": "[ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can\na machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages 4791\u20134800, 2019.\nA Example prompt for benchmarks\nQuestion:\nSolve for x:(\u22121\n3)(\u22124\u22123x)=1\n2\nOptions:\nA.\u22125\n6\nB.7\n6\nC.5\n3\nD.1\n6\nAnswer: A\nQuestion:\nWhich of the following is the body cavity that contains the pituitary gland?\nOptions:\nA. Abdominal\nB. Cranial\nC. Pleural\nD. Spinal\nAnswer: B\nQuestion:\nWhere was the most famous site of the mystery cults in Greece?\nOptions:\nA. Ephesus\nB. Corinth\nC. Athens\nD. Eleusis\nAnswer:\n11\nB Authors\nMarah Abdin Russell J. Hewett Corby Rosset\nSam Ade Jacobs Jamie Huynh Olatunji Ruwase\nAmmar Ahmad Awan Mojan Javaheripi Olli Saarikivi\nJyoti Aneja Xin Jin Amin Saied\nAhmed Awadallah Piero Kauffmann Adil Salim\nHany Awadalla Nikos Karampatziakis Michael Santacroce\nNguyen Bach Dongwoo Kim Shital Shah\nAmit Bahree Mahoud Khademi Ning Shang\nArash Bakhtiari Lev Kurilenko Hiteshi Sharma\nHarkirat Behl James R. Lee Xia Song\nAlon Benhaim Yin Tat Lee Xin Wang\nMisha Bilenko Yuanzhi Li Rachel Ward\nJohan Bjorck Chen Liang Guanhua Wang\nS\u00b4 ebastien Bubeck Weishung Liu Philipp Witte\nMartin Cai Eric Lin Michael Wyatt\nCaio C\u00b4 esar Teodoro Mendes Zeqi Lin Jiahang Xu\nWeizhu Chen Piyush Madan Can Xu\nVishrav Chaudhary Arindam Mitra Sonali Yadav\nParul Chopra Hardik Modi Fan Yang\nAllie Del Giorno Brandon Norick Ziyi Yang\nGustavo de Rosa Anh Nguyen Donghan Yu\nMatthew Dixon Barun Patra Chengruidong Zhang\nRonen Eldan Daniel Perez-Becker Cyril Zhang\nDan Iter Heyang Qin Jianwen Zhang\nAbhishek Goswami Thomas Portet Li Lyna Zhang\nSuriya Gunasekar Reid Pryzant Yi Zhang\nEmman Haider Sambudha Roy Yunan Zhang\nJunheng Hao Marko Radmilac Xiren Zhou\n12", "start_char_idx": 25290, "end_char_idx": 27084, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f98839d3-cec4-4fe6-a6d9-04a5e7f25875": {"__data__": {"id_": "f98839d3-cec4-4fe6-a6d9-04a5e7f25875", "embedding": null, "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention", "node_type": "4", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2e0f0a65137957aa87b09912b7e4beee6c570c85d000c2836c629c317c737404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9606dfa-42bd-48f3-bb6a-771d9c74274f", "node_type": "1", "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "5ca1638423184f57a5d2bd0a1941b3a204ea454ac39775d08be35de8508beeee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34264931-bbb0-479c-9b81-e643fdeabce6", "node_type": "1", "metadata": {}, "hash": "3d5392fa10c4264e0c95d638aa19c0e0d785d6820f877e7ae7407ed1373fa796", "class_name": "RelatedNodeInfo"}}, "text": "Preprint. Under review.\nLeave No Context Behind:\nEfficient Infinite Context Transformers with Infini-attention\nTsendsuren Munkhdalai, Manaal Faruqui and Siddharth Gopal\nGoogle\ntsendsuren@google.com\nAbstract\nThis work introduces an efficient method to scale Transformer-based Large\nLanguage Models (LLMs) to infinitely long inputs with bounded memory\nand computation. A key component in our proposed approach is a new at-\ntention technique dubbed Infini-attention. The Infini-attention incorporates\na compressive memory into the vanilla attention mechanism and builds\nin both masked local attention and long-term linear attention mechanisms\nin a single Transformer block. We demonstrate the effectiveness of our\napproach on long-context language modeling benchmarks, 1M sequence\nlength passkey context block retrieval and 500K length book summarization\ntasks with 1B and 8B LLMs. Our approach introduces minimal bounded\nmemory parameters and enables fast streaming inference for LLMs.\n1 Introduction\nMemory serves as a cornerstone of intelligence, as it enables efficient computations tailored\nto specific contexts. However, Transformers (Vaswani et al., 2017) and Transformer-based\nLLMs (Brown et al., 2020; Touvron et al., 2023; Anil et al., 2023; Groeneveld et al., 2024) have\na constrained context-dependent memory, due to the nature of the attention mechanism.\nUpdate \nVVConcat Concat \nQ V\nQ V\nQs{KV}sCompressive memory & \nLinear attention Causal scaled dot-product \nattention & PE Linear \nprojection \n{KV}s-1Retrieve \nFigure 1: Infini-attention has an addi-\ntional compressive memory with linear\nattention for processing infinitely long\ncontexts. {KV}s\u22121and{KV}sare atten-\ntion key and values for current and previ-\nous input segments, respectively and Qs\nthe attention queries. PE denotes position\nembeddings.The attention mechanism in Transformers ex-\nhibits quadratic complexity in both memory\nfootprint and computation time. For example,\nthe attention Key-Value (KV) states have 3TB\nmemory footprint for a 500B model with batch\nsize 512 and context length 2048 (Pope et al.,\n2023). Indeed, scaling LLMs to longer sequences\n(i.e. 1M tokens) is challenging with the standard\nTransformer architectures and serving longer\nand longer context models becomes costly finan-\ncially.\nCompressive memory systems promise to be\nmore scalable and efficient than the attention\nmechanism for extremely long sequences (Kan-\nerva, 1988; Munkhdalai et al., 2019). Instead\nof using an array that grows with the input se-\nquence length, a compressive memory primarily\nmaintains a fixed number of parameters to store\nand recall information with a bounded storage\nand computation costs. In the compressive mem-\nory, new information is added to the memory\nby changing its parameters with an objective\nthat this information can be recovered back later\non. However, the LLMs in their current state\nhave yet to see an effective, practical compres-\nsive memory technique that balances simplicity along with quality.\n1arXiv:2404.07143v1  [cs.CL]  10 Apr 2024\nPreprint. Under review.\nIn this work, we introduce a novel approach that enables Transformer LLMs to effectively\nprocess infinitely long inputs with bounded memory footprint and computation. A key\ncomponent in our proposed approach is a new attention technique dubbed Infini-attention\n(Figure 1). The Infini-attention incorporates a compressive memory into the vanilla attention\nmechanism (Bahdanau et al., 2014; Vaswani et al., 2017) and builds in both masked local\nattention and long-term linear attention mechanisms in a single Transformer block.\nSuch a subtle but critical modification to the Transformer attention layer enables a natural\nextension of existing LLMs to infinitely long contexts via continual pre-training and fine-\ntuning.\nOur Infini-attention reuses all the key, value and query states of the standard attention\ncomputation for long-term memory consolidation and retrieval. We store old KV states of\nthe attention in the compressive memory, instead of discarding them like in the standard\nattention mechanism. We then retrieve the values from the memory by using the attention\nquery states when processing subsequent sequences. To compute the final contextual\noutput, the Infini-attention aggregates the long-term memory-retrieved values and the local\nattention contexts.", "start_char_idx": 0, "end_char_idx": 4331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34264931-bbb0-479c-9b81-e643fdeabce6": {"__data__": {"id_": "34264931-bbb0-479c-9b81-e643fdeabce6", "embedding": null, "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention", "node_type": "4", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2e0f0a65137957aa87b09912b7e4beee6c570c85d000c2836c629c317c737404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f98839d3-cec4-4fe6-a6d9-04a5e7f25875", "node_type": "1", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "f47e301d8cd58758a109449698350f162234cd411c7ea4e4533bcf40e53e7680", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "088f9899-8dba-4ada-bee4-7c8fc4ccb19f", "node_type": "1", "metadata": {}, "hash": "a6ca0c4dd4518b0e313e50582727f23230453a7a00d577eed157be0512b19e86", "class_name": "RelatedNodeInfo"}}, "text": "Such a subtle but critical modification to the Transformer attention layer enables a natural\nextension of existing LLMs to infinitely long contexts via continual pre-training and fine-\ntuning.\nOur Infini-attention reuses all the key, value and query states of the standard attention\ncomputation for long-term memory consolidation and retrieval. We store old KV states of\nthe attention in the compressive memory, instead of discarding them like in the standard\nattention mechanism. We then retrieve the values from the memory by using the attention\nquery states when processing subsequent sequences. To compute the final contextual\noutput, the Infini-attention aggregates the long-term memory-retrieved values and the local\nattention contexts.\nIn our experiments, we show that our approach outperforms baseline models on long-\ncontext language modeling benchmarks while having 114x comprehension ratio in terms of\nmemory size. The model achieves even better perplexity when trained with 100K sequence\nlength. A 1B LLM naturally scales to 1M sequence length and solves the passkey retrieval\ntask when injected with Infini-attention. Finally, we show that a 8B model with Infini-\nattention reaches a new SOTA result on a 500K length book summarization task after\ncontinual pre-training and task fine-tuning.\nIn summary, our work makes the following contributions:\n1.We introduce a practical and yet powerful attention mechanism \u2013 Infini-attention\nwith long-term compressive memory and local causal attention for efficiently mod-\neling both long and short-range contextual dependencies.\n2.Infini-attention introduces minimal change to the standard scaled dot-product atten-\ntion and supports plug-and-play continual pre-training and long-context adaptation\nby design.\n3.Our approach enables Transformer LLMs to scale to infinitely long context with a\nbounded memory and compute resource by processing extremely long inputs in a\nstreaming fashion.\n2 Method\nFigure 2 compares our model, Infini-Transformer, and Transformer-XL (Dai et al., 2019).\nSimilar to Transformer-XL, Infini-Transformer operates on a sequence of segments. We\ncompute the standard causal dot-product attention context within each segment. So the\ndot-product attention computation is local in a sense that it covers a total Nnumber of\ntokens of the current segment with index S(Nis the segment length).\nThe local attention (Dai et al., 2019), however, discards the attention states of the previous\nsegment when processing the next one. In Infini-Transformers, instead of leaving out the\nold KV attention states, we propose to reuse them to maintain the entire context history\nwith a compressive memory. So each attention layer of Infini-Transformers has both global\ncompressive and local fine-grained states. We call such an efficient attention mechanism\nInfini-attention, which is illustrated in Figure 1 and described formally in the following\nsections.\n2.1 Infini-attention\nAs shown Figure 1, our Infini-attention computes both local and global context states and\ncombine them for its output. Similar to multi-head attention (MHA), it maintains Hnumber\n2\nPreprint. Under review.\nSegment 1 Segment 2 Segment 3 \nSegment 1 Segment 2 Segment 3 Transformer block: Infini-T ransformer \nT ransformer-XL Compressive memory: \nMemory update: \nMemory retrieval: \nEffective context: \nInput segment: Segment 1 \nFigure 2: Infini-Transformer (top) has an entire context history whereas Transformer-XL\n(bottom) discards old contexts since it caches the KV states for the last segment only.\nof parallel compressive memory per attention layer ( His the number of attention heads) in\naddition to the dot-product attention.\n2.1.1 Scaled Dot-product Attention\nThe multi-head scaled dot-product attention (Vaswani et al., 2017), specially its self-attention\nvariant (Munkhdalai et al., 2016; Cheng et al., 2016), has been the main building block in\nLLMs. The MHA\u2019s strong capability to model context-dependent dynamic computation and\nits conveniences of temporal masking have been leveraged extensively in the autoregressive\ngenerative models.\nA single head in the vanilla MHA computes its attention context Adot\u2208I RN\u00d7dvaluefrom\nsequence of input segments X\u2208I RN\u00d7dmodel as follows. First, it computes attention query,\nkey, and value states:\nK=XW K,V=XW Vand Q=XW Q.", "start_char_idx": 3589, "end_char_idx": 7899, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "088f9899-8dba-4ada-bee4-7c8fc4ccb19f": {"__data__": {"id_": "088f9899-8dba-4ada-bee4-7c8fc4ccb19f", "embedding": null, "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention", "node_type": "4", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2e0f0a65137957aa87b09912b7e4beee6c570c85d000c2836c629c317c737404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34264931-bbb0-479c-9b81-e643fdeabce6", "node_type": "1", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2d52c7da11b18162d485b7a4131356403d9bdf95719008bfe1d10b63b7edd66c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fce8519d-3e34-4885-a7a6-c4b2514bab06", "node_type": "1", "metadata": {}, "hash": "bc1bde53c0ccfff1369277beeac2903ef7098313925c73145ebf3b32f482b7db", "class_name": "RelatedNodeInfo"}}, "text": "of parallel compressive memory per attention layer ( His the number of attention heads) in\naddition to the dot-product attention.\n2.1.1 Scaled Dot-product Attention\nThe multi-head scaled dot-product attention (Vaswani et al., 2017), specially its self-attention\nvariant (Munkhdalai et al., 2016; Cheng et al., 2016), has been the main building block in\nLLMs. The MHA\u2019s strong capability to model context-dependent dynamic computation and\nits conveniences of temporal masking have been leveraged extensively in the autoregressive\ngenerative models.\nA single head in the vanilla MHA computes its attention context Adot\u2208I RN\u00d7dvaluefrom\nsequence of input segments X\u2208I RN\u00d7dmodel as follows. First, it computes attention query,\nkey, and value states:\nK=XW K,V=XW Vand Q=XW Q. (1)\nHere, WK\u2208I Rdmodel\u00d7dkey,WV\u2208I Rdmodel\u00d7dvalueand WQ\u2208I Rdmodel\u00d7dkeyare trainable projection\nmatrices. Then, the attention context is calculated as a weighted average of all other values\nas\nAdot=softmax\u0012QKT\n\u221admodel\u0013\nV. (2)\nFor MHA, we compute Hnumber of attention context vectors for each sequence element\nin parallel, concatenate them along the second dimension and then finally project the\nconcatenated vector to the model space to obtain attention the output.\n2.1.2 Compressive Memory\nIn Infini-attention, instead of computing new memory entries for compressive memory, we\nreuse the query, key and value states ( Q,Kand V) from the dot-product attention compu-\ntation. The state sharing and reusing between the dot-product attention and compressive\nmemory not only enables efficient plug-in-play long-context adaptation but also speeds up\ntraining and inference. Similar to the prior work (Munkhdalai et al., 2019), our goal is to\nstore bindings of key and value states in the compressive memory and retrieve by using the\nquery vectors.\n3\nPreprint. Under review.\nWhile there are different forms of compressive memory proposed in the literature (Hop-\nfield, 1982; Kanerva, 1988; Schlag et al., 2019; Munkhdalai et al., 2019), for simplicity and\ncomputational efficiency, in this work we parameterize the memory with an associative\nmatrix (Schlag et al., 2020). This approach further allows us to cast the memory update\nand retrieval process as linear attention mechanism (Shen et al., 2018) and to leverage\nstable training techniques from the related methods. Specially, we adopt the update rule\nand retrieval mechanism by Katharopoulos et al. (2020) mainly due to its simplicity and\ncompetitive performance.\nMemory retrieval. In Infini-attention, we retrieve new content Amem\u2208I RN\u00d7dvaluefrom the\nmemory Ms\u22121\u2208I Rdkey\u00d7dvalueby using the query Q\u2208I RN\u00d7dkeyas:\nAmem=\u03c3(Q)Ms\u22121\n\u03c3(Q)zs\u22121. (3)\nHere, \u03c3and zs\u22121\u2208I Rdkeyare a nonlinear activation function and a normalization term,\nrespectively. As the choice of the non-linearity and the norm method is crucial for training\nstability, following Katharopoulos et al. (2020) we record a sum over all keys as the normal-\nization term zs\u22121and use element-wise ELU + 1 as the activation function (Clevert et al.,\n2015).\nMemory update. Once the retrieval is done, we update the memory and the normalization\nterm with the new KV entries and obtain the next states as\nMs\u2190Ms\u22121+\u03c3(K)TVand zs\u2190zs\u22121+N\n\u2211\nt=1\u03c3(Kt). (4)\nThe new memory states Msand zsare then passed to the next segment S+1, building in\na recurrence in each attention layer. The right side term \u03c3(K)TVin Eq. (4)is known as an\nassociative binding operator (Smolensky, 1990; Hebb, 2005; Schlag et al., 2020).\nInspired by the success of delta rule (Munkhdalai et al., 2019; Schlag et al., 2020; 2021),\nwe have also incorporated it into our Infini-attention.", "start_char_idx": 7130, "end_char_idx": 10745, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fce8519d-3e34-4885-a7a6-c4b2514bab06": {"__data__": {"id_": "fce8519d-3e34-4885-a7a6-c4b2514bab06", "embedding": null, "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention", "node_type": "4", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2e0f0a65137957aa87b09912b7e4beee6c570c85d000c2836c629c317c737404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "088f9899-8dba-4ada-bee4-7c8fc4ccb19f", "node_type": "1", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "fd42aa5db32ac559c466632278354e5ba9f1be5bd6207f3a950aae9e0183adb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f0476ca-26bc-4dc0-ac95-b80d959e0b60", "node_type": "1", "metadata": {}, "hash": "cbe19ff2bf78d56cf73286812d4f3943fefc09f1403822e30aae57f64ea25bad", "class_name": "RelatedNodeInfo"}}, "text": "Memory update. Once the retrieval is done, we update the memory and the normalization\nterm with the new KV entries and obtain the next states as\nMs\u2190Ms\u22121+\u03c3(K)TVand zs\u2190zs\u22121+N\n\u2211\nt=1\u03c3(Kt). (4)\nThe new memory states Msand zsare then passed to the next segment S+1, building in\na recurrence in each attention layer. The right side term \u03c3(K)TVin Eq. (4)is known as an\nassociative binding operator (Smolensky, 1990; Hebb, 2005; Schlag et al., 2020).\nInspired by the success of delta rule (Munkhdalai et al., 2019; Schlag et al., 2020; 2021),\nwe have also incorporated it into our Infini-attention. The delta rule attempts a slightly\nimproved memory update by first retrieving existing value entries and subtracting them\nfrom the new values before applying the associative bindings as new update.\nMs\u2190Ms\u22121+\u03c3(K)T(V\u2212\u03c3(K)Ms\u22121\n\u03c3(K)zs\u22121). (5)\nThis update rule ( Linear +Delta ) leaves the associative matrix unmodified if the KV binding\nalready exists in the memory while still tracking the same normalization term as the former\none ( Linear ) for numerical stability.\nLong-term context injection. We aggregate the local attention state Adotand memory\nretrieved content Amemvia a learned gating scalar \u03b2:\nA=sigmoid (\u03b2)\u2299Amem+ (1\u2212sigmoid (\u03b2))\u2299Adot. (6)\nThis adds only a single scalar value as training parameter per head while allowing a\nlearnable trade-off between the long-term and local information flows in the model (Wu\net al., 2022).\nSimilar to the standard MHA, for the multi-head Infini-attention we compute Hnumber of\ncontext states in parallel, and concatenate and project them for the final attention output\nO\u2208I RN\u00d7dmodel:\nO= [A1; . . .AH]WO (7)\nwhere WO\u2208I RH\u00d7dvalue\u00d7dmodelis trainable weights.\n2.2 Memory and Effective Context Window\nOur Infini-Transformer enables an unbounded context window with a bounded memory\nfootprint. To illustrate this, Table 1 lists the previous segment-level memory models with\n4\nPreprint. Under review.\nModel Memory (cache) footprint Context length Memory update Memory retrieval\nTransformer-XL (dkey+dvalue)\u00d7H\u00d7N\u00d7l N \u00d7l Discarded Dot-product attention\nCompressive Transformer dmodel\u00d7(c+N)\u00d7l (c\u00d7r+N)\u00d7l Discarded Dot-product attention\nMemorizing Transformers (dkey+dvalue)\u00d7H\u00d7N\u00d7S N \u00d7S None kNN + dot-product attention\nRMT dmodel\u00d7p\u00d7l\u00d72 N\u00d7S Discarded Soft-prompt input\nAutoCompressors dmodel\u00d7p\u00d7(m+1)\u00d7l N \u00d7S Discarded Soft-prompt input\nInfini-Transformers dkey\u00d7(dvalue+1)\u00d7H\u00d7l N \u00d7S Incremental Linear attention\nTable 1: Transformer models with segment-level memory are compared. For each model, the\nmemory size and effective context length are defined in terms of their model parameters ( N:\ninput segment length, S: the number of segments, l: the number of layers, H: the number\nof attention heads, c: Compressive Transformer memory size, r: compression ratio, p: the\nnumber of soft-prompt summary vectors and m: summary vector accumulation steps).\ntheir context-memory footprint and effective context length defined in terms of model\nparameters and input segment length. Infini-Transformer has a constant memory complexity\nofdkey\u00d7dvalue+dkeyfor storing compressed context in Msand zsfor each head in single\nlayer while for the other models, the complexity grows along with the sequence dimension\n- the memory complexity depends either on the cache size for Transformer-XL (Dai et al.,\n2019), Compressive Transformer (Rae et al., 2019) and Memorizing Transformers (Wu et al.,\n2022) or on the soft-prompt size for RTM (Bulatov et al., 2022) and AutoCompressors (Ge\net al., 2023).", "start_char_idx": 10156, "end_char_idx": 13652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f0476ca-26bc-4dc0-ac95-b80d959e0b60": {"__data__": {"id_": "0f0476ca-26bc-4dc0-ac95-b80d959e0b60", "embedding": null, "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention", "node_type": "4", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2e0f0a65137957aa87b09912b7e4beee6c570c85d000c2836c629c317c737404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fce8519d-3e34-4885-a7a6-c4b2514bab06", "node_type": "1", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "18c679e3ef971589fbbec7d0e7f0a087554b150a892c66325dea0cc405edd2de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3f9f3fb-b8e0-471b-9430-886ad9ddfa16", "node_type": "1", "metadata": {}, "hash": "eb17535205bd73ba482ffd1360701a8e1d70e3de2eda66290eeab7c03cdbcdeb", "class_name": "RelatedNodeInfo"}}, "text": "their context-memory footprint and effective context length defined in terms of model\nparameters and input segment length. Infini-Transformer has a constant memory complexity\nofdkey\u00d7dvalue+dkeyfor storing compressed context in Msand zsfor each head in single\nlayer while for the other models, the complexity grows along with the sequence dimension\n- the memory complexity depends either on the cache size for Transformer-XL (Dai et al.,\n2019), Compressive Transformer (Rae et al., 2019) and Memorizing Transformers (Wu et al.,\n2022) or on the soft-prompt size for RTM (Bulatov et al., 2022) and AutoCompressors (Ge\net al., 2023).\nEarly \nlayers \nAttention heads \nFigure 3: There are two types of heads\nemerged in Infini-attention after training: spe-\ncialized heads with gating score near 0 or\n1 and mixer heads with score close to 0.5.\nThe specialized heads either process contex-\ntual information via the local attention mech-\nanism or retrieve from the compressive mem-\nory whereas the mixer heads aggregate both\ncurrent contextual information and long-term\nmemory content together into single output.Transformer-XL computes attention over KV\nstates cached from the last segment in addition\nto the current states. Since this is done for each\nlayer, Transformer-XL extends the context win-\ndow from NtoN\u00d7ltokens with an additional\nmemory footprint of (dkey+dvalue)\u00d7H\u00d7N\u00d7l.\nCompressive Transformer adds a second cache\nto Transformer-XL and stores compressed rep-\nresentations of past segment activations. So it\nextends the Transformer-XL\u2019s context window\nbyc\u00d7r\u00d7lbut still has a large context-memory\ncomplexity. Taking the idea further, Memoriz-\ning Transformers opt to store the entire KV states\nas context for input sequences. Since the stor-\nage becomes prohibitively expensive in this case,\nthey restrict the contextual computation to a sin-\ngle layer only. By utilizing a fast kNN retriever,\nMemorizing Transformers then build a context\nwindow covering the entire sequence history of\nlength N\u00d7Sat an increased cost of storage. Our\nexperiments show that Infini-Transformer LM\ncan achieve more than 100x compression rate on\ntop of Memorizing Transformers while further\nimproving the perplexity score.\nRMT and AutoCompressors allow for a poten-\ntially infinite context length since they compress\nthe input into summary vectors and then pass\nthem as extra soft-prompt inputs for the subsequent segments. However, in practice the\nsuccess of those techniques highly depends on the size of soft-prompt vectors. Namely, it\nis necessary to increase the number of soft-prompt (summary) vectors to achieve a better\nperformance with AutoCompressors (Chevalier et al., 2023) and with that, the memory and\ncompute complexity grow quickly resulting in diminished efficiency. It was also observed in\nAutoCompressors (Chevalier et al., 2023) that an efficient compression objective is needed\nfor training such prompt compression techniques (Ge et al., 2023).\n5\nPreprint. Under review.\nModel Memory size (comp.) XL cache Segment length PG19 Arxiv-math\nTransformer-XL 50M (3.7x) 2048 2048 11.88 2.42\nMemorizing Transformers 183M (1x) 2048 2048 11.37 2.26\nRMT 2.5M (73x) None 2048 13.27 2.55\nInfini-Transformer (Linear) 1.6M (114x) None 2048 9.65 2.24\nInfini-Transformer (Linear + Delta) 1.6M (114x) None 2048 9.67 2.23\nTable 2: Long-context language modeling results are compared in terms of average token-\nlevel perplexity. Comp. denotes compression ratio. Infini-Transformer outperforms memo-\nrizing transformers with memory length of 65K and achieves 114x compression ratio.\n3 Experiments\nWe evaluated our Infini-Transformer models on benchmarks involving extremely long input\nsequences: long-context language modeling, 1M length passkey context block retrieval\nand 500K length book summarization tasks.", "start_char_idx": 13023, "end_char_idx": 16813, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3f9f3fb-b8e0-471b-9430-886ad9ddfa16": {"__data__": {"id_": "f3f9f3fb-b8e0-471b-9430-886ad9ddfa16", "embedding": null, "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention", "node_type": "4", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2e0f0a65137957aa87b09912b7e4beee6c570c85d000c2836c629c317c737404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f0476ca-26bc-4dc0-ac95-b80d959e0b60", "node_type": "1", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "61a2361bda78926e2fca1f4671e3808e8da175c46eafe18d6137579f75b10b90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2d44ff4-3e3c-4523-9f9d-fe6a3d692572", "node_type": "1", "metadata": {}, "hash": "14826c368a54b6caaed8fa889cdcd17f15c6d437f883b2cdca19a6a5ace5cbef", "class_name": "RelatedNodeInfo"}}, "text": "Comp. denotes compression ratio. Infini-Transformer outperforms memo-\nrizing transformers with memory length of 65K and achieves 114x compression ratio.\n3 Experiments\nWe evaluated our Infini-Transformer models on benchmarks involving extremely long input\nsequences: long-context language modeling, 1M length passkey context block retrieval\nand 500K length book summarization tasks. For the language modeling benchmark, we\ntrain our models from scratch while for the passkey and book summarization tasks, we\ncontinually pre-train existing LLMs in order to highlight a plug-and-play long-context\nadaptation capability of our approach.\n3.1 Long-context Language Modeling\nWe trained and evaluated small Infini-Transformer models on PG19 (Rae et al., 2019) and\nArxiv-math (Wu et al., 2022) benchmarks. Our setup closely resembles that of Memorizing\nTransformers (Wu et al., 2022). Namely, all our models have 12 layers and 8 attention heads\nof dimension 128 each and FFNs with hidden layer 4096.\nWe set the Infini-attention segment length Nto 2048 for all attention layers and the input\nsequence length to 32768 for training. This allows the Infini-attention to unroll over 16 steps\nw.r.t its compressive memory states. For the RMT baseline, we performed several runs with\nsummary prompt lengths 50, 100 and 150 and sequence lengths 4096, 8196 and 32768. RMT\nwith 100 summary vectors gave the best result when trained on 8196 length sequences.\nThe main results from the language modeling experiments are summarized in Table 2. Our\nInfini-Transformer outperforms both Transformer-XL (Dai et al., 2019) and Memorizing\nTransformers (Wu et al., 2022) baselines while maintaining 114x less memory parameters\nthan the Memorizing Transformer model with a vector retrieval-based KV memory with\nlength of 65K at its 9thlayer.\n100K length training. We further increased the training sequence length to 100K from\n32K and trained the models on Arxiv-math dataset. 100K training further decreased the\nperplexity score to 2.21 and 2.20 forLinear and Linear +Delta models.\nGating score visualization. Figure 3 visualizes the gating score, sigmoid (\u03b2)for the compres-\nsive memory for all attention heads in each layer. There are two types of heads emerged in\nInfini-attention after training: specialized heads with a gating score near 0 or 1 and mixer\nheads with a score close to 0.5. The specialized heads either process contextual information\nvia the local attention computation or retrieve from the compressive memory whereas the\nZero-shot\n32K 128K 256K 512K 1M\nInfini-Transformer (Linear) 14/13/98 11/14/100 6/3/100 6/7/99 8/6/98\nInfini-Transformer (Linear + Delta) 13/11/99 6/9/99 7/5/99 6/8/97 7/6/97\nFT (400 steps)\nInfini-Transformer (Linear) 100/100/100 100/100/100 100/100/100 97/99/100 96/94/100\nInfini-Transformer (Linear + Delta) 100/100/100 100/100/99 100/100/99 100/100/100 100/100/100\nTable 3: Infini-Transformers solved the passkey task with up to 1M context length when\nfine-tuned on 5K length inputs. We report token-level retrieval accuracy for passkeys hidden\nin a different part ( start/middle/end ) of long inputs with lengths 32K to 1M.\n6\nPreprint. Under review.", "start_char_idx": 16432, "end_char_idx": 19596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2d44ff4-3e3c-4523-9f9d-fe6a3d692572": {"__data__": {"id_": "a2d44ff4-3e3c-4523-9f9d-fe6a3d692572", "embedding": null, "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention", "node_type": "4", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2e0f0a65137957aa87b09912b7e4beee6c570c85d000c2836c629c317c737404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3f9f3fb-b8e0-471b-9430-886ad9ddfa16", "node_type": "1", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "30cf349df77caed4dbf9aa7dfb1b3a2e568ac601ccc5e7d2bec41c5621707216", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a96d7f4-1a22-4c2f-9cea-7b46be1a7674", "node_type": "1", "metadata": {}, "hash": "619c999bf5d000fc702adbba3206ae07f42abe322961d66ed9dbbe9d350c5b76", "class_name": "RelatedNodeInfo"}}, "text": "We report token-level retrieval accuracy for passkeys hidden\nin a different part ( start/middle/end ) of long inputs with lengths 32K to 1M.\n6\nPreprint. Under review.\nModel Rouge-1 Rouge-2 Rouge-L Overall\nBART 36.4 7.6 15.3 16.2\nBART + Unlimiformer 36.8 8.3 15.7 16.9\nPRIMERA 38.6 7.2 15.6 16.3\nPRIMERA + Unlimiformer 37.9 8.2 16.3 17.2\nInfini-Transformers (Linear) 37.9 8.7 17.6 18.0\nInfini-Transformers (Linear + Delta) 40.0 8.8 17.9 18.5\nTable 4: 500K length book summarization (BookSum) results. The BART, PRIMERA and\nUnlimiformer results are from Bertsch et al. (2024).\nmixer heads aggregate both current contextual information and long-term memory content\ntogether into a single output. Interestingly, each layer has at least a single short-range\nhead, allowing a forward-propagation of input signal up until the output layer. We also\nobserved an interleaving of long and short-term content retrievals throughout the forward\ncomputation.\n3.2 LLM Continual Pre-training\nWe performed a lightweight continual pre-training for long-context adaptation of existing\nLLMs. The pre-training data includes the PG19 and Arxiv-math corpus as well as C4\ntext (Raffel et al., 2020) with length more than 4K tokens. The segment length Nwas set to\n2K throughout our experiments.\n1M passkey retrieval benchmark. We replaced the vanilla MHA in a 1B LLM with Infini-\nattention and continued to pre-train on inputs with length of 4K. The model was trained for\n30K steps with batch size of 64 before fine-tuning on the passkey retrieval task (Mohtashami\n& Jaggi, 2024).\nThe passkey task hides a random number into a long text and asks it back at the model\noutput. The length of the distraction text is varied by repeating a text chunk multiple times.\nThe previous work (Chen et al., 2023a) showed that a 8B LLaMA model can solve the task up\nto 32K length when fine-tuned with the same 32K length inputs with Position Interpolation.\nWe take this challenge further and fine-tune on only 5K length inputs to test on 1M length\nregime.\nInput lengthRouge overall score\n17181920\n16K 32K 64K 128K 256K 500K\nFigure 4: Infini-Transformers obtain better\nRouge overall scores with more book text pro-\nvided as input.Table 3 reports the token-level accuracy for\ntest subsets with input lengths ranging from\n32K to 1M. For each test subset, we con-\ntrolled the position of the passkey so that it\nis either located around the beginning, mid-\ndle or the end of the input sequence. We\nreported both zero-shot accuracy and fine-\ntuning accuracy. Infini-Transformers solved\nthe task with up to 1M context length af-\nter fine-tuning on 5K length inputs for 400\nsteps.\n500K length book summarization (Book-\nSum). We further scaled our approach by\ncontinuously pre-training a 8B LLM model\nwith 8K input length for 30K steps. We then\nfine-tuned on a book summarization task,\nBookSum (Kry \u00b4sci\u00b4nski et al., 2021) where the\ngoal is to generate a summary of an entire\nbook text.\nWe set the input length to 32K for fine-tuning and increase to 500K for evaluating. We use a\ngeneration temperature of 0.5 and top p=0.95 and set the number of decoding steps to 1024\nto generate a summary of each book.\n7\nPreprint. Under review.\nTable 4 compares our model against the encoder-decoder models that were built particularly\nfor the summarization task (Lewis et al., 2019; Xiao et al., 2021) and their retrieval-based\nlong-context extension (Bertsch et al., 2024).", "start_char_idx": 19430, "end_char_idx": 22843, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a96d7f4-1a22-4c2f-9cea-7b46be1a7674": {"__data__": {"id_": "3a96d7f4-1a22-4c2f-9cea-7b46be1a7674", "embedding": null, "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention", "node_type": "4", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2e0f0a65137957aa87b09912b7e4beee6c570c85d000c2836c629c317c737404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2d44ff4-3e3c-4523-9f9d-fe6a3d692572", "node_type": "1", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "fa1a4a57cbf2436a2eba74ae8542e796bf1a55749e17c48f6153bff004db9559", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5f96973-ca2b-4ace-b207-ee5e6c7b4a9d", "node_type": "1", "metadata": {}, "hash": "f200946211a16a3deebf8ca1addc3074bdba5a4b5830d324fa7104d280d7321b", "class_name": "RelatedNodeInfo"}}, "text": "We then\nfine-tuned on a book summarization task,\nBookSum (Kry \u00b4sci\u00b4nski et al., 2021) where the\ngoal is to generate a summary of an entire\nbook text.\nWe set the input length to 32K for fine-tuning and increase to 500K for evaluating. We use a\ngeneration temperature of 0.5 and top p=0.95 and set the number of decoding steps to 1024\nto generate a summary of each book.\n7\nPreprint. Under review.\nTable 4 compares our model against the encoder-decoder models that were built particularly\nfor the summarization task (Lewis et al., 2019; Xiao et al., 2021) and their retrieval-based\nlong-context extension (Bertsch et al., 2024). Our model outperforms the previous best\nresults and achieves a new SOTA on BookSum by processing the entire text from book. We\nhave also plotted the overall Rouge score on validation split of BookSum data in Figure 4.\nThere is a clear trend showing that with more text provided as input from books, Our\nInfini-Transformers improves its summarization performance metric.\n4 Related Work\nCompressive memory. Inspired by the plasticity in biological neurons (Munkhdalai & Yu,\n2017a; Miconi et al., 2018), compressive memory approaches cast parameterized functions\nas memory to store and retrieve information (Hinton & Plaut, 1987; Schmidhuber, 1992; Ba\net al., 2016; Munkhdalai et al., 2019). Unlike the Transformer KV memory array (Vaswani\net al., 2017; Wu et al., 2022), which grows with input sequence length, compressive memory\nsystems maintain a constant number of memory parameters for computational efficiency.\nThe parameters are modified with an update rule to store information, which is then\nretrieved via a memory reading mechanism (Graves et al., 2014; Sukhbaatar et al., 2015;\nMunkhdalai & Yu, 2017b).\nCompressed input representations can be viewed as a summary of past sequence seg-\nments (Rae et al., 2019; Chevalier et al., 2023). Along this direction, more recent works\nhave been utilizing a Transformer LLM itself to compress input sequence for efficient long-\ncontext modeling (Bulatov et al., 2022; Chevalier et al., 2023; Ge et al., 2023; Mu et al.,\n2024). However, the previous segment-level compression methods, including Compressive\nTransformers (Rae et al., 2019) still discard the memory entries of old segments in order to\nfree up space for the new ones, limiting their context window to the most recent segments.\nThis is in contrast to our Infini-attention that computes incremental memory updates to a\nfixed amount of memory parameters in a recurrent fashion.\nLong-context continual pre-training. There is a line of work that extends the do-product\nattention layers and continues to train LLMs for long-context (Xiong et al., 2023; Fu et al.,\n2024). The attention extensions include incorporating sparsity into the attention layer (Chen\net al., 2023b; Ratner et al., 2022; Mohtashami & Jaggi, 2024) as well as manipulating the\nposition encodings (Chen et al., 2023a; Peng et al., 2023) Although the position encoding-\nbased methods such as position interpolation techniques (Chen et al., 2023a) can be data\nefficient as they only adjust the positional bias in the attention layer, they are still costly for\ninference.\nThe attention mechanism is also prone to the issues of attention sink (Xiao et al., 2023) and\nlost-in-the-middle (Liu et al., 2024). Consequently, they struggle in a regime where context\nlength is longer than what was observed during training (Press et al., 2021; Kazemnejad\net al., 2024). The proposed Infini-attention addresses those issues by enabling a segment-\nlevel streaming computation over long sequences with a fixed local attention window. Our\nInfini-Transformers successfully extrapolate to 1M input length regimes when trained on\n32K and even 5K length sequences.\nEfficient attention. The efficient attention techniques attempt to improve the efficiency of\nthe dot-product attention with an approximation or a system-level optimization.", "start_char_idx": 22218, "end_char_idx": 26136, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5f96973-ca2b-4ace-b207-ee5e6c7b4a9d": {"__data__": {"id_": "e5f96973-ca2b-4ace-b207-ee5e6c7b4a9d", "embedding": null, "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention", "node_type": "4", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2e0f0a65137957aa87b09912b7e4beee6c570c85d000c2836c629c317c737404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a96d7f4-1a22-4c2f-9cea-7b46be1a7674", "node_type": "1", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "3a199742379cfa9cd18c9d187a83595d50f2b4b1f702830708896238514b492d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea090aea-2a3c-408b-8190-945e25e739c6", "node_type": "1", "metadata": {}, "hash": "6df401c4f9e767bffb3fd00759538ba6c30e5c2102bc2d8316fa6e4f279f6091", "class_name": "RelatedNodeInfo"}}, "text": "The attention mechanism is also prone to the issues of attention sink (Xiao et al., 2023) and\nlost-in-the-middle (Liu et al., 2024). Consequently, they struggle in a regime where context\nlength is longer than what was observed during training (Press et al., 2021; Kazemnejad\net al., 2024). The proposed Infini-attention addresses those issues by enabling a segment-\nlevel streaming computation over long sequences with a fixed local attention window. Our\nInfini-Transformers successfully extrapolate to 1M input length regimes when trained on\n32K and even 5K length sequences.\nEfficient attention. The efficient attention techniques attempt to improve the efficiency of\nthe dot-product attention with an approximation or a system-level optimization. Multiple\ndirections have been explored for different forms of efficient attention approximation,\nincluding sparsity-based (Child et al., 2019; Beltagy et al., 2020; Sukhbaatar et al., 2021;\nDing et al., 2023) and linear attention approximation (Shen et al., 2018; Katharopoulos et al.,\n2020; Schlag et al., 2021). Among those, the linear attention variants are closely related\nto the associative memory matrix (Schlag et al., 2020; 2021) and the metalearned neural\nmemory (Munkhdalai et al., 2019), where KV bindings (Smolensky, 1990) are stored in\nFast-Weights (Hinton & Plaut, 1987; Schmidhuber, 1992; Ba et al., 2016) that are modified\nin with respect to new contextual information. More recently, system-level optimization\ntechniques have been proposed by leveraging specific hardware architecture to make the\nexact attention computation more efficient (Dao et al., 2022; Liu et al., 2023).\n8\nPreprint. Under review.\n5 Conclusion\nAn effective memory system is crucial not just for comprehending long contexts with LLMs,\nbut also for reasoning, planning, continual adaptation for fresh knowledge, and even for\nlearning how to learn. This work introduces a close integration of compressive memory mod-\nule into the vanilla dot-product attention layer. This subtle but critical modification to the\nattention layer enables LLMs to process infinitely long contexts with bounded memory and\ncomputation resources. We show that our approach can naturally scale to a million length\nregime of input sequences, while outperforming the baselines on long-context language\nmodeling benchmark and book summarization tasks. We also demonstrate a promising\nlength generalization capability of our approach. 1B model that was fine-tuned on up to 5K\nsequence length passkey instances solved the 1M length problem.\nReferences\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre\nPassos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2\ntechnical report. arXiv preprint arXiv:2305.10403 , 2023.\nJimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using\nfast weights to attend to the recent past. Advances in neural information processing systems ,\n29, 2016.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\njointly learning to align and translate. arXiv preprint arXiv:1409.0473 , 2014.\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document trans-\nformer. arXiv preprint arXiv:2004.05150 , 2020.\nAmanda Bertsch, Uri Alon, Graham Neubig, and Matthew Gormley. Unlimiformer: Long-\nrange transformers with unlimited length input. Advances in Neural Information Processing\nSystems , 36, 2024.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neural information processing systems ,\n33:1877\u20131901, 2020.\nAydar Bulatov, Yury Kuratov, and Mikhail Burtsev.", "start_char_idx": 25387, "end_char_idx": 29167, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea090aea-2a3c-408b-8190-945e25e739c6": {"__data__": {"id_": "ea090aea-2a3c-408b-8190-945e25e739c6", "embedding": null, "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention", "node_type": "4", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2e0f0a65137957aa87b09912b7e4beee6c570c85d000c2836c629c317c737404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5f96973-ca2b-4ace-b207-ee5e6c7b4a9d", "node_type": "1", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "7d7a190f8f5f9b227a554e6c3b637b9dfcc51804fcc87f736e79e82e2af75ae7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5250accc-3425-42a3-906f-902217ca1440", "node_type": "1", "metadata": {}, "hash": "e444cdba6c4d9601fbf0e2e7df406a10ec750ec4916ff3883f8916f4e462af52", "class_name": "RelatedNodeInfo"}}, "text": "Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document trans-\nformer. arXiv preprint arXiv:2004.05150 , 2020.\nAmanda Bertsch, Uri Alon, Graham Neubig, and Matthew Gormley. Unlimiformer: Long-\nrange transformers with unlimited length input. Advances in Neural Information Processing\nSystems , 36, 2024.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neural information processing systems ,\n33:1877\u20131901, 2020.\nAydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. Advances\nin Neural Information Processing Systems , 35:11079\u201311091, 2022.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending con-\ntext window of large language models via positional interpolation. arXiv preprint\narXiv:2306.15595 , 2023a.\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.\nLonglora: Efficient fine-tuning of long-context large language models. arXiv preprint\narXiv:2309.12307 , 2023b.\nJianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for\nmachine reading. arXiv preprint arXiv:1601.06733 , 2016.\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language\nmodels to compress contexts. arXiv preprint arXiv:2305.14788 , 2023.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with\nsparse transformers. arXiv preprint arXiv:1904.10509 , 2019.\nDjork-Arn \u00b4e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep\nnetwork learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289 , 2015.\n9\nPreprint. Under review.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhut-\ndinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv\npreprint arXiv:1901.02860 , 2019.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R \u00b4e. Flashattention: Fast\nand memory-efficient exact attention with io-awareness. Advances in Neural Information\nProcessing Systems , 35:16344\u201316359, 2022.\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang,\nNanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens.\narXiv preprint arXiv:2307.02486 , 2023.\nYao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and\nHao Peng. Data engineering for scaling language models to 128k context. arXiv preprint\narXiv:2402.10171 , 2024.\nTao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context\ncompression in a large language model. arXiv preprint arXiv:2307.06945 , 2023.\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint\narXiv:1410.5401 , 2014.", "start_char_idx": 28522, "end_char_idx": 31401, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5250accc-3425-42a3-906f-902217ca1440": {"__data__": {"id_": "5250accc-3425-42a3-906f-902217ca1440", "embedding": null, "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention", "node_type": "4", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2e0f0a65137957aa87b09912b7e4beee6c570c85d000c2836c629c317c737404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea090aea-2a3c-408b-8190-945e25e739c6", "node_type": "1", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "3ba6fe6a79ed019feb7a29d020d897b1066d134a36774cad0d3c267caf2d52ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55e2c282-a495-4863-8042-234f76ab1f2e", "node_type": "1", "metadata": {}, "hash": "7ae6ce5f20a6a771ea4d155c824aa9166eb8e7ff87b36a37e59b29cde89125e8", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2307.02486 , 2023.\nYao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and\nHao Peng. Data engineering for scaling language models to 128k context. arXiv preprint\narXiv:2402.10171 , 2024.\nTao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context\ncompression in a large language model. arXiv preprint arXiv:2307.06945 , 2023.\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint\narXiv:1410.5401 , 2014.\nDirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord,\nAnanya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Acceler-\nating the science of language models. arXiv preprint arXiv:2402.00838 , 2024.\nDonald Olding Hebb. The organization of behavior: A neuropsychological theory . Psychology\npress, 2005.\nGeoffrey E Hinton and David C Plaut. Using fast weights to deblur old memories. In\nProceedings of the ninth annual conference of the Cognitive Science Society , pp. 177\u2013186, 1987.\nJohn J Hopfield. Neural networks and physical systems with emergent collective computa-\ntional abilities. Proceedings of the national academy of sciences , 79(8):2554\u20132558, 1982.\nPentti Kanerva. Sparse distributed memory . MIT press, 1988.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran c \u00b8ois Fleuret. Transformers\nare rnns: Fast autoregressive transformers with linear attention. In International conference\non machine learning , pp. 5156\u20135165. PMLR, 2020.\nAmirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and\nSiva Reddy. The impact of positional encoding on length generalization in transformers.\nAdvances in Neural Information Processing Systems , 36, 2024.\nWojciech Kry \u00b4sci\u00b4nski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir\nRadev. Booksum: A collection of datasets for long-form narrative summarization. arXiv\npreprint arXiv:2105.08209 , 2021.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\nOmer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and comprehension. arXiv\npreprint arXiv:1910.13461 , 2019.\nHao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for\nnear-infinite context. arXiv preprint arXiv:2310.01889 , 2023.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,\nand Percy Liang. Lost in the middle: How language models use long contexts. Transactions\nof the Association for Computational Linguistics , 12:157\u2013173, 2024.\nThomas Miconi, Kenneth Stanley, and Jeff Clune. Differentiable plasticity: training plastic\nneural networks with backpropagation. In International Conference on Machine Learning ,\npp. 3559\u20133568. PMLR, 2018.\n10\nPreprint. Under review.\nAmirkeivan Mohtashami and Martin Jaggi. Random-access infinite context length for\ntransformers. Advances in Neural Information Processing Systems , 36, 2024.\nJesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens.\nAdvances in Neural Information Processing Systems , 36, 2024.", "start_char_idx": 30892, "end_char_idx": 34072, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55e2c282-a495-4863-8042-234f76ab1f2e": {"__data__": {"id_": "55e2c282-a495-4863-8042-234f76ab1f2e", "embedding": null, "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention", "node_type": "4", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2e0f0a65137957aa87b09912b7e4beee6c570c85d000c2836c629c317c737404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5250accc-3425-42a3-906f-902217ca1440", "node_type": "1", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "40b6ad7b4e183b08fe020026fb8418f69f46671d46f750a725fc0a5d792ebadb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd94529f-38e1-4590-b98c-29b46afc1e3e", "node_type": "1", "metadata": {}, "hash": "404d16254e68e48eca4b065a67357dd9f1ed436839ea2e6db945cf7c48243c7e", "class_name": "RelatedNodeInfo"}}, "text": "Lost in the middle: How language models use long contexts. Transactions\nof the Association for Computational Linguistics , 12:157\u2013173, 2024.\nThomas Miconi, Kenneth Stanley, and Jeff Clune. Differentiable plasticity: training plastic\nneural networks with backpropagation. In International Conference on Machine Learning ,\npp. 3559\u20133568. PMLR, 2018.\n10\nPreprint. Under review.\nAmirkeivan Mohtashami and Martin Jaggi. Random-access infinite context length for\ntransformers. Advances in Neural Information Processing Systems , 36, 2024.\nJesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens.\nAdvances in Neural Information Processing Systems , 36, 2024.\nTsendsuren Munkhdalai and Hong Yu. Meta networks. In International conference on machine\nlearning , pp. 2554\u20132563. PMLR, 2017a.\nTsendsuren Munkhdalai and Hong Yu. Neural semantic encoders. In Proceedings of the\nconference. Association for Computational Linguistics. Meeting , volume 1, pp. 397. NIH Public\nAccess, 2017b.\nTsendsuren Munkhdalai, John P Lalor, and Hong Yu. Citation analysis with neural attention\nmodels. In Proceedings of the Seventh International Workshop on Health Text Mining and\nInformation Analysis , pp. 69\u201377, 2016.\nTsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler. Metalearned\nneural memory. Advances in Neural Information Processing Systems , 32, 2019.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context\nwindow extension of large language models. arXiv preprint arXiv:2309.00071 , 2023.\nReiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury,\nJonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling trans-\nformer inference. Proceedings of Machine Learning and Systems , 5, 2023.\nOfir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear\nbiases enables input length extrapolation. arXiv preprint arXiv:2108.12409 , 2021.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive\ntransformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507 , 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. The Journal of Machine Learning Research , 21(1):5485\u20135551,\n2020.\nNir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Omri Abend, Ehud Karpas, Amnon\nShashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows improve\nin-context learning of large language models. arXiv preprint arXiv:2212.10947 , 2022.\nImanol Schlag, Paul Smolensky, Roland Fernandez, Nebojsa Jojic, J \u00a8urgen Schmidhuber,\nand Jianfeng Gao. Enhancing the transformer with explicit relational encoding for math\nproblem solving. arXiv preprint arXiv:1910.06611 , 2019.\nImanol Schlag, Tsendsuren Munkhdalai, and J \u00a8urgen Schmidhuber. Learning associative\ninference using fast weight memory. arXiv preprint arXiv:2011.07831 , 2020.\nImanol Schlag, Kazuki Irie, and J \u00a8urgen Schmidhuber. Linear transformers are secretly\nfast weight programmers. In International Conference on Machine Learning , pp. 9355\u20139366.\nPMLR, 2021.\nJ\u00a8urgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic\nrecurrent networks.", "start_char_idx": 33393, "end_char_idx": 36740, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd94529f-38e1-4590-b98c-29b46afc1e3e": {"__data__": {"id_": "cd94529f-38e1-4590-b98c-29b46afc1e3e", "embedding": null, "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention", "node_type": "4", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2e0f0a65137957aa87b09912b7e4beee6c570c85d000c2836c629c317c737404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55e2c282-a495-4863-8042-234f76ab1f2e", "node_type": "1", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "86fd17a0e10ea72a47bc1ad2c328c30253abb614d9447d51abbcd1576d8ef009", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb44d17a-a68d-40e0-8ef7-d555bea3b23c", "node_type": "1", "metadata": {}, "hash": "e65cb00f0a8313cfca2d164719725c46e9ecef55a8efc48fee2d282f6b983db0", "class_name": "RelatedNodeInfo"}}, "text": "Enhancing the transformer with explicit relational encoding for math\nproblem solving. arXiv preprint arXiv:1910.06611 , 2019.\nImanol Schlag, Tsendsuren Munkhdalai, and J \u00a8urgen Schmidhuber. Learning associative\ninference using fast weight memory. arXiv preprint arXiv:2011.07831 , 2020.\nImanol Schlag, Kazuki Irie, and J \u00a8urgen Schmidhuber. Linear transformers are secretly\nfast weight programmers. In International Conference on Machine Learning , pp. 9355\u20139366.\nPMLR, 2021.\nJ\u00a8urgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic\nrecurrent networks. Neural Computation , 4(1):131\u2013139, 1992.\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear\nmemory cost. In International Conference on Machine Learning , pp. 4596\u20134604. PMLR, 2018.\nZhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient\nattention: Attention with linear complexities. arXiv preprint arXiv:1812.01243 , 2018.\nPaul Smolensky. Tensor product variable binding and the representation of symbolic\nstructures in connectionist systems. Artificial intelligence , 46(1-2):159\u2013216, 1990.\n11\nPreprint. Under review.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks.\nAdvances in neural information processing systems , 28, 2015.\nSainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston,\nand Angela Fan. Not all memories are created equal: Learning to forget by expiring. In\nInternational Conference on Machine Learning , pp. 9902\u20139912. PMLR, 2021.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems , 30, 2017.\nYuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing\ntransformers. arXiv preprint arXiv:2203.08913 , 2022.\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient stream-\ning language models with attention sinks. arXiv preprint arXiv:2309.17453 , 2023.\nWen Xiao, Iz Beltagy, Giuseppe Carenini, and Arman Cohan. Primera: Pyramid-\nbased masked sentence pre-training for multi-document summarization. arXiv preprint\narXiv:2110.08499 , 2021.\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis\nMartin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective\nlong-context scaling of foundation models. arXiv preprint arXiv:2309.16039 , 2023.\nA Additional Training Details\nFor the long-context language modeling task, we set the learning rate to 0.01 by perform-\ning small search over values of 0.003, 0.005, 0.01 and 0.03. We used the Adafactor opti-\nmizer (Shazeer & Stern, 2018) with linear warmup with 1000 steps, followed by cosine\ndecay. We applied gradient checkpointing after each segment to save to save memory. The\nbatch size was set to 64.", "start_char_idx": 36156, "end_char_idx": 39328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb44d17a-a68d-40e0-8ef7-d555bea3b23c": {"__data__": {"id_": "cb44d17a-a68d-40e0-8ef7-d555bea3b23c", "embedding": null, "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention", "node_type": "4", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2e0f0a65137957aa87b09912b7e4beee6c570c85d000c2836c629c317c737404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd94529f-38e1-4590-b98c-29b46afc1e3e", "node_type": "1", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "83d316095ad3a6b0f8c2f6e7ad747212beefb79153dc365e5ddaaea162e0bde0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "313cf5ae-84a6-4bb7-8779-e640ba4f8bde", "node_type": "1", "metadata": {}, "hash": "a11ec67e52c3b7e3377d4bf31935cdd24c47e9d6f0f16e7e39a6a10f195f2efc", "class_name": "RelatedNodeInfo"}}, "text": "Effective\nlong-context scaling of foundation models. arXiv preprint arXiv:2309.16039 , 2023.\nA Additional Training Details\nFor the long-context language modeling task, we set the learning rate to 0.01 by perform-\ning small search over values of 0.003, 0.005, 0.01 and 0.03. We used the Adafactor opti-\nmizer (Shazeer & Stern, 2018) with linear warmup with 1000 steps, followed by cosine\ndecay. We applied gradient checkpointing after each segment to save to save memory. The\nbatch size was set to 64. For the LLM experiments, we set the learning rate to 0.0001 during\ncontinual pre-training and task fine-tuning.\nB Passkey Retrieval Task\nBelow we showed the input format of the passkey task.\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I\nwill quiz you about the important information there. The grass is green. The sky is blue. The sun\nis yellow. Here we go. There and back again. (repeat x times) The pass key is 9054 . Remember\nit.9054 is the pass key. The grass is green. The sky is blue. The sun is yellow. Here we go.\nThere and ack again. (repeat y times) What is the pass key? The pass key is\n12", "start_char_idx": 38828, "end_char_idx": 39979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "313cf5ae-84a6-4bb7-8779-e640ba4f8bde": {"__data__": {"id_": "313cf5ae-84a6-4bb7-8779-e640ba4f8bde", "embedding": null, "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models", "node_type": "4", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "ae4b74afb321dead2220dbf2ce32c7f9aca8886d0cfeea9547667981469ffd05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb44d17a-a68d-40e0-8ef7-d555bea3b23c", "node_type": "1", "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "bda92c04458ff581ce2c72c0c871a1a1c9284948eb761741a7e9d171ba73b554", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab935868-42a1-451c-aae5-ea359f68442b", "node_type": "1", "metadata": {}, "hash": "4d8f75eaa0a55d2bb3d4c706bd094fafc6822a792a1fb80f45b3a912aeb75cf3", "class_name": "RelatedNodeInfo"}}, "text": "Mixture-of-Depths: Dynamically allocating\ncompute in transformer-based language\nmodels\nDavid Raposo1*, Sam Ritter1, Blake Richards1,2, Timothy Lillicrap1, Peter Conway Humphreys1and Adam\nSantoro1*\n1Google DeepMind,2McGill University & Mila,*Equal Contribution\nTransformer-based language models spread FLOPs uniformly across input sequences. In this work we\ndemonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute ) to specific\npositions in a sequence, optimising the allocation along the sequence for different layers across the\nmodel depth. Our method enforces a total compute budget by capping the number of tokens ( \ud835\udc58) that\ncan participate in the self-attention and MLP computations at a given layer. The tokens to be processed\nare determined by the network using a top- \ud835\udc58routing mechanism. Since \ud835\udc58is defined a priori, this\nsimple procedure uses a static computation graph with known tensor sizes, unlike other conditional\ncomputation techniques. Nevertheless, since the identities of the \ud835\udc58tokens are fluid, this method can\nexpend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure\nis entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do\nmodels trained in this way learn to dynamically allocate compute, they do so efficiently. These models\nmatch baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of\nthe FLOPs per forward pass, and can be upwards of 50% faster to step during post-training sampling.\n1. Introduction\nNotallproblemsrequirethesameamountoftimeorefforttosolve. Analogously, inlanguagemodeling\nnot all tokens and sequences require the same time or effort to accurately make a prediction. And\nyet, transformer models expend the same amount of compute per token in a forward pass. Ideally,\ntransformers would use smaller total compute budgets by not spending compute unnecessarily.\nConditional computation is a technique that tries to reduce total compute by expending it only\nwhen needed (Bengio et al., 2016; Bengio, 2013; Bengio et al., 2013). Various algorithms offer\nsolutions to when and how much compute should be used (Ainslie et al., 2023; Bapna et al., 2020;\nFedus et al., 2022). However, general formulations of this challenging problem may not work well with\nexisting hardware constraints since they tend to introduce dynamic computation graphs (Dehghani\net al., 2018; Graves, 2016). The most promising conditional computation methods may instead be\nthose that are harmonious with our current hardware stack, which prioritizes static computation\ngraphs, and known tensor sizes that are selected to maximize hardware utilization.\nHere we consider the problem of language modeling using a staticcompute budget that can be\nmade less than that used by a vanilla transformer. The network must learn how to dynamically allocate\nthe available compute by making decisions per-token, in each layer, about where to spend compute\nfrom the available budget. In our implementation total compute is user defined and unchanging\nprior to training, rather than being a function of the network\u2019s on-the-fly decisions. Thus, hardware\nefficiency gains\u2014such as reduced memory footprint, or reduced FLOPs per forward pass\u2014can be\nanticipated and exploited ahead of time. As we will show, these gains can be had without sacrificing\noverall performance.\nCorresponding author(s): draposo@google.com, adamsantoro@google.com\n\u00a92024 Google DeepMind. All rights reservedarXiv:2404.02258v1  [cs.LG]  2 Apr 2024\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\nWe leverage an approach akin to Mixture of Experts (MoE) transformers, in which dynamic\ntoken-level routing decisions are made across the network depth. Departing from MoE, we choose\nto either apply a computation to a token (as would be the case for a standard transformer), or pass\nit through a residual connection (remaining unchanged and saving compute). Also in contrast to\nMoE, we apply this routing to both forward MLPs and multi-head attention. Since this therefore\nalso impacts the keys and queries we process, the routing makes decisions not only about which\ntokens to update, but also which tokens are made available to attend to.", "start_char_idx": 0, "end_char_idx": 4297, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab935868-42a1-451c-aae5-ea359f68442b": {"__data__": {"id_": "ab935868-42a1-451c-aae5-ea359f68442b", "embedding": null, "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models", "node_type": "4", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "ae4b74afb321dead2220dbf2ce32c7f9aca8886d0cfeea9547667981469ffd05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "313cf5ae-84a6-4bb7-8779-e640ba4f8bde", "node_type": "1", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "3f749da4522dc609b566d1d89c39bd5b151fe46ea4538721f25854e891e24051", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9daad84-c0c2-43c7-a8ec-a9bcb4d18198", "node_type": "1", "metadata": {}, "hash": "786aa4859651ad054ce72eddd3e52ea239f0d43f24a59e0620a493709d51467a", "class_name": "RelatedNodeInfo"}}, "text": "All rights reservedarXiv:2404.02258v1  [cs.LG]  2 Apr 2024\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\nWe leverage an approach akin to Mixture of Experts (MoE) transformers, in which dynamic\ntoken-level routing decisions are made across the network depth. Departing from MoE, we choose\nto either apply a computation to a token (as would be the case for a standard transformer), or pass\nit through a residual connection (remaining unchanged and saving compute). Also in contrast to\nMoE, we apply this routing to both forward MLPs and multi-head attention. Since this therefore\nalso impacts the keys and queries we process, the routing makes decisions not only about which\ntokens to update, but also which tokens are made available to attend to. We refer to this strategy\nas Mixture-of-Depths (MoD) to emphasize how individual tokens pass through different numbers of\nlayers, or blocks, through the depth of the transformer (see figure 1).\nThe MoD technique also allows one to trade-off performance with speed. On the one hand, one\ncan train an MoD transformer that improves upon vanilla transformers by as much as 1.5%on the\nfinal log probability training objective for equivalent training FLOPs (isoFLOP), and while taking an\nequivalent amount of wall-clock time to train. On the other hand, one can train an MoD transformer\nthat achieves training loss parity with an isoFLOP optimal vanilla transformer, but which uses a\nfraction of the FLOPs (upwards of 50%) per forward pass, and hence is faster to step. Together, these\nresults imply that MoD transformers learn to route intelligently (i.e., skipping computations that are\nunnecessary) since they can achieve equal or better log probabilities per sequence despite a smaller\nFLOP footprint per forward pass.\n2. Background\nThe transformer architecture has become the workhorse of a revolution in practical artificial in-\ntelligence, bringing unprecedented capabilities at the cost of expensive training runs and serving\nprocedures. This has spurred tremendous interest in making transformer architectures more effi-\ncient (Gupta and Agrawal, 2021; Tay et al., 2020). One of the promising approaches is conditional\ncomputation , whereby learned mechanisms determine when and how to expend computation. This\nterminology was introduced by Bengio (2013), and the concept was explored further over the next\nseveral years (Bengio et al., 2016, 2013; Cho and Bengio, 2014; Graves, 2016; Jernite et al., 2017;\nWang et al., 2017).\nA wide variety of recent work has developed conditional computation methods for transformers.\nSome of this work focuses on \"early exiting\", that is, learning to decide when to end computation on\na given token, allowing the token to skip any remaining transformer layers after the exit decision\nis made (Elbayad et al., 2019; Liu et al., 2021; Schuster et al., 2022). In MoD, unlike in early-exit\nmethods, a token can skip middle layers, then be updated via self-attention with tokens that that\nhave gone through all the middle layers. We speculate that this might be a useful property.\nOther work has developed methods for iterating transformer layers with shared weights for\nan adaptive number of steps (Dehghani et al., 2018; Simoulin and Crabb\u00e9, 2021). Bolya et al.\n(2023) developed a method for choosing tokens to merge when running inference on a trained vision\ntransformer which notably requires no learning. Lei et al. (2023) make use of conditional computation\nin a fine tuning setting by building on adapter approaches (He et al., 2021) to learn to skip blocks of\nfrozen pre-trained weights in favor of running only a small fine-tuned adapter.\nCoLT5 (Ainslie et al., 2023) uses conditional routing to select whether a given token will pass\nthrough a heavy or light pathway for each feedforward layer. Further, they use the same routing\nmechanism to select whether a token will attend to all other tokens or to a select few, as in Guo et al.\n(2022). Like MoD, CoLT5 uses soft top-k for making routing decisions. However, CoLT5 focuses on\na encoder-decoder setting, and thus does need to contend with the problem of efficient sequential\ndecoding given the non-causal nature of the top-k operation.", "start_char_idx": 3511, "end_char_idx": 7732, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9daad84-c0c2-43c7-a8ec-a9bcb4d18198": {"__data__": {"id_": "f9daad84-c0c2-43c7-a8ec-a9bcb4d18198", "embedding": null, "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models", "node_type": "4", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "ae4b74afb321dead2220dbf2ce32c7f9aca8886d0cfeea9547667981469ffd05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab935868-42a1-451c-aae5-ea359f68442b", "node_type": "1", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "f24ab077f5ff0b6cdb12ce07a7a3076358ce9900a9e2bce25c4b9574ded2e067", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68df46e3-3932-4d86-b538-cf3fa32c80b1", "node_type": "1", "metadata": {}, "hash": "022910967f5b7c638bce4ed737b15c1b64d462ea6158f9cdd9b8f4911f0977f2", "class_name": "RelatedNodeInfo"}}, "text": "Lei et al. (2023) make use of conditional computation\nin a fine tuning setting by building on adapter approaches (He et al., 2021) to learn to skip blocks of\nfrozen pre-trained weights in favor of running only a small fine-tuned adapter.\nCoLT5 (Ainslie et al., 2023) uses conditional routing to select whether a given token will pass\nthrough a heavy or light pathway for each feedforward layer. Further, they use the same routing\nmechanism to select whether a token will attend to all other tokens or to a select few, as in Guo et al.\n(2022). Like MoD, CoLT5 uses soft top-k for making routing decisions. However, CoLT5 focuses on\na encoder-decoder setting, and thus does need to contend with the problem of efficient sequential\ndecoding given the non-causal nature of the top-k operation. In contrast, our current work with\n2\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\nFigure 1|Mixture-of-Depths Transformer. As in mixture-of-experts (MoE) transformers we use a\nrouter to choose among potential computational paths. But unlike in MoE transformers the possible\nchoices are a standard block\u2019s computation (i.e., self-attention and MLP) or a residual connection.\nSince some tokens take this second route, Mixture-of-Depths (MoD) transformers have a smaller\ntotal FLOP footprint compared to vanilla or MoE transformers. On the top right is depicted a trained\nmodel\u2019sroutingdecisionsforashortsequencetruncatedto64tokensforvisualizationpurposes. When\nexamining the choices one can find tokens processed by later blocks\u2019 layers, despite passing through\nrelatively few total blocks throughout the model\u2019s depth. This is a unique feature of MoD compared\nto conventional halting-based, or \"early-exit\" conditional computation, which instead engage blocks\nserially, or vanilla transformers, which engage every block.\nMoD focuses on the decoder-only setting, and so we propose a predictive router to enable efficient\ninference for conditional computation in transformers.\nOne successful formulation of conditional computation is the the \"mixture-of-experts\" layer (MoE)\nas introduced by Shazeer et al. (2017). Developed initially in the context of LSTMs, later work showed\ncompelling empirical results for MoE with transformers (Fedus et al., 2022; Lepikhin et al., 2020;\nZoph et al., 2022). Unlike other conditional computation approaches that try to conserve or expend\nadditional compute, MoE transformers use conditional logic to route tokens to one of many expert\nMLPs while keeping total compute expenditure constant. Our mixture-of-depths method can be\nthought of as using the routing logic from MoE transformers, but rather than having multiple experts,\nMoD deploys a single expert which can be dynamically skipped.\n3. Implementing Mixture-of-Depths Transformers\nOur high-level strategy is as follows:\n\u2022Set a static compute budget that is less than that of an equivalent vanilla transformer by limiting\n3\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\nthe number of tokens in a sequence that can participate in a block\u2019s computations (i.e., self-\nattention and subsequent MLP). For example, while a vanilla transformer might permit all the\ntokens in a sequence to participate in self-attention, we might limit the number to 50% of the\ntokens in a sequence. See section 3.1.\n\u2022Use a per-block router to emit a scalar weight for each token, which expresses the router\u2019s\npreference for that token to participate in a block\u2019s computations or to route around it. See\nsection 3.2.\n\u2022Identify the top- \ud835\udc58scalar weights (per sequence, per block) to select those tokens that will\nparticipate in a block\u2019s computations. Since precisely \ud835\udc58tokens will participate in the block\u2019s\ncomputations, the computation graph and tensor sizes remain static throughout training; it is\nmerely the tokens\u2019 participation that is dynamic and context-sensitive, as determined by the\nrouter. See section 3.3.\nWe then discuss some complications when sampling post-training in section 3.5.\n3.1. Defining a compute budget\nTo enforce a total compute budget per forward pass we leverage the notion of capacity , which\ndefines the total number of tokens that comprise the input to a given computation (e.g., the tokens\nparticipatinginself-attention, agivenexpertinMoEtransformers, etc).", "start_char_idx": 6943, "end_char_idx": 11269, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68df46e3-3932-4d86-b538-cf3fa32c80b1": {"__data__": {"id_": "68df46e3-3932-4d86-b538-cf3fa32c80b1", "embedding": null, "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models", "node_type": "4", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "ae4b74afb321dead2220dbf2ce32c7f9aca8886d0cfeea9547667981469ffd05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9daad84-c0c2-43c7-a8ec-a9bcb4d18198", "node_type": "1", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "43dd7709f4b0979c7a003e53d697aa9f2342326a11ec595c611bbecacb5912cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abe64ea2-e9c7-4afe-89a7-0224c679db57", "node_type": "1", "metadata": {}, "hash": "f97138faca44b536c85976de8ef163cec80f37c38287caa57d05d2470eddb73f", "class_name": "RelatedNodeInfo"}}, "text": "See\nsection 3.2.\n\u2022Identify the top- \ud835\udc58scalar weights (per sequence, per block) to select those tokens that will\nparticipate in a block\u2019s computations. Since precisely \ud835\udc58tokens will participate in the block\u2019s\ncomputations, the computation graph and tensor sizes remain static throughout training; it is\nmerely the tokens\u2019 participation that is dynamic and context-sensitive, as determined by the\nrouter. See section 3.3.\nWe then discuss some complications when sampling post-training in section 3.5.\n3.1. Defining a compute budget\nTo enforce a total compute budget per forward pass we leverage the notion of capacity , which\ndefines the total number of tokens that comprise the input to a given computation (e.g., the tokens\nparticipatinginself-attention, agivenexpertinMoEtransformers, etc). Forexample, theself-attention\nand MLP in each vanilla transformer block have a capacity of \ud835\udc47\u2014the total number of tokens across\nthe sequence and batch. MoE transformers, on the other hand, use a capacity less than \ud835\udc47per expert\nMLP so as to more evenly divide the total compute across each expert. But, since they use multiple\nexperts per block, their total capacity is approximately equal to that of a vanilla transformer.\nGenerally, it is the token capacity that determines the total FLOPs for transformers that use\nconditional computation, rather than the outcomes of any routing decisions. This is because static-\ngraph implementations account for the worst-case scenarios decisions; e.g., a computation\u2019s inputs\nwill be padded to its capacity amount even if relatively few tokens actually end up routing to it,\nand/or tokens will be dropped from the computation if the capacity is exceeded.\nWe can achieve our goal of using a smaller compute budget per forward pass compared to a\nvanilla transformer by lowering the capacity of the computations. However, using a smaller compute\nbudget haphazardly will result in a performance degradation. We hypothesize that certaintokens\nmight not require as much processing as others, and these tokens can be identified through learning.\nTherefore, if the network learns to choose the right tokens to fill up its capacities, then it may preserve\nits performance. In the following we describe routing schemes that can be used for this purpose.\n3.2. Routing around transformer blocks\nWe consider the setting whereby we route tokens to one of two computational paths: (1) self-attention\nand MLP blocks, and (2) a residual connection. The latter is computationally cheap, and results in a\nblock output that is entirely determined by the value of its input. The former path is computationally\nexpensive.\nThe total number of FLOPs per forward pass will be fewer than that in a vanilla transformer if\nwe set the capacity for path (1) to be anything less than \ud835\udc47(the total number of tokens across the\nsequence and batch). For example, if we were to set a block\u2019s capacity to\ud835\udc47\n2(i.e., half the number\nof tokens as would be the case in a vanilla transformer) then query-times-key matrix multiplication\nduring self-attention becomes 25%as FLOP-intensive as in a vanilla transformer ( (\ud835\udc47\n2)2vs.\ud835\udc472). Similar\ncalculations can determine the FLOP-savings for the MLP.\n4\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\nIntuitively, the total FLOPs per forward pass decreases (and the time to complete a forward pass\ndecreases) in proportion to how aggressively we shrink the blocks\u2019 capacities. However, downstream\nperformance will also be affected by how aggressively we shrink the blocks capacities, and by the\nrouting algorithm we implement.\nAt one extreme, if we leave each block\u2019s capacity at \ud835\udc47and route every token to (rather than\naround) each block, then we recover a vanilla transformer. At the other extreme, if we set each\nblock\u2019s capacity to 0and route all tokens aroundeach block, then we\u2019re left with a very fast model\nthat doesn\u2019t engage with the vast majority of the transformer\u2019s parameters, and undoubtedly has\npoor downstream performance. We hypothesize that somewhere between these two extremes is an\noptimal model that is faster than a vanilla Transformer and performs as well, if not better, all while\nbeing faster to step.\n3.3. Routing schemes\nNaively, one can leverage stochasticity to route tokens, akin to layer or block \u201cdropout\u201d. We present\nthis routing scheme as a control, and will show that it significantly under-performs relative to vanilla\ntransformers.\nWe hypothesize that learnedrouting is preferable.", "start_char_idx": 10480, "end_char_idx": 14966, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abe64ea2-e9c7-4afe-89a7-0224c679db57": {"__data__": {"id_": "abe64ea2-e9c7-4afe-89a7-0224c679db57", "embedding": null, "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models", "node_type": "4", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "ae4b74afb321dead2220dbf2ce32c7f9aca8886d0cfeea9547667981469ffd05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68df46e3-3932-4d86-b538-cf3fa32c80b1", "node_type": "1", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "266fd9986c1f91983543874f0c0b10698960aa0e4cc327bd7629dd97fd0988a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d247bf45-ed8f-4e83-98d4-8f5cbb4af2b6", "node_type": "1", "metadata": {}, "hash": "16adf6e8179d681edc9f107868aee37f54a73dd37bcedc87bb8cdfb9d09ba08e", "class_name": "RelatedNodeInfo"}}, "text": "At one extreme, if we leave each block\u2019s capacity at \ud835\udc47and route every token to (rather than\naround) each block, then we recover a vanilla transformer. At the other extreme, if we set each\nblock\u2019s capacity to 0and route all tokens aroundeach block, then we\u2019re left with a very fast model\nthat doesn\u2019t engage with the vast majority of the transformer\u2019s parameters, and undoubtedly has\npoor downstream performance. We hypothesize that somewhere between these two extremes is an\noptimal model that is faster than a vanilla Transformer and performs as well, if not better, all while\nbeing faster to step.\n3.3. Routing schemes\nNaively, one can leverage stochasticity to route tokens, akin to layer or block \u201cdropout\u201d. We present\nthis routing scheme as a control, and will show that it significantly under-performs relative to vanilla\ntransformers.\nWe hypothesize that learnedrouting is preferable. Intuitively, the network should be able to learn\nwhich tokens require more or less processing than others. If we are correct that Transformers often\nexpend more compute than they need to make their predictions, then it is an empirical question as to\nhow aggressively we can shrink each block\u2019s capacity, and hence, how many tokens we can afford to\nroute around each block.\nThere are two learned routing schemes we consider (see figure 2): token-choice and expert-choice.\nIn token-choice routing, a router produces per-token probability distributions across computational\npaths (e.g., across expert identities in MoE Transformers). Tokens are then shuttled to the path\nthey prefer\u2014i.e., that with the highest probability\u2014and auxiliary losses ensure that all tokens don\u2019t\nconverge to the same path. Token-choice routing can have load balancing problems since there isn\u2019t a\nguarantee that tokens divide themselves appropriately between the possible paths. \u201cExpert choice\nrouting\u201d flips this recipe on its head: rather than having tokens choose the path they prefer, each\npath instead chooses the top- \ud835\udc58tokens based on the tokens\u2019 preferences. This ensures a perfect load\nbalance since \ud835\udc58tokens are guaranteed to be shuttled to each path. However, it could result in over- or\nunder-processing of some tokens, since some tokens may be among the top- \ud835\udc58for multiple paths, or\nfor none of them.\nWe decided to leverage expert-choice routing for a few reasons. First, it obviates the need for an\nauxiliary balancing loss. Second, since the top- \ud835\udc58operation depends on the magnitude of the router\nweights, this routing scheme allows for relative routing weights to help determine which tokens most\nneed the block\u2019s computations; routers can try to ensurethat the most critical tokens are among the\ntop-\ud835\udc58by setting their weight appropriately, which is not possible with token-choice routing schemes.\nFor our specific use-case, wherein one computational path is essentially a null operation, it might\nbe critical that important tokens are routed away from the null operation. Third, because we only\nroute through two paths, a single top- \ud835\udc58operation can efficiently split the tokens into two mutually\nexclusive sets, one for each computational path, preventing the over- or under-processing problem\nmentioned above.\n5\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\nFigure 2|Routing schemes. Tokens are funnelled to the computational path of their choice when\nusing token-choice routing (left). If a given path exceeds its capacity (e.g., more than two tokens in\nthis example) then surplus tokens must be dropped (purple token). The exact token that is ultimately\ndropped depends on the precise implementation in the underlying code. For example, priority is\noften given to those tokens that come earlier in the sequence or batch order. With expert-choice\nrouting (middle), precisely \ud835\udc58(in this case, two) tokens are chosen per path using a top- \ud835\udc58mechanism\nacross the tokens\u2019 router weights. Here, tokens are dropped if they are not among the top- \ud835\udc58with\nrespect to any given path (orange token), and some tokens may even be funnelled to multiple paths\n(yellow token). In this work we deploy expert-choice routing (right). However, because we use just\na single path, we leverage the implicit knowledge that tokens will be dropped if \ud835\udc58is less than the\nsequence length so that we can route tokens away from the self-attention and MLP computations,\nthus expending fewer FLOPs in a given forward pass of the model.\n3.4.", "start_char_idx": 14075, "end_char_idx": 18503, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d247bf45-ed8f-4e83-98d4-8f5cbb4af2b6": {"__data__": {"id_": "d247bf45-ed8f-4e83-98d4-8f5cbb4af2b6", "embedding": null, "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models", "node_type": "4", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "ae4b74afb321dead2220dbf2ce32c7f9aca8886d0cfeea9547667981469ffd05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abe64ea2-e9c7-4afe-89a7-0224c679db57", "node_type": "1", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "bffd8458b931a104fc40993ef8632c92e81f41dc4373d1d3a4fdf18f9ccbbb05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f3aff55-9de9-4d75-9a37-2df328337463", "node_type": "1", "metadata": {}, "hash": "cf564879b039651a12a31ee4ce9cb792e83f0ca63ab2b167a20e272421ef069a", "class_name": "RelatedNodeInfo"}}, "text": "For example, priority is\noften given to those tokens that come earlier in the sequence or batch order. With expert-choice\nrouting (middle), precisely \ud835\udc58(in this case, two) tokens are chosen per path using a top- \ud835\udc58mechanism\nacross the tokens\u2019 router weights. Here, tokens are dropped if they are not among the top- \ud835\udc58with\nrespect to any given path (orange token), and some tokens may even be funnelled to multiple paths\n(yellow token). In this work we deploy expert-choice routing (right). However, because we use just\na single path, we leverage the implicit knowledge that tokens will be dropped if \ud835\udc58is less than the\nsequence length so that we can route tokens away from the self-attention and MLP computations,\nthus expending fewer FLOPs in a given forward pass of the model.\n3.4. Routing implementation\nAs a reminder of the high-level intuition, each token is processed by a router to produce a scalar\nweight, and the top- \ud835\udc58weights are then used to choose the token identities that will route through a\ntransformer\u2019s block, which comprises self-attention and the subsequent MLP.\nSuppose we have the set of token embeddings in a sequence of length \ud835\udc46for a given layer \ud835\udc59; that\nis\ud835\udc4b\ud835\udc59={\ud835\udc65\ud835\udc59\n\ud835\udc56|\ud835\udc56is an integer, 1\u2264\ud835\udc56\u2264\ud835\udc46}. The router weight for a given token embedding is a scalar\nproduced as a result of a linear projection, \ud835\udc5f\ud835\udc59\n\ud835\udc56=\ud835\udc64\ud835\udc47\n\ud835\udf03\ud835\udc65\ud835\udc59\n\ud835\udc56.\nOur goal is to use these router weights to determine the output of a block\u2019s computation of each\ntoken. Suppose \ud835\udc43\ud835\udefd(\ud835\udc45\ud835\udc59)is the\ud835\udefd-th percentile of the set of router weights \ud835\udc45\ud835\udc59, where\ud835\udefd=1\u2212\ud835\udc36/\ud835\udc46and\ud835\udc36\nis the user-defined capacity per batch element (an integer < \ud835\udc46that defines the number of tokens from\na sequence that will be processed by a given function). A block\u2019s output for a given token is:\n\ud835\udc65\ud835\udc59+1\n\ud835\udc56=(\n\ud835\udc5f\ud835\udc59\n\ud835\udc56\ud835\udc53\ud835\udc56(\u02dc\ud835\udc4b\ud835\udc59)+\ud835\udc65\ud835\udc59\n\ud835\udc56,if\ud835\udc5f\ud835\udc59\n\ud835\udc56> \ud835\udc43\ud835\udefd(\ud835\udc45\ud835\udc59)\n\ud835\udc65\ud835\udc59\n\ud835\udc56, if\ud835\udc5f\ud835\udc59\n\ud835\udc56< \ud835\udc43\ud835\udefd(\ud835\udc45\ud835\udc59)(1)\nHere, \u02dc\ud835\udc4b\ud835\udc59is the set of tokens whose router values \ud835\udc5f\ud835\udc59\n\ud835\udc56> \ud835\udc43\ud835\udefd(\ud835\udc45\ud835\udc59)(that is, the \u201ctop-k\u201d tokens), and \ud835\udc53\ncomprises self-attention and the subsequent MLP. Note that the output for a given token \ud835\udc65\ud835\udc59+1\n\ud835\udc56might\ndepend on other tokens \ud835\udc65\ud835\udc59\n\ud835\udc56\u2260\ud835\udc57because of the self-attention operation. The cardinality of \u02dc\ud835\udc4b\ud835\udc59is\ud835\udc36(or\ud835\udc58):\nthe user-defined capacity. Therefore, the mixture-of-depths transformer accrues compute savings\n6\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\nrelative to the baseline because the input to the block\u2019s computations \ud835\udc53comprise fewer tokens than\nusual (\ud835\udc36 < \ud835\udc46), rendering the self-attention and MLP less expensive.\nNotably, we multiply the output of the function \ud835\udc53by the router weights. This puts the router\nweights along the \u201cgradient path\u201d, thus subjecting them to the forces of gradient descent through the\ncourse of the language modeling task (We experimented with versions where the router weights are\nalso included along the computational path for those tokens that bypass the block\u2019s computations,\nbut it seems to be sufficient\u2014and implementationally simpler\u2014to only include the router weights\nalong the computational path for those tokens that do not bypass the block\u2019s computations).\n3.5. Sampling\nWhile expert-choice routing has a number of advantages, it has one distinct problem: the top- \ud835\udc58\noperation is non-causal.", "start_char_idx": 17724, "end_char_idx": 20867, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f3aff55-9de9-4d75-9a37-2df328337463": {"__data__": {"id_": "8f3aff55-9de9-4d75-9a37-2df328337463", "embedding": null, "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models", "node_type": "4", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "ae4b74afb321dead2220dbf2ce32c7f9aca8886d0cfeea9547667981469ffd05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d247bf45-ed8f-4e83-98d4-8f5cbb4af2b6", "node_type": "1", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "f8edd89538686e21fbef0f8a29baf4225962d8944382379cc376af46abd4b0ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "775b688a-1f18-4cc4-bb47-acc5492557e2", "node_type": "1", "metadata": {}, "hash": "287c003880cea2ecc0b35991527ee5892b1a2490cc7d76cb26b19d36a57fb8c1", "class_name": "RelatedNodeInfo"}}, "text": "Notably, we multiply the output of the function \ud835\udc53by the router weights. This puts the router\nweights along the \u201cgradient path\u201d, thus subjecting them to the forces of gradient descent through the\ncourse of the language modeling task (We experimented with versions where the router weights are\nalso included along the computational path for those tokens that bypass the block\u2019s computations,\nbut it seems to be sufficient\u2014and implementationally simpler\u2014to only include the router weights\nalong the computational path for those tokens that do not bypass the block\u2019s computations).\n3.5. Sampling\nWhile expert-choice routing has a number of advantages, it has one distinct problem: the top- \ud835\udc58\noperation is non-causal. This means that whether a given token\u2019s routing weight is among the top- \ud835\udc58\nfor the sequence depends on the values of the routing weights for tokens that come after it, which we\ndon\u2019t have access to when autoregressively sampling.\nWe tested two methods to work around this problem. The first introduces a simple auxiliary loss\nthat empirically affects the primary language modeling objective by approximately 0.2\u22120.3%, but\nallows us to sample from the model autoregressively. We use a binary cross-entropy loss wherein\nthe router\u2019s outputs provide the logits, and the top- \ud835\udc58selections of these logits provide the targets\n(i.e. 1 if a token was among the top- \ud835\udc58, and 0 if not). Intuitively, this loss centers the sigmoid of the\nrouter\u2019s outputs around 0.5; those tokens that are selected among the top-k are pressured to produce\nrouter outputs above 0.5, and those not among the top-k will be pressured to produce router outputs\nbelow 0.5. The second method introduces a small auxiliary MLP predictor (akin to a second router)\nthat receives the same inputs as the router (with a stop gradient), but whose output is a prediction\nwhether that token will be among the top- \ud835\udc58or not in the sequence. This method does not affect the\nlanguage modeling objective, and empirically does not significantly impact the step speed.\nEquipped with these new methods, we can sample autoregressively by choosing to route tokens\ntoorarounda block based on the router\u2019s output, which does not depend on any information from\nfuture tokens. We provide empirical evidence that this is a relatively easy auxiliary task that quickly\nachieves 99%accuracy.\n3.6. Training methods\nAll models use the same basic hyperparameter configurations (e.g. cosine schedules equal to 1\u00d7the\ntraining steps, 128 batch size, 2048 sequence length) except for changes to the number of layers,\nheads, and embedding size to produce differently sized models during isoFLOP analyses.\n4. Results\n4.1. Training, isoFLOP comparisons\nWe first trained models with a relatively small FLOP budget (6e18) to determine optimal hyperpa-\nrameters (see figure 3). In general, we found that MoD transformers drag the baseline isoFLOP curve\n\"down and to the right\". That is, the optimal MoD transformer achieves a lower loss than the optimal\nbaseline, and also has more parameters. A fortunate consequence of this effect is that there exist\nsmaller MoD models that, while they are not themselves isoFLOP optimal for their hyperparameter\nsetting, are nevertheless as- or better-performing than the optimal baseline model while being faster\nto step. For example, a 220M parameter MoD (figure 3 model #3) variant slightly outperforms\n7\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\nFigure 3|MoD hyperparameter tuning. Variants of the MoD transformer were trained for 6e18\nFLOPs to determine the optimal hyperparameters for further isoFLOP analyses. On the left plot, the\ngrey box indicates models that perform better than the isoFLOP optimal baseline. We found the best\nMoD variant to be that which has the option to route every other block, and which uses a top-k of\n256(so,256, or 12.5% of the sequence\u2019s tokens are processed by self-attention and the subsequent\nMLP, while 1792tokens, or 87.5%of the sequence\u2019s tokens route aroundthe block). Shown on the\nright are the learning curves for a selection of models. Notably, model #3 achieves equal performance\nto the isoFLOP optimal baseline but steps 66% faster, due to the relatively fewer FLOPs needed per\nforward pass.", "start_char_idx": 20155, "end_char_idx": 24413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "775b688a-1f18-4cc4-bb47-acc5492557e2": {"__data__": {"id_": "775b688a-1f18-4cc4-bb47-acc5492557e2", "embedding": null, "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models", "node_type": "4", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "ae4b74afb321dead2220dbf2ce32c7f9aca8886d0cfeea9547667981469ffd05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f3aff55-9de9-4d75-9a37-2df328337463", "node_type": "1", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "e240452c382b979a6355cbc29dc52cec4d9008759ce1c45bb6edce75fb1819b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc093955-24e9-48e7-91bd-134fc32fdf10", "node_type": "1", "metadata": {}, "hash": "59a6c1e0b3b7b40509e8757eaa5c2344a9c3c3290fe4ff7377e82e2e55b2fc9c", "class_name": "RelatedNodeInfo"}}, "text": "Variants of the MoD transformer were trained for 6e18\nFLOPs to determine the optimal hyperparameters for further isoFLOP analyses. On the left plot, the\ngrey box indicates models that perform better than the isoFLOP optimal baseline. We found the best\nMoD variant to be that which has the option to route every other block, and which uses a top-k of\n256(so,256, or 12.5% of the sequence\u2019s tokens are processed by self-attention and the subsequent\nMLP, while 1792tokens, or 87.5%of the sequence\u2019s tokens route aroundthe block). Shown on the\nright are the learning curves for a selection of models. Notably, model #3 achieves equal performance\nto the isoFLOP optimal baseline but steps 66% faster, due to the relatively fewer FLOPs needed per\nforward pass.\nthe isoFLOP optimal baseline (also 220M, figure 3 model #1), but is upwards of 60% faster to step\nduring training. Crucially, when run on equivalent hardware these two model variants take take\napproximately the same amount of wall-clock time to train (figure 3).\nWe tested routing every block or every other block, using capacities from 12.5% to 95% of the\ntotal sequence. While routing every otherblock was crucial for strong performance, we found that\naggressive capacity reduction was best (gradual improvements were observed when reducing the\ncapacitydownto12.5%ofthetotalsequence, correspondingto87.5%oftokensrouting aroundblocks,\nwith performance degrading beyond this point). So, it seems the network is robust to significant\ncapacity reductions as long as there is frequent opportunity for full capacity self-attention and MLP\ncomputations.\nLearned routing is crucial, as MoD transformers that use stochastic routing (implemented using a\ntop-\ud835\udc58operation on router weights sampled from a Gaussian distribution) perform drastically worse\nthan both the baseline and normal MoD transformer (figure 3).\nDepicted in figure 4 is an isoFLOP analysis for 6e18, 2e19, and 1e20 total FLOPs. The trend\nthat FLOP-optimal MoD transformers have more parameters than the baseline continues for these\nlarger FLOP budgets. Notably, there exist MoD variants that are appreciably faster to step than the\nisoFLOP-optimal baseline (measured as steps per second when training on equivalent hardware) while\nalso achieving a lower loss (in figure 4 we depict normalized FLOPs per forward pass rather than\nwall-clock step time per se, but from our experiments the two are tightly correlated. A similar plot\ncan be produced showing relative wall-clock step times and the same basic trend is present).\nStep-wise speed gains come from two sources. First, the FLOP-per-parameter ratio in MoD\n8\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\nFigure 4|isoFLOP analysis. We used the 12.5% capacity MoD variant to perform an isoFLOP analysis\nfor 6e18, 2e19, and 1e20 FLOPs, training models varying in size from 60M to 3B parameters. Depicted\non the right are the relative FLOPs per forward pass (normalized to the isoFLOP optimal baseline).\nThere exist MoD variants that are both faster to step (by virtue of requiring fewer FLOPs per forward\npass) and better performing than the isoFLOP optimal baseline.\ntransformers is less than in the baselines because some proportion of tokens are routed around blocks.\nSo, for a given model size, a transformer requires fewer FLOPs per forward pass. Second, since\nisoFLOP-optimal MoD transformers are both bigger and achieve a lower loss than the isoFLOP-optimal\nbaseline, there exist smaller MoD variants that perform as well or better than the isoFLOP-optimal\nbaseline, and these variants are faster to step because they are smaller. Altogether, then, there exist\nMoD transformers that perform as well as isoFLOP-optimal baselines and are faster to step, both\nbecause they use fewer FLOPs per parameter and because they use fewer parameters.\nFigure 4 also reveals another important finding: the optimal MoD transformer is that which uses\nas many FLOPs per forward pass as the isoFLOP optimal baseline.", "start_char_idx": 23659, "end_char_idx": 27670, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc093955-24e9-48e7-91bd-134fc32fdf10": {"__data__": {"id_": "cc093955-24e9-48e7-91bd-134fc32fdf10", "embedding": null, "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models", "node_type": "4", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "ae4b74afb321dead2220dbf2ce32c7f9aca8886d0cfeea9547667981469ffd05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "775b688a-1f18-4cc4-bb47-acc5492557e2", "node_type": "1", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b2aa7a1ba3e7e377e386f19de291206ffc4cd429e260833c51030a2a20d4a027", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdf9cd3e-f264-477b-8b6d-722054c50b74", "node_type": "1", "metadata": {}, "hash": "4695d7044907b8eabf5f020028e5b1ab9c139d7e3ac94179e246417434f06794", "class_name": "RelatedNodeInfo"}}, "text": "transformers is less than in the baselines because some proportion of tokens are routed around blocks.\nSo, for a given model size, a transformer requires fewer FLOPs per forward pass. Second, since\nisoFLOP-optimal MoD transformers are both bigger and achieve a lower loss than the isoFLOP-optimal\nbaseline, there exist smaller MoD variants that perform as well or better than the isoFLOP-optimal\nbaseline, and these variants are faster to step because they are smaller. Altogether, then, there exist\nMoD transformers that perform as well as isoFLOP-optimal baselines and are faster to step, both\nbecause they use fewer FLOPs per parameter and because they use fewer parameters.\nFigure 4 also reveals another important finding: the optimal MoD transformer is that which uses\nas many FLOPs per forward pass as the isoFLOP optimal baseline. This finding allows one to directly\npredict which sized MoD transformer will perform optimally for a given isoFLOP training budget: one\njust needs to tune the model size for a given MoD configuration (i.e., capacity and routing frequency)\nto produce a model that uses as many FLOPs per forward pass as the isoFLOP-optimal baseline, and\nthey will have the optimally performing MoD variant for that configuration. Empirically, we find that\nit is better to add depth than to add width when adding FLOPs to the model.\nNevertheless, while the FLOPs per forward pass determines which model will be the isoFLOP\noptimal, it does not predict whether the optimal loss will improve upon the baseline (see figure 3.\nNamely, the optimal capacity appears to be empirically determinable. We found that it is best to use\n12.5% capacity blocks, every other block.\nWe noticed that MoD transformers had memory savings relative to equivalently sized baseline\nmodels at larger sizes, with some variants requiring fewer total devices (i.e., a smaller TPU topology).\nWe did not study this extensively, but we anticipate that as one scales to larger models, these savings\ncouldbeanimportantconsiderationwhenchoosingmodelvariantstotrain, andcouldhavesignificant\npositive effects in regards to the KV cache size during autoregressive sampling.\nFigure 5 shows the routing decisions for an MoD transformer trained with interleaved routing\nblocks. Despite aggressive routing around the blocks, transformers are able to achieve performance\n9\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\nFigure 5|Routing analysis. We trained an MoD transformer that interleaved 12.5%capacity routing\nblocks with full-attention blocks. As expected, the number of tokens that route to (rather than around)\na block is sparse in routing blocks, though the network does sometimes preferentially route certain\ntokens to each block along its depth. This can be seen in the left figure that depicts routing decisions,\nwhere we observe a vertical band of dark blue towards the end of the sequence. As expected, the\ndistribution of router weights are as the auxiliary loss dictates: approximately 12.5%of weights are\nabove 0.5 and 87.5%are below (histogram, right).\nimprovements relative to baselines. We observe patterns that might warrant further study; namely,\nsome tokens appear to engage each block along the transformer\u2019s depth, while others decide to route\naround blocks whenever possible. Preliminary analyses suggest that the tokens that engage with\nblocks more frequently are correlated with output predictions that have higher entropy, which possibly\ncorresponds to predictions that are more difficult to make.\n4.2. Auto-regressive Evaluation\nWe evaluated MoD variants during auto-regressive sampling (see figure 6). Each model was tested\non exactly the same held-out data comprising 256000 sequences ( 500M tokens). When switching\nfrom the top- \ud835\udc58routing method to the predictor-based routing method we observed little performance\ndegradation. As in the training setting, there exist MoD variants that are better performing than the\nisoFLOP-optimal baseline, while requiring fewer FLOPs per forward pass. These results suggest that\nthe compute savings offered by MoD transformers should translate beyond the training setting.\n4.3. Mixture-of-Depths-and-Experts (MoDE)\nThe MoD technique can be naturally integrated with MoE models (together comprising MoDE models)\nin addition to vanilla transformers. In figure 7 we present results showing that the performance\nimprovments offered by MoD compound with those of MoE.", "start_char_idx": 26833, "end_char_idx": 31282, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdf9cd3e-f264-477b-8b6d-722054c50b74": {"__data__": {"id_": "bdf9cd3e-f264-477b-8b6d-722054c50b74", "embedding": null, "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models", "node_type": "4", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "ae4b74afb321dead2220dbf2ce32c7f9aca8886d0cfeea9547667981469ffd05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc093955-24e9-48e7-91bd-134fc32fdf10", "node_type": "1", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "bbe9e44bd4dc29001a0ddf7d9a067105b6da8ea4285931a60c25bd24ae79142f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3af10ba-05e8-4289-b39f-9c0b5a85bafa", "node_type": "1", "metadata": {}, "hash": "a4a4906d030675c4e66fcf2f8f1706b3590968e1df59cde4d79f16951f484e79", "class_name": "RelatedNodeInfo"}}, "text": "Each model was tested\non exactly the same held-out data comprising 256000 sequences ( 500M tokens). When switching\nfrom the top- \ud835\udc58routing method to the predictor-based routing method we observed little performance\ndegradation. As in the training setting, there exist MoD variants that are better performing than the\nisoFLOP-optimal baseline, while requiring fewer FLOPs per forward pass. These results suggest that\nthe compute savings offered by MoD transformers should translate beyond the training setting.\n4.3. Mixture-of-Depths-and-Experts (MoDE)\nThe MoD technique can be naturally integrated with MoE models (together comprising MoDE models)\nin addition to vanilla transformers. In figure 7 we present results showing that the performance\nimprovments offered by MoD compound with those of MoE. We tried two variants: in staged MoDE,\nwhich routes tokens around or towards blocks prior to the self-attention step, and integrated MoDE,\nwhich implements MoD routing by integrating \u201cno-op\u201d experts among the conventional MLP experts.\nThe former is advantageous because it allows for tokens to skip the self-attention step, while the\nlatter is advantageous because it simplifies the routing machinery. We noticed that implementing\nMoDE in the integrated manner was distinctly better than simply reducing the capacity of experts in\nconventional MoE models, and relying on token dropping to implement residual routing. We believe\nthis is because with the integrated MoDE machinery, tokens explicitly learn to choose the residual\npath around the experts, as opposed to preferring an expert but being dropped when implemented as\n10\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\nFigure 6|Auto-regressive evaluation. Switching from the non-causal top- \ud835\udc58routing scheme in\ntraining to a causal predictor-based approach during auto-regressive sampling leads to minimal\nperformance degradation. This is perhaps due to the ease of learning this prediction problem, which\nis upwards of 97% accurate soon into training.\na capacity reduction.\n5. Discussion\nMixture-of-Depths transformers empirically demonstrate that one can improve on isoFLOP-optimal\nbaseline performance with models that use fewer FLOPs per forward pass. This means that\u2014for a\ngiventrainingFLOPbudget\u2014wecantrainmodelsthatarebothfasterandbetterperformingthantheir\nbaseline counterparts. Previously, to train models that are both faster and as- or better-performing\nthan isoFLOP-optimal models, one would have to use surplus compute to overtrain smaller models\n(notably, this overtraining technique is still possible with MoD transformers, and speed gains should\ncompound).\nWhile MoD transformers require fewer FLOPs per forward pass, one cannot forego FLOPs indis-\ncriminately. Rather, it is crucial to use learned routing decisions\u2014much like in Mixture-of-Experts\ntransformers\u2014to determine whether a token should participate in self-attention and the subsequent\nMLP (requiring FLOPs), or not (saving FLOPs).We can then use any saved FLOPs by, for example,\nmaking the model bigger or training it for longer. Our results show that indeed FLOPs may be\ninefficiently used in vanilla transformer models, and that there may be more efficient ways for them\nto be expended.\nLearned routing mechanisms are sometimes non-causal ; that is, information about the future is\nusedtodetermineagiventoken\u2019sroutingdecision. Thisisgenerallytruefortop-kroutingmechanisms,\nwhich are useful because they forego the need for auxiliary balancing losses. However, top-k routing\nmechanisms present difficulties in post-training autoregressive sampling, where it is impossible to\nuse information about future token identities to determine routing decisions. In this work we show\nthat one can successfully use a top-k routing scheme during training, but not require it during later\nautoregressive sampling. Eiher a simple auxiliary classifier, or auxiliary loss on the router, is sufficient\nto learn the top- \ud835\udc58routing decisions such that it can mimic the top- \ud835\udc58decisions during autoregressive\n11\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\nFigure 7|Mixture-of-Depths-and-Experts (MoDE). The MoD technique can be implemented along-\nside MoE (together comprising MoDE models) in two straightforward manners: staged, which first\nimplementsMoDmachinerypriortoMoEmachinery, andintegrated, whichusesoneroutingoperation\nto funnel tokens to either experts or no-op operations.\nsampling, with minimal to no performance degradation.", "start_char_idx": 30484, "end_char_idx": 35016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3af10ba-05e8-4289-b39f-9c0b5a85bafa": {"__data__": {"id_": "e3af10ba-05e8-4289-b39f-9c0b5a85bafa", "embedding": null, "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models", "node_type": "4", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "ae4b74afb321dead2220dbf2ce32c7f9aca8886d0cfeea9547667981469ffd05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdf9cd3e-f264-477b-8b6d-722054c50b74", "node_type": "1", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "f091d7a36da781012e86509246f2ccb95cb68ffbfdf3dfa9e141403f600f1324", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71b3447b-d1c9-4ebe-bda1-5927740a15e2", "node_type": "1", "metadata": {}, "hash": "def22e230006fa3362d02a2ca5acb52b4b1d8ebcf993643e24878076d0dab7eb", "class_name": "RelatedNodeInfo"}}, "text": "In this work we show\nthat one can successfully use a top-k routing scheme during training, but not require it during later\nautoregressive sampling. Eiher a simple auxiliary classifier, or auxiliary loss on the router, is sufficient\nto learn the top- \ud835\udc58routing decisions such that it can mimic the top- \ud835\udc58decisions during autoregressive\n11\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\nFigure 7|Mixture-of-Depths-and-Experts (MoDE). The MoD technique can be implemented along-\nside MoE (together comprising MoDE models) in two straightforward manners: staged, which first\nimplementsMoDmachinerypriortoMoEmachinery, andintegrated, whichusesoneroutingoperation\nto funnel tokens to either experts or no-op operations.\nsampling, with minimal to no performance degradation.\nIntuitively, a token might learn to route around blocks because the prediction being made at that\nstep is easier, and hence, does not require as much compute. However, this strategy is undoubtedly\nnot all that the network learns. If a token does not participate in self-attention at a certain block,\nthen later tokens will also not be able to attend to it. Thus, whether tokens decide to route or not\nimpacts both the current step\u2019s prediction and future predictions via causal self-attention, and how\nthe network balances these effects is guided by their influence on the overall language modeling\nobjective.\nThis insight opens the door to MoD variants that decouple the routing for queries, keys and values.\nFor example, perhaps a token would prefer to be among the queries, but not the keys, for a given\nself-attention computation. One can imagine extending this idea even further into the domain of\n\"long-term memory\": perhaps there are tokens that would be extremely valuable as keys, regardless\nof whether it is useful for them to also be among the queries at the step of their occurrence. Learned\nrouting could be a powerful mechanism for deciding which tokens these might be, perhaps funnelling\nthem into a long-term memory buffer that is available during future self-attention. One advantage\nof such an approach to long-term memory is that tokens decide once, at the moment of \"memory\nencoding\", whether they should be retrieved in the future. This is more computationally efficient\nthan performing a full content-based lookup across an entire memory buffer for each step in the\nfuture, and could be one step towards drastically increasing the context-length available for making a\nprediction.\nUnlike MoE transformers that route between effectively the same computation (usually MLPs),\n12\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\nMoD transformers demonstrate the value of routing among different typesof computations. In this\nwork the types were either the conventional transformer block, or a null computation (functionally\nequivalent to multiplying by zero). However, one can imagine extending this idea further by routing\nbetween even more types of computation. For example, perhaps some tokens are routed to \"memory\nlookup\" functions, and others are routed to \"tool use\" functions. In general, the routing machinery we\ndeployed provides a knob for adjusting the types of computations available to the network and their\nrelative cost (in total FLOPs); if one wants to introduce an expensive computation, then this can be\noffset by setting its capacity to some small amount, and hence, by routing only a small number of\ntokens to it.\nAltogether, MoD transformers are another tool one can use to tune a model\u2019s compute per forward\npass (and hence inference time). The machinery used to implement MoD is also generic, and opens\nthe doors to many extensions and integration with other techniques, such as MoE.\nReferences\nJ. Ainslie, T. Lei, M. de Jong, S. Onta\u00f1\u00f3n, S. Brahma, Y. Zemlyanskiy, D. Uthus, M. Guo, J. Lee-\nThorp, Y. Tay, Y.-H. Sung, and S. Sanghai. Colt5: Faster long-range transformers with conditional\ncomputation, 2023.\nA. Bapna, N. Arivazhagan, and O. Firat. Controlling computation versus quality for neural sequence\nmodels. CoRR, abs/2002.07106, 2020. URL https://arxiv.org/abs/2002.07106 .\nE. Bengio, P.-L. Bacon, J. Pineau, and D. Precup. Conditional computation in neural networks for\nfaster models, 2016.\nY. Bengio.", "start_char_idx": 34210, "end_char_idx": 38508, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71b3447b-d1c9-4ebe-bda1-5927740a15e2": {"__data__": {"id_": "71b3447b-d1c9-4ebe-bda1-5927740a15e2", "embedding": null, "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models", "node_type": "4", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "ae4b74afb321dead2220dbf2ce32c7f9aca8886d0cfeea9547667981469ffd05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3af10ba-05e8-4289-b39f-9c0b5a85bafa", "node_type": "1", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "1c7499a0c18ac1d6ad86ea4f66b8ca419d1f419f837cc53731c6e96e1b61d5f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62116594-a9ba-407a-bfb9-6339b47cddc3", "node_type": "1", "metadata": {}, "hash": "588be1f92bd0fd67fd34ac67707f59cc7bd215ef344769bb02ea67b8272a037f", "class_name": "RelatedNodeInfo"}}, "text": "Sung, and S. Sanghai. Colt5: Faster long-range transformers with conditional\ncomputation, 2023.\nA. Bapna, N. Arivazhagan, and O. Firat. Controlling computation versus quality for neural sequence\nmodels. CoRR, abs/2002.07106, 2020. URL https://arxiv.org/abs/2002.07106 .\nE. Bengio, P.-L. Bacon, J. Pineau, and D. Precup. Conditional computation in neural networks for\nfaster models, 2016.\nY. Bengio. Deep learning of representations: Looking forward, 2013.\nY. Bengio, N. L\u00e9onard, and A. Courville. Estimating or propagating gradients through stochastic\nneurons for conditional computation, 2013.\nD. Bolya, C.-Y. Fu, X. Dai, P. Zhang, C. Feichtenhofer, and J. Hoffman. Token merging: Your vit but\nfaster, 2023.\nK. Cho and Y. Bengio. Exponentially increasing the capacity-to-computation ratio for conditional\ncomputation in deep learning, 2014.\nM. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and \u0141. Kaiser. Universal transformers. arXiv preprint\narXiv:1807.03819 , 2018.\nM. Elbayad, J. Gu, E. Grave, and M. Auli. Depth-adaptive transformer. CoRR, abs/1910.10073, 2019.\nURLhttp://arxiv.org/abs/1910.10073 .\nW. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with\nsimple and efficient sparsity, 2022.\nA. Graves. Adaptive computation time for recurrent neural networks. CoRR, abs/1603.08983, 2016.\nURLhttp://arxiv.org/abs/1603.08983 .\nM. Guo, J. Ainslie, D. Uthus, S. Ontanon, J. Ni, Y.-H. Sung, and Y. Yang. Longt5: Efficient text-to-text\ntransformer for long sequences, 2022.\nM. Gupta and P. Agrawal. Compression of deep learning models for text: A survey, 2021.\n13\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\nJ.He,C.Zhou,X.Ma,T.Berg-Kirkpatrick,andG.Neubig. Towardsaunifiedviewofparameter-efficient\ntransfer learning. arXiv preprint arXiv:2110.04366 , 2021.\nY. Jernite, E. Grave, A. Joulin, and T. Mikolov. Variable computation in recurrent neural networks,\n2017.\nT. Lei, J. Bai, S. Brahma, J. Ainslie, K. Lee, Y. Zhou, N. Du, V. Y. Zhao, Y. Wu, B. Li, Y. Zhang, and\nM.-W. Chang. Conditional adapters: Parameter-efficient transfer learning with fast inference, 2023.\nD. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen.\nGshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint\narXiv:2006.16668 , 2020.\nZ. Liu, Z. Xu, H.-J. Wang, T. Darrell, and E. Shelhamer. Anytime dense prediction with confidence\nadaptivity. arXiv preprint arXiv:2104.00749 , 2021.\nT. Schuster, A. Fisch, J. Gupta, M. Dehghani, D. Bahri, V. Q. Tran, Y. Tay, and D. Metzler. Confident\nadaptive language modeling, 2022.\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large\nneural networks: The sparsely-gated mixture-of-experts layer.", "start_char_idx": 38110, "end_char_idx": 40922, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62116594-a9ba-407a-bfb9-6339b47cddc3": {"__data__": {"id_": "62116594-a9ba-407a-bfb9-6339b47cddc3", "embedding": null, "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models", "node_type": "4", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "ae4b74afb321dead2220dbf2ce32c7f9aca8886d0cfeea9547667981469ffd05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71b3447b-d1c9-4ebe-bda1-5927740a15e2", "node_type": "1", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "81415e4179ff4ddeca96ecc4ef3e616868fa33fecbb735b3f89640b7466f7a0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17656648-675c-4e4c-9f74-f8a1898a0e13", "node_type": "1", "metadata": {}, "hash": "6715f492279f2ff767431b7c1646f8d4d7381c82d0d96f45ce5e8ddb04c6f2c4", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint\narXiv:2006.16668 , 2020.\nZ. Liu, Z. Xu, H.-J. Wang, T. Darrell, and E. Shelhamer. Anytime dense prediction with confidence\nadaptivity. arXiv preprint arXiv:2104.00749 , 2021.\nT. Schuster, A. Fisch, J. Gupta, M. Dehghani, D. Bahri, V. Q. Tran, Y. Tay, and D. Metzler. Confident\nadaptive language modeling, 2022.\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large\nneural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 ,\n2017.\nA.SimoulinandB.Crabb\u00e9. Howmanylayersandwhy? Ananalysisofthemodeldepthintransformers.\nInProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Processing: Student Research Workshop ,\npages 221\u2013228, Online, Aug. 2021. Association for Computational Linguistics. doi: 10.18653/v1/\n2021.acl-srw.23. URL https://aclanthology.org/2021.acl-srw.23 .\nY. Tay, M. Dehghani, D. Bahri, and D. Metzler. Efficient transformers: A survey. CoRR, abs/2009.06732,\n2020. URL https://arxiv.org/abs/2009.06732 .\nX. Wang, F. Yu, Z. Dou, and J. E. Gonzalez. Skipnet: Learning dynamic routing in convolutional\nnetworks. CoRR, abs/1711.09485, 2017. URL http://arxiv.org/abs/1711.09485 .\nB. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer, and W. Fedus. St-moe: Designing\nstable and transferable sparse expert models, 2022.\n14", "start_char_idx": 40436, "end_char_idx": 41873, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17656648-675c-4e4c-9f74-f8a1898a0e13": {"__data__": {"id_": "17656648-675c-4e4c-9f74-f8a1898a0e13", "embedding": null, "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "attention_is_all_you_need", "node_type": "4", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b772de61c571b5c2b1faf24bb4153e24ceecb37d9c22944aee6be95e4b266c23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62116594-a9ba-407a-bfb9-6339b47cddc3", "node_type": "1", "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "6a1470c8d31c39ca25688c8a98bab04801ace7a6c1b046a9c4464be468c5fc9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46161918-fde5-404b-a93f-cd7f392f1a8f", "node_type": "1", "metadata": {}, "hash": "432844f06dee03f3ad01deb745edb049df18875633121583502039667947b05b", "class_name": "RelatedNodeInfo"}}, "text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\u2217\nGoogle Brain\navaswani@google.comNoam Shazeer\u2217\nGoogle Brain\nnoam@google.comNiki Parmar\u2217\nGoogle Research\nnikip@google.comJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.comAidan N. Gomez\u2217 \u2020\nUniversity of Toronto\naidan@cs.toronto.edu\u0141ukasz Kaiser\u2217\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin\u2217 \u2021\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n1 Introduction\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.", "start_char_idx": 0, "end_char_idx": 4585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46161918-fde5-404b-a93f-cd7f392f1a8f": {"__data__": {"id_": "46161918-fde5-404b-a93f-cd7f392f1a8f", "embedding": null, "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "attention_is_all_you_need", "node_type": "4", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b772de61c571b5c2b1faf24bb4153e24ceecb37d9c22944aee6be95e4b266c23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17656648-675c-4e4c-9f74-f8a1898a0e13", "node_type": "1", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b540b7618b72c2d5c2e9828b3c7d348a015d0b77a38f23bfb25dde1d05342a4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "591ef5c2-b978-4a27-91a1-c337d874dd70", "node_type": "1", "metadata": {}, "hash": "ea64a94023c6ccf0a90c6e7fd1aec19ed024f648c62f9e25137da9ad2907a1da", "class_name": "RelatedNodeInfo"}}, "text": "Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512 .\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions.", "start_char_idx": 3762, "end_char_idx": 8530, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "591ef5c2-b978-4a27-91a1-c337d874dd70": {"__data__": {"id_": "591ef5c2-b978-4a27-91a1-c337d874dd70", "embedding": null, "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "attention_is_all_you_need", "node_type": "4", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b772de61c571b5c2b1faf24bb4153e24ceecb37d9c22944aee6be95e4b266c23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46161918-fde5-404b-a93f-cd7f392f1a8f", "node_type": "1", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "c4dcc0c89fcd37396d8af135a4d0e20ca519c58b8bef488bfe9a0be7993e90d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76890a72-4f06-4667-acc0-76843cc527a1", "node_type": "1", "metadata": {}, "hash": "05a7d5f8697d82c5536525eba24b08ddc5587d5e39ec5abe11321f813e1b2228", "class_name": "RelatedNodeInfo"}}, "text": "That is, the output of each sub-layer is\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512 .\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position ican depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by\u221adk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\nthe matrix of outputs as:\nAttention( Q, K, V ) = softmax(QKT\n\u221adk)V (1)\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof1\u221adk. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients4. To counteract this effect, we scale the dot products by1\u221adk.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q\u00b7k=Pdk\ni=1qiki, has mean 0and variance dk.\n4\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\nwhere head i= Attention( QWQ\ni, KWK\ni, V WV\ni)\nWhere the projections are parameter matrices WQ\ni\u2208Rdmodel\u00d7dk,WK\ni\u2208Rdmodel\u00d7dk,WV\ni\u2208Rdmodel\u00d7dv\nandWO\u2208Rhdv\u00d7dmodel.\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\ndk=dv=dmodel/h= 64 .", "start_char_idx": 7753, "end_char_idx": 12055, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76890a72-4f06-4667-acc0-76843cc527a1": {"__data__": {"id_": "76890a72-4f06-4667-acc0-76843cc527a1", "embedding": null, "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "attention_is_all_you_need", "node_type": "4", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b772de61c571b5c2b1faf24bb4153e24ceecb37d9c22944aee6be95e4b266c23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "591ef5c2-b978-4a27-91a1-c337d874dd70", "node_type": "1", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "4073babf1adfd6e21c694aac2763b3e846092bd3ff78be579d9b41eb62eb6ba1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f24a72f4-a72a-45c6-9a50-8a8d05513a0b", "node_type": "1", "metadata": {}, "hash": "23ea627794406bdb7dd199c2e443b399279f9b48cc1e428cc4e67edb791113e9", "class_name": "RelatedNodeInfo"}}, "text": "4\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\nwhere head i= Attention( QWQ\ni, KWK\ni, V WV\ni)\nWhere the projections are parameter matrices WQ\ni\u2208Rdmodel\u00d7dk,WK\ni\u2208Rdmodel\u00d7dk,WV\ni\u2208Rdmodel\u00d7dv\nandWO\u2208Rhdv\u00d7dmodel.\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n\u2022In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by\u221admodel.\n5\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2\u00b7d) O(1) O(1)\nRecurrent O(n\u00b7d2) O(n) O(n)\nConvolutional O(k\u00b7n\u00b7d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r\u00b7n\u00b7d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed.", "start_char_idx": 11422, "end_char_idx": 15573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f24a72f4-a72a-45c6-9a50-8a8d05513a0b": {"__data__": {"id_": "f24a72f4-a72a-45c6-9a50-8a8d05513a0b", "embedding": null, "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "attention_is_all_you_need", "node_type": "4", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b772de61c571b5c2b1faf24bb4153e24ceecb37d9c22944aee6be95e4b266c23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76890a72-4f06-4667-acc0-76843cc527a1", "node_type": "1", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "6607e6097cb3be5a37ca3d59ca9a55c5f1ed73ebd953d581d9aa9802c14d84e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5caa6f1-858b-4b0e-9d43-50307d4c33a0", "node_type": "1", "metadata": {}, "hash": "9a15d892564534da1f89f704a3dfa1cdd6225f355251b96e4adb7e800fe3b316", "class_name": "RelatedNodeInfo"}}, "text": "Layer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2\u00b7d) O(1) O(1)\nRecurrent O(n\u00b7d2) O(n) O(n)\nConvolutional O(k\u00b7n\u00b7d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r\u00b7n\u00b7d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i)=sin(pos/100002i/d model)\nPE(pos,2i+1)=cos(pos/100002i/d model)\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0to10000 \u00b72\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi\u2208Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6\nlength nis smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k\u00b7n\u00b7d+n\u00b7d2).", "start_char_idx": 14876, "end_char_idx": 19150, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5caa6f1-858b-4b0e-9d43-50307d4c33a0": {"__data__": {"id_": "c5caa6f1-858b-4b0e-9d43-50307d4c33a0", "embedding": null, "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "attention_is_all_you_need", "node_type": "4", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b772de61c571b5c2b1faf24bb4153e24ceecb37d9c22944aee6be95e4b266c23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f24a72f4-a72a-45c6-9a50-8a8d05513a0b", "node_type": "1", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "a4ef224a9a8a7b1841d0c3e7698a4cf8457f44de2662402d286c3089038e7c79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d82c73e-304a-4f3a-80c2-3e64487ad704", "node_type": "1", "metadata": {}, "hash": "53fd909b22c3faf45903f9c97473b60f28f1b24841e90d278a42cd11070000c3", "class_name": "RelatedNodeInfo"}}, "text": "To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k\u00b7n\u00b7d+n\u00b7d2). Even with k=n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [ 20] with \u03b21= 0.9,\u03b22= 0.98and\u03f5= 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate =d\u22120.5\nmodel\u00b7min(step_num\u22120.5, step _num\u00b7warmup _steps\u22121.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup _steps = 4000 .\n5.4 Regularization\nWe employ three types of regularization during training:\n7\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.", "start_char_idx": 18281, "end_char_idx": 21541, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d82c73e-304a-4f3a-80c2-3e64487ad704": {"__data__": {"id_": "3d82c73e-304a-4f3a-80c2-3e64487ad704", "embedding": null, "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "attention_is_all_you_need", "node_type": "4", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b772de61c571b5c2b1faf24bb4153e24ceecb37d9c22944aee6be95e4b266c23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5caa6f1-858b-4b0e-9d43-50307d4c33a0", "node_type": "1", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "5a3cc399b273c4527d48d1922cd01ad1262abe4a93d5dc246fed4f30989e5005", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b5a090b-5488-4693-b018-fc85db5e5597", "node_type": "1", "metadata": {}, "hash": "9159a344f412b5d28faf6e566feb22c3a861b83ca0604bfc65350690b4dfd234", "class_name": "RelatedNodeInfo"}}, "text": "We varied the learning\nrate over the course of training, according to the formula:\nlrate =d\u22120.5\nmodel\u00b7min(step_num\u22120.5, step _num\u00b7warmup _steps\u22121.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup _steps = 4000 .\n5.4 Regularization\nWe employ three types of regularization during training:\n7\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModelBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0\u00b71020\nGNMT + RL [38] 24.6 39.92 2.3\u00b710191.4\u00b71020\nConvS2S [9] 25.16 40.46 9.6\u00b710181.5\u00b71020\nMoE [32] 26.03 40.56 2.0\u00b710191.2\u00b71020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0\u00b71020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8\u00b710201.1\u00b71021\nConvS2S Ensemble [9] 26.36 41.29 7.7\u00b710191.2\u00b71021\nTransformer (base model) 27.3 38.1 3.3\u00b71018\nTransformer (big) 28.4 41.8 2.3\u00b71019\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop= 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls= 0.1[36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop= 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4and length penalty \u03b1= 0.6[38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU5.", "start_char_idx": 20894, "end_char_idx": 24194, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b5a090b-5488-4693-b018-fc85db5e5597": {"__data__": {"id_": "2b5a090b-5488-4693-b018-fc85db5e5597", "embedding": null, "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "attention_is_all_you_need", "node_type": "4", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b772de61c571b5c2b1faf24bb4153e24ceecb37d9c22944aee6be95e4b266c23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d82c73e-304a-4f3a-80c2-3e64487ad704", "node_type": "1", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "1c94958f343223867aeaa953fd5f298b0aaf001cbaa2076932ef9c30c1242161", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6ae20ed-1773-4a80-bc6e-bcc837a7ea87", "node_type": "1", "metadata": {}, "hash": "10bfeaedec1e91734b611a6adbd3c0633f707dc03c60fe0baa74e2f710218cc9", "class_name": "RelatedNodeInfo"}}, "text": "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4and length penalty \u03b1= 0.6[38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dvPdrop \u03f5lstrain PPL BLEU params\nsteps (dev) (dev) \u00d7106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B)16 5.16 25.1 58\n32 5.01 25.4 60\n(C)2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].", "start_char_idx": 23383, "end_char_idx": 26749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6ae20ed-1773-4a80-bc6e-bcc837a7ea87": {"__data__": {"id_": "a6ae20ed-1773-4a80-bc6e-bcc837a7ea87", "embedding": null, "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "attention_is_all_you_need", "node_type": "4", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b772de61c571b5c2b1faf24bb4153e24ceecb37d9c22944aee6be95e4b266c23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b5a090b-5488-4693-b018-fc85db5e5597", "node_type": "1", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "7566ece69353ec42975976dbf40d9d4d8476880a1b31a9aba24e71a1190660ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6fe8244d-d01a-4367-bef8-6bf6b29496ee", "node_type": "1", "metadata": {}, "hash": "a82d387a1a7506c17196cec4e5d2dee6294187d04efdf8e1f39ac6f838663ddb", "class_name": "RelatedNodeInfo"}}, "text": "This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21and\u03b1= 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor .\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.", "start_char_idx": 25894, "end_char_idx": 29994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6fe8244d-d01a-4367-bef8-6bf6b29496ee": {"__data__": {"id_": "6fe8244d-d01a-4367-bef8-6bf6b29496ee", "embedding": null, "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "attention_is_all_you_need", "node_type": "4", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b772de61c571b5c2b1faf24bb4153e24ceecb37d9c22944aee6be95e4b266c23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6ae20ed-1773-4a80-bc6e-bcc837a7ea87", "node_type": "1", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "a22111e1713eddae4fd60bca8e57fd6ad4c4322e0d5a8d588bda37548492de45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0296c0e0-2a66-4aed-986c-a3fa62c6a5fd", "node_type": "1", "metadata": {}, "hash": "a16604f1747a7431934075c8c2089be44e6d94dc12f92826fc68426bd84dfaf9", "class_name": "RelatedNodeInfo"}}, "text": "On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor .\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733 , 2016.\n10\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR , abs/1406.1078, 2014.\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357 , 2016.\n[7]Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL , 2016.\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850 , 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing , pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention?", "start_char_idx": 29143, "end_char_idx": 32490, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0296c0e0-2a66-4aed-986c-a3fa62c6a5fd": {"__data__": {"id_": "0296c0e0-2a66-4aed-986c-a3fa62c6a5fd", "embedding": null, "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "attention_is_all_you_need", "node_type": "4", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b772de61c571b5c2b1faf24bb4153e24ceecb37d9c22944aee6be95e4b266c23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6fe8244d-d01a-4367-bef8-6bf6b29496ee", "node_type": "1", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "2bff995149fdd1f0594964f25cc71dfa14fa9ac0c6b993e755abd456a24b0a8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "437d6809-94e8-4638-82ec-dadd0ac2e706", "node_type": "1", "metadata": {}, "hash": "677a4f6c0e6866e9330d606c1d264571c382270fab8532f25bfb49c9cfce99e3", "class_name": "RelatedNodeInfo"}}, "text": "[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing , pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS) , 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR) , 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nInInternational Conference on Learning Representations , 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722 , 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130 , 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\n11\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\npages 152\u2013159. ACL, June 2006.\n[27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing , 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433\u2013440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859 , 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.", "start_char_idx": 31906, "end_char_idx": 35081, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "437d6809-94e8-4638-82ec-dadd0ac2e706": {"__data__": {"id_": "437d6809-94e8-4638-82ec-dadd0ac2e706", "embedding": null, "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "attention_is_all_you_need", "node_type": "4", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b772de61c571b5c2b1faf24bb4153e24ceecb37d9c22944aee6be95e4b266c23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0296c0e0-2a66-4aed-986c-a3fa62c6a5fd", "node_type": "1", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "77527360d848306d181c6d45667d8d666e73f49ac5bb376cd3757524706caca2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "010b42cb-6cde-4e2e-b2c1-87a26bfd755d", "node_type": "1", "metadata": {}, "hash": "42f8309878cba57127a5de270a23844161e066463b59b27f989ec321e428a8a2", "class_name": "RelatedNodeInfo"}}, "text": "A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433\u2013440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859 , 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research , 15(1):1929\u20131958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28 , pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\u20133112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems , 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144 , 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers) , pages 434\u2013443. ACL, August 2013.\n12\nAttention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.", "start_char_idx": 34430, "end_char_idx": 37430, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "010b42cb-6cde-4e2e-b2c1-87a26bfd755d": {"__data__": {"id_": "010b42cb-6cde-4e2e-b2c1-87a26bfd755d", "embedding": null, "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "attention_is_all_you_need", "node_type": "4", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b772de61c571b5c2b1faf24bb4153e24ceecb37d9c22944aee6be95e4b266c23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "437d6809-94e8-4638-82ec-dadd0ac2e706", "node_type": "1", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "996d68c0cd6e7bfc99fdc4875800c93cd352c25affd6b8372239cabb3b2c2052", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ca433ae-99b9-4a01-9613-f422ed83191c", "node_type": "1", "metadata": {}, "hash": "fdd5fc59707cbb7527004d07b4b43dfd60df003dce66e16529491c5075e0c7e2", "class_name": "RelatedNodeInfo"}}, "text": "Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers) , pages 434\u2013443. ACL, August 2013.\n12\nAttention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\n13\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15", "start_char_idx": 36874, "end_char_idx": 39486, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ca433ae-99b9-4a01-9613-f422ed83191c": {"__data__": {"id_": "5ca433ae-99b9-4a01-9613-f422ed83191c", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "010b42cb-6cde-4e2e-b2c1-87a26bfd755d", "node_type": "1", "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "f2a4276202495a83d892f88ab10a8ac0966005449e5217eceb1c46f961e34916", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45092251-c78b-4dfb-9f4a-4aea28b74238", "node_type": "1", "metadata": {}, "hash": "f63252dd1826c1da2469cfad96ab24cfdb498c40814119269995a332528ba50a", "class_name": "RelatedNodeInfo"}}, "text": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\nPatrick Esser*Sumith Kulal Andreas Blattmann Rahim Entezari Jonas M \u00a8uller Harry Saini Yam Levi\nDominik Lorenz Axel Sauer Frederic Boesel Dustin Podell Tim Dockhorn Zion English\nKyle Lacey Alex Goodwin Yannik Marek Robin Rombach*\nStability AI\nFigure 1. High-resolution samples from our 8B rectified flow model, showcasing its capabilities in typography, precise prompt following\nand spatial reasoning, attention to fine details, and high image quality across a wide variety of styles.\nAbstract\nDiffusion models create data from noise by invert-\ning the forward paths of data towards noise and\nhave emerged as a powerful generative modeling\ntechnique for high-dimensional, perceptual data\nsuch as images and videos. Rectified flow is a re-\ncent generative model formulation that connects\ndata and noise in a straight line. Despite its better\ntheoretical properties and conceptual simplicity, it\nis not yet decisively established as standard prac-\ntice. In this work, we improve existing noise sam-\npling techniques for training rectified flow mod-\nels by biasing them towards perceptually relevant\nscales. Through a large-scale study, we demon-\n*Equal contribution . <first.last >@stability.ai.strate the superior performance of this approach\ncompared to established diffusion formulations\nfor high-resolution text-to-image synthesis. Ad-\nditionally, we present a novel transformer-based\narchitecture for text-to-image generation that uses\nseparate weights for the two modalities and en-\nables a bidirectional flow of information between\nimage and text tokens, improving text comprehen-\nsion, typography, and human preference ratings.\nWe demonstrate that this architecture follows pre-\ndictable scaling trends and correlates lower vali-\ndation loss to improved text-to-image synthesis as\nmeasured by various metrics and human evalua-\ntions. Our largest models outperform state-of-the-\nart models, and we will make our experimental\ndata, code, and model weights publicly available.\n1arXiv:2403.03206v1  [cs.CV]  5 Mar 2024\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\n1. Introduction\nDiffusion models create data from noise (Song et al., 2020).\nThey are trained to invert forward paths of data towards\nrandom noise and, thus, in conjunction with approximation\nand generalization properties of neural networks, can be\nused to generate new data points that are not present in\nthe training data but follow the distribution of the training\ndata (Sohl-Dickstein et al., 2015; Song & Ermon, 2020).\nThis generative modeling technique has proven to be very\neffective for modeling high-dimensional, perceptual data\nsuch as images (Ho et al., 2020). In recent years, diffusion\nmodels have become the de-facto approach for generating\nhigh-resolution images and videos from natural language\ninputs with impressive generalization capabilities (Saharia\net al., 2022b; Ramesh et al., 2022; Rombach et al., 2022;\nPodell et al., 2023; Dai et al., 2023; Esser et al., 2023;\nBlattmann et al., 2023b; Betker et al., 2023; Blattmann et al.,\n2023a; Singer et al., 2022). Due to their iterative nature\nand the associated computational costs, as well as the long\nsampling times during inference, research on formulations\nfor more efficient training and/or faster sampling of these\nmodels has increased (Karras et al., 2023; Liu et al., 2022).\nWhile specifying a forward path from data to noise leads to\nefficient training, it also raises the question of which path\nto choose. This choice can have important implications\nfor sampling. For example, a forward process that fails to\nremove all noise from the data can lead to a discrepancy\nin training and test distribution and result in artifacts such\nas gray image samples (Lin et al., 2024). Importantly, the\nchoice of the forward process also influences the learned\nbackward process and, thus, the sampling efficiency. While\ncurved paths require many integration steps to simulate the\nprocess, a straight path could be simulated with a single\nstep and is less prone to error accumulation. Since each step\ncorresponds to an evaluation of the neural network, this has\na direct impact on the sampling speed.", "start_char_idx": 0, "end_char_idx": 4227, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45092251-c78b-4dfb-9f4a-4aea28b74238": {"__data__": {"id_": "45092251-c78b-4dfb-9f4a-4aea28b74238", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ca433ae-99b9-4a01-9613-f422ed83191c", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "01131f84fa2c6b97b9ec3e27ee036f47c4b5f70b3076c9535a9c0a3de79e5373", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abfc959e-5184-4b04-acdb-d1672f33d50a", "node_type": "1", "metadata": {}, "hash": "4cffed50186bfd38ea2d7486521fc383208627b558925f94dca96fa350ecfa1c", "class_name": "RelatedNodeInfo"}}, "text": "While specifying a forward path from data to noise leads to\nefficient training, it also raises the question of which path\nto choose. This choice can have important implications\nfor sampling. For example, a forward process that fails to\nremove all noise from the data can lead to a discrepancy\nin training and test distribution and result in artifacts such\nas gray image samples (Lin et al., 2024). Importantly, the\nchoice of the forward process also influences the learned\nbackward process and, thus, the sampling efficiency. While\ncurved paths require many integration steps to simulate the\nprocess, a straight path could be simulated with a single\nstep and is less prone to error accumulation. Since each step\ncorresponds to an evaluation of the neural network, this has\na direct impact on the sampling speed.\nA particular choice for the forward path is a so-called Rec-\ntified Flow (Liu et al., 2022; Albergo & Vanden-Eijnden,\n2022; Lipman et al., 2023), which connects data and noise\non a straight line. Although this model class has better\ntheoretical properties, it has not yet become decisively es-\ntablished in practice. So far, some advantages have been\nempirically demonstrated in small and medium-sized ex-\nperiments (Ma et al., 2024), but these are mostly limited to\nclass-conditional models. In this work, we change this by in-\ntroducing a re-weighting of the noise scales in rectified flow\nmodels, similar to noise-predictive diffusion models (Ho\net al., 2020). Through a large-scale study, we compare\nour new formulation to existing diffusion formulations and\ndemonstrate its benefits.\nWe show that the widely used approach for text-to-image\nsynthesis, where a fixed text representation is fed directlyinto the model (e.g., via cross-attention (Vaswani et al.,\n2017; Rombach et al., 2022)), is not ideal, and present\na new architecture that incorporates learnable streams for\nboth image and text tokens, which enables a two-way flow\nof information between them. We combine this with our\nimproved rectified flow formulation and investigate its scala-\nbility. We demonstrate a predictable scaling trend in the val-\nidation loss and show that a lower validation loss correlates\nstrongly with improved automatic and human evaluations.\nOur largest models outperform state-of-the art open models\nsuch as SDXL (Podell et al., 2023), SDXL-Turbo (Sauer\net al., 2023), Pixart- \u03b1(Chen et al., 2023), and closed-source\nmodels such as DALL-E 3 (Betker et al., 2023) both in\nquantitative evaluation (Ghosh et al., 2023) of prompt un-\nderstanding and human preference ratings.\nThe core contributions of our work are: (i) We conduct a\nlarge-scale, systematic study on different diffusion model\nand rectified flow formulations to identify the best setting.\nFor this purpose, we introduce new noise samplers for recti-\nfied flow models that improve performance over previously\nknown samplers. (ii) We devise a novel, scalable architec-\nture for text-to-image synthesis that allows bi-directional\nmixing between text and image token streams within the\nnetwork. We show its benefits compared to established back-\nbones such as UViT (Hoogeboom et al., 2023) and DiT (Pee-\nbles & Xie, 2023). Finally, we (iii) perform a scaling study\nof our model and demonstrate that it follows predictable\nscaling trends. We show that a lower validation loss cor-\nrelates strongly with improved text-to-image performance\nassessed via metrics such as T2I-CompBench (Huang et al.,\n2023), GenEval (Ghosh et al., 2023) and human ratings. We\nmake results, code, and model weights publicly available.\n2. Simulation-Free Training of Flows\nWe consider generative models that define a mapping be-\ntween samples x1from a noise distribution p1to samples\nx0from a data distribution p0in terms of an ordinary differ-\nential equation (ODE),\ndyt=v\u0398(yt, t)dt , (1)\nwhere the velocity vis parameterized by the weights \u0398of\na neural network. Prior work by Chen et al. (2018) sug-\ngested to directly solve Equation (1) via differentiable ODE\nsolvers.", "start_char_idx": 3416, "end_char_idx": 7424, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abfc959e-5184-4b04-acdb-d1672f33d50a": {"__data__": {"id_": "abfc959e-5184-4b04-acdb-d1672f33d50a", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45092251-c78b-4dfb-9f4a-4aea28b74238", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "24c65485a3001c67cda002588026e62ce874a0a976af234515c888feb54d194d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07734fec-4445-4ee1-a66a-55c466de346a", "node_type": "1", "metadata": {}, "hash": "c631b4dc77b239e3bf35cd544dfebe40d5aa72e691a885b057bdcd7c4ca068bf", "class_name": "RelatedNodeInfo"}}, "text": "We show that a lower validation loss cor-\nrelates strongly with improved text-to-image performance\nassessed via metrics such as T2I-CompBench (Huang et al.,\n2023), GenEval (Ghosh et al., 2023) and human ratings. We\nmake results, code, and model weights publicly available.\n2. Simulation-Free Training of Flows\nWe consider generative models that define a mapping be-\ntween samples x1from a noise distribution p1to samples\nx0from a data distribution p0in terms of an ordinary differ-\nential equation (ODE),\ndyt=v\u0398(yt, t)dt , (1)\nwhere the velocity vis parameterized by the weights \u0398of\na neural network. Prior work by Chen et al. (2018) sug-\ngested to directly solve Equation (1) via differentiable ODE\nsolvers. However, this process is computationally expensive,\nespecially for large network architectures that parameterize\nv\u0398(yt, t). A more efficient alternative is to directly regress\na vector field utthat generates a probability path between\np0andp1. To construct such a ut, we define a forward\nprocess, corresponding to a probability path ptbetween p0\nandp1=N(0,1), as\nzt=atx0+bt\u03f5where \u03f5\u223c N(0, I). (2)\n2\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nFora0= 1, b0= 0, a1= 0andb1= 1, the marginals,\npt(zt) =E\u03f5\u223cN(0,I)pt(zt|\u03f5), (3)\nare consistent with the data and noise distribution.\nTo express the relationship between zt, x0and\u03f5, we intro-\nduce\u03c8tandutas\n\u03c8t(\u00b7|\u03f5) :x07\u2192atx0+bt\u03f5 (4)\nut(z|\u03f5):=\u03c8\u2032\nt(\u03c8\u22121\nt(z|\u03f5)|\u03f5) (5)\nSince ztcan be written as solution to the ODE z\u2032\nt=ut(zt|\u03f5),\nwith initial value z0=x0,ut(\u00b7|\u03f5)generates pt(\u00b7|\u03f5). Re-\nmarkably, one can construct a marginal vector field utwhich\ngenerates the marginal probability paths pt(Lipman et al.,\n2023) (see B.1), using the conditional vector fields ut(\u00b7|\u03f5):\nut(z) =E\u03f5\u223cN(0,I)ut(z|\u03f5)pt(z|\u03f5)\npt(z)(6)\nWhile regressing utwith the Flow Matching objective\nLFM=Et,pt(z)||v\u0398(z, t)\u2212ut(z)||2\n2. (7)\ndirectly is intractable due to the marginalization in Equa-\ntion 6, Conditional Flow Matching (see B.1),\nLCFM =Et,pt(z|\u03f5),p(\u03f5)||v\u0398(z, t)\u2212ut(z|\u03f5)||2\n2,(8)\nwith the conditional vector fields ut(z|\u03f5)provides an equiv-\nalent yet tractable objective.\nTo convert the loss into an explicit form we insert\n\u03c8\u2032\nt(x0|\u03f5) =a\u2032\ntx0+b\u2032\nt\u03f5and\u03c8\u22121\nt(z|\u03f5) =z\u2212bt\u03f5\natinto (5)\nz\u2032\nt=ut(zt|\u03f5) =a\u2032\nt\natzt\u2212\u03f5bt(a\u2032\nt\nat\u2212b\u2032\nt\nbt).", "start_char_idx": 6716, "end_char_idx": 8984, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07734fec-4445-4ee1-a66a-55c466de346a": {"__data__": {"id_": "07734fec-4445-4ee1-a66a-55c466de346a", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abfc959e-5184-4b04-acdb-d1672f33d50a", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "249e92cd649ae6fc64897b4321eb6327cb58442de9b70a0377bbb664feb25b48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0111e1a-f9e8-4076-8946-f00e4ccf83ab", "node_type": "1", "metadata": {}, "hash": "64b294f490375e97447b52abd9fe602648e8119f9d0e9789041dbd0cb12814d6", "class_name": "RelatedNodeInfo"}}, "text": "(7)\ndirectly is intractable due to the marginalization in Equa-\ntion 6, Conditional Flow Matching (see B.1),\nLCFM =Et,pt(z|\u03f5),p(\u03f5)||v\u0398(z, t)\u2212ut(z|\u03f5)||2\n2,(8)\nwith the conditional vector fields ut(z|\u03f5)provides an equiv-\nalent yet tractable objective.\nTo convert the loss into an explicit form we insert\n\u03c8\u2032\nt(x0|\u03f5) =a\u2032\ntx0+b\u2032\nt\u03f5and\u03c8\u22121\nt(z|\u03f5) =z\u2212bt\u03f5\natinto (5)\nz\u2032\nt=ut(zt|\u03f5) =a\u2032\nt\natzt\u2212\u03f5bt(a\u2032\nt\nat\u2212b\u2032\nt\nbt). (9)\nNow, consider the signal-to-noise ratio \u03bbt:= loga2\nt\nb2\nt. With\n\u03bb\u2032\nt= 2(a\u2032\nt\nat\u2212b\u2032\nt\nbt), we can rewrite Equation (9) as\nut(zt|\u03f5) =a\u2032\nt\natzt\u2212bt\n2\u03bb\u2032\nt\u03f5 (10)\nNext, we use Equation (10) to reparameterize Equation (8)\nas a noise-prediction objective:\nLCFM =Et,pt(z|\u03f5),p(\u03f5)||v\u0398(z, t)\u2212a\u2032\nt\natz+bt\n2\u03bb\u2032\nt\u03f5||2\n2(11)\n=Et,pt(z|\u03f5),p(\u03f5)\u0012\n\u2212bt\n2\u03bb\u2032\nt\u00132\n||\u03f5\u0398(z, t)\u2212\u03f5||2\n2(12)\nwhere we defined \u03f5\u0398:=\u22122\n\u03bb\u2032\ntbt(v\u0398\u2212a\u2032\nt\natz).\nNote that the optimum of the above objective does not\nchange when introducing a time-dependent weighting. Thus,one can derive various weighted loss functions that provide\na signal towards the desired solution but might affect the\noptimization trajectory. For a unified analysis of different\napproaches, including classic diffusion formulations, we\ncan write the objective in the following form (following\nKingma & Gao (2023)):\nLw(x0) =\u22121\n2Et\u223cU(t),\u03f5\u223cN(0,I)\u0002\nwt\u03bb\u2032\nt\u2225\u03f5\u0398(zt, t)\u2212\u03f5\u22252\u0003\n,\nwhere wt=\u22121\n2\u03bb\u2032\ntb2\ntcorresponds to LCFM .\n3. Flow Trajectories\nIn this work, we consider different variants of the above\nformalism that we briefly describe in the following.\nRectified Flow Rectified Flows (RFs) (Liu et al., 2022;\nAlbergo & Vanden-Eijnden, 2022; Lipman et al., 2023)\ndefine the forward process as straight paths between the\ndata distribution and a standard normal distribution, i.e.\nzt= (1\u2212t)x0+t\u03f5 , (13)\nand uses LCFM which then corresponds to wRF\nt=t\n1\u2212t.\nThe network output directly parameterizes the velocity v\u0398.\nEDM EDM (Karras et al., 2022) uses a forward process\nof the form\nzt=x0+bt\u03f5 (14)\nwhere (Kingma & Gao, 2023) bt= exp F\u22121\nN(t|Pm, P2\ns)\nwithF\u22121\nNbeing the quantile function of the normal distribu-\ntion with mean Pmand variance P2\ns. Note that this choice\nresults in\n\u03bbt\u223c N(\u22122Pm,(2Ps)2)fort\u223c U(0,1) (15)\nThe network is parameterized through an F-prediction\n(Kingma & Gao, 2023; Karras et al., 2022) and the loss\ncan be written as LwEDM\ntwith\nwEDM\nt=N(\u03bbt| \u22122Pm,(2Ps)2)(e\u2212\u03bbt+ 0.52) (16)\nCosine (Nichol & Dhariwal, 2021) proposed a forward\nprocess of the form\nzt= cos\u0000\u03c0\n2t\u0001\nx0+ sin\u0000\u03c0\n2t\u0001\n\u03f5 .", "start_char_idx": 8580, "end_char_idx": 10991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0111e1a-f9e8-4076-8946-f00e4ccf83ab": {"__data__": {"id_": "d0111e1a-f9e8-4076-8946-f00e4ccf83ab", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07734fec-4445-4ee1-a66a-55c466de346a", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b9a52080f3f3901b557cc553d44456c7fe5e7cc9bd4c7f9fd97f2058af3b72df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09a30549-613c-4f86-96a2-0e62e5467107", "node_type": "1", "metadata": {}, "hash": "a77d1ef66fd9d603a77a5ac7d4e35da6f17cb7ab2a44a01199612385600787f6", "class_name": "RelatedNodeInfo"}}, "text": "(17)\nIn combination with an \u03f5-parameterization and loss, this\ncorresponds to a weighting wt= sech( \u03bbt/2). When com-\nbined with a v-prediction loss (Kingma & Gao, 2023), the\nweighting is given by wt=e\u2212\u03bbt/2.\n3\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\n(LDM-)Linear LDM (Rombach et al., 2022) uses a mod-\nification of the DDPM schedule (Ho et al., 2020). Both are\nvariance preserving schedules, i.e. bt=p\n1\u2212a2\nt, and de-\nfineatfor discrete timesteps t= 0, . . . , T \u22121in terms\nof diffusion coefficients \u03b2tasat= (Qt\ns=0(1\u2212\u03b2s))1\n2.\nFor given boundary values \u03b20and\u03b2T\u22121, DDPM uses\n\u03b2t=\u03b20+t\nT\u22121(\u03b2T\u22121\u2212\u03b20)and LDM uses \u03b2t=\u0010p\n\u03b20+t\nT\u22121(p\n\u03b2T\u22121\u2212p\n\u03b20)\u00112\n.\n3.1. Tailored SNR Samplers for RF models\nThe RF loss trains the velocity v\u0398uniformly on all timesteps\nin[0,1]. Intuitively, however, the resulting velocity predic-\ntion target \u03f5\u2212x0is more difficult for tin the middle of\n[0,1], since for t= 0, the optimal prediction is the mean\nofp1, and for t= 1the optimal prediction is the mean of\np0. In general, changing the distribution over tfrom the\ncommonly used uniform distribution U(t)to a distribution\nwith density \u03c0(t)is equivalent to a weighted loss Lw\u03c0\ntwith\nw\u03c0\nt=t\n1\u2212t\u03c0(t) (18)\nThus, we aim to give more weight to intermediate timesteps\nby sampling them more frequently. Next, we describe the\ntimestep densities \u03c0(t)that we use to train our models.\nLogit-Normal Sampling One option for a distribution\nthat puts more weight on intermediate steps is the logit-\nnormal distribution (Atchison & Shen, 1980). Its density,\n\u03c0ln(t;m, s) =1\ns\u221a\n2\u03c01\nt(1\u2212t)exp\u0010\n\u2212(logit(t)\u2212m)2\n2s2\u0011\n,\n(19)\nwhere logit(t) = logt\n1\u2212t, has a location parameter, m, and\na scale parameter, s. The location parameter enables us to\nbias the training timesteps towards either data p0(negative\nm) or noise p1(positive m). As shown in Figure 11, the\nscale parameters controls how wide the distribution is.\nIn practice, we sample the random variable ufrom a nor-\nmal distribution u\u223c N (u;m, s)and map it through the\nstandard logistic function.\nMode Sampling with Heavy Tails The logit-normal den-\nsity always vanishes at the endpoints 0and1. To study\nwhether this has adverse effects on the performance, we\nalso use a timestep sampling distribution with strictly posi-\ntive density on [0,1]. For a scale parameter s, we define\nfmode(u;s) = 1\u2212u\u2212s\u00b7\u0010\ncos2\u0000\u03c0\n2u\u0001\n\u22121 +u\u0011\n.(20)\nFor\u22121\u2264s\u22642\n\u03c0\u22122, this function is monotonic, and we\ncan use it to sample from the implied density \u03c0mode(t;s) =\f\fd\ndtf\u22121\nmode(t)\f\f. As seen in Figure 11, the scale parametercontrols the degree to which either the midpoint (positive\ns) or the endpoints (negative s) are favored during sam-\npling. This formulation also includes a uniform weighting\n\u03c0mode(t;s= 0) = U(t)fors= 0, which has been used\nwidely in previous works on Rectified Flows (Liu et al.,\n2022; Ma et al., 2024).\nCosMap Finally, we also consider the cosine schedule\n(Nichol & Dhariwal, 2021) from Section 3 in the RF setting.", "start_char_idx": 10992, "end_char_idx": 13929, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09a30549-613c-4f86-96a2-0e62e5467107": {"__data__": {"id_": "09a30549-613c-4f86-96a2-0e62e5467107", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0111e1a-f9e8-4076-8946-f00e4ccf83ab", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "c2687d907680e761b36accb8f77f4ea9ed6f0afc569d05308760dfd7abe0030d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e994813-dccd-4900-a4a1-62866edcb020", "node_type": "1", "metadata": {}, "hash": "99769d45b88417d5f192a13338cdaeb5827ac515e919bbc02358044bc696acae", "class_name": "RelatedNodeInfo"}}, "text": "(20)\nFor\u22121\u2264s\u22642\n\u03c0\u22122, this function is monotonic, and we\ncan use it to sample from the implied density \u03c0mode(t;s) =\f\fd\ndtf\u22121\nmode(t)\f\f. As seen in Figure 11, the scale parametercontrols the degree to which either the midpoint (positive\ns) or the endpoints (negative s) are favored during sam-\npling. This formulation also includes a uniform weighting\n\u03c0mode(t;s= 0) = U(t)fors= 0, which has been used\nwidely in previous works on Rectified Flows (Liu et al.,\n2022; Ma et al., 2024).\nCosMap Finally, we also consider the cosine schedule\n(Nichol & Dhariwal, 2021) from Section 3 in the RF setting.\nIn particular, we are looking for a mapping f:u7\u2192f(u) =\nt, u\u2208[0,1], such that the log-snr matches that of the cosine\nschedule: 2 logcos(\u03c0\n2u)\nsin(\u03c0\n2u)= 2 log1\u2212f(u)\nf(u). Solving for f, we\nobtain for u\u223c U(u)\nt=f(u) = 1\u22121\ntan(\u03c0\n2u) + 1, (21)\nfrom which we obtain the density\n\u03c0CosMap (t) =\f\f\f\fd\ndtf\u22121(t)\f\f\f\f=2\n\u03c0\u22122\u03c0t+ 2\u03c0t2. (22)\n4. Text-to-Image Architecture\nFor text-conditional sampling of images, our model has to\ntake both modalities, text and images, into account. We\nuse pretrained models to derive suitable representations and\nthen describe the architecture of our diffusion backbone. An\noverview of this is presented in Figure 2.\nOur general setup follows LDM (Rombach et al., 2022)\nfor training text-to-image models in the latent space of a\npretrained autoencoder. Similar to the encoding of images to\nlatent representations, we also follow previous approaches\n(Saharia et al., 2022b; Balaji et al., 2022) and encode the text\nconditioning cusing pretrained, frozen text models. Details\ncan be found in Appendix B.2.\nMultimodal Diffusion Backbone Our architecture builds\nupon the DiT (Peebles & Xie, 2023) architecture. DiT only\nconsiders class conditional image generation and uses a\nmodulation mechanism to condition the network on both\nthe timestep of the diffusion process and the class label.\nSimilarly, we use embeddings of the timestep tandcvec\nas inputs to the modulation mechanism. However, as the\npooled text representation retains only coarse-grained infor-\nmation about the text input (Podell et al., 2023), the network\nalso requires information from the sequence representation\ncctxt.\nWe construct a sequence consisting of embeddings of the\ntext and image inputs. Specifically, we add positional en-\ncodings and flatten 2\u00d72patches of the latent pixel rep-\nresentation x\u2208Rh\u00d7w\u00d7cto a patch encoding sequence of\nlength1\n2\u00b7h\u00b71\n2\u00b7w. After embedding this patch encoding\nand the text encoding cctxtto a common dimensionality, we\n4\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nCaption\nCLIP-L/14 CLIP-G/14 T5 XXLPooled\nLinear\ncMLP\nMLP\nSinusoidal Encoding\nTimestep+ yNoised Latent\nPatching\nLinear\n+Positional\nEmbedding\nx\nMM-DiT -Block 1\nMM-DiT -Block 2\n. . .", "start_char_idx": 13338, "end_char_idx": 16122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e994813-dccd-4900-a4a1-62866edcb020": {"__data__": {"id_": "9e994813-dccd-4900-a4a1-62866edcb020", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09a30549-613c-4f86-96a2-0e62e5467107", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "603ac9fd30bb224d726042afc0262d807bea02aa4e1b89e5586f61b07558551b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c8b2255-e058-4df9-92e0-dad2ddea4bb9", "node_type": "1", "metadata": {}, "hash": "f3e37b5f188f2b026c545276af965221cb2bb06481fc3cca48b27b4a7c168397", "class_name": "RelatedNodeInfo"}}, "text": "We construct a sequence consisting of embeddings of the\ntext and image inputs. Specifically, we add positional en-\ncodings and flatten 2\u00d72patches of the latent pixel rep-\nresentation x\u2208Rh\u00d7w\u00d7cto a patch encoding sequence of\nlength1\n2\u00b7h\u00b71\n2\u00b7w. After embedding this patch encoding\nand the text encoding cctxtto a common dimensionality, we\n4\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nCaption\nCLIP-L/14 CLIP-G/14 T5 XXLPooled\nLinear\ncMLP\nMLP\nSinusoidal Encoding\nTimestep+ yNoised Latent\nPatching\nLinear\n+Positional\nEmbedding\nx\nMM-DiT -Block 1\nMM-DiT -Block 2\n. . .\nMM-DiT -Block d\nModulation\nLinear\nUnpatching\nOutput77 + 77 tokens\n4096\nchannel\n(a) Overview of all components.c x\nLayernorm\nMod: \u03b1c\u00b7 \u2022+\u03b2c\nLinear\nAttention\nLinear\n\u2217\n+\nLayernorm\nMod: \u03b4c\u00b7 \u2022+\u03f5c\nMLP\n\u2217\n+\u03b1c\n\u03b2c\n\u03b3c\n\u03b4c\n\u03f5c\n\u03b6cy\nSiLU\nLinear\nLayernorm\nMod: \u03b1x\u00b7 \u2022+\u03b2x\nLinear\nLinear\n\u2217\n+\nLayernorm\nMod: \u03b4x\u00b7 \u2022+\u03f5x\nMLP\n\u2217\n+\u03b1x\n\u03b2x\n\u03b3x\n\u03b4x\n\u03f5x\n\u03b6xSiLU\nLinear\nK Q V\u2299\u2299\u2299Opt.\nRMS-\nNormOpt.\nRMS-\nNormOpt.\nRMS-\nNormOpt.\nRMS-\nNorm\n(b) One MM-DiT block\nFigure 2. Our model architecture. Concatenation is indicated by \u2299and element-wise multiplication by \u2217. The RMS-Norm for QandK\ncan be added to stabilize training runs. Best viewed zoomed in.\nconcatenate the two sequences. We then follow DiT and\napply a sequence of modulated attention and MLPs.\nSince text and image embeddings are conceptually quite\ndifferent, we use two separate sets of weights for the two\nmodalities. As shown in Figure 2b, this is equivalent to\nhaving two independent transformers for each modality, but\njoining the sequences of the two modalities for the attention\noperation, such that both representations can work in their\nown space yet take the other one into account.\nFor our scaling experiments, we parameterize the size of\nthe model in terms of the model\u2019s depth d,i.e. the number\nof attention blocks, by setting the hidden size to 64\u00b7d\n(expanded to 4\u00b764\u00b7dchannels in the MLP blocks), and the\nnumber of attention heads equal to d.\n5. Experiments\n5.1. Improving Rectified Flows\nWe aim to understand which of the approaches for\nsimulation-free training of normalizing flows as in Equa-\ntion 1 is the most efficient. To enable comparisons across\ndifferent approaches, we control for the optimization algo-\nrithm, the model architecture, the dataset and samplers. Inaddition, the losses of different approaches are incomparable\nand also do not necessarily correlate with the quality of out-\nput samples; hence we need evaluation metrics that allow for\na comparison between approaches. We train models on Ima-\ngeNet (Russakovsky et al., 2014) and CC12M (Changpinyo\net al., 2021), and evaluate both the training and the EMA\nweights of the models during training using validation losses,\nCLIP scores (Radford et al., 2021; Hessel et al., 2021), and\nFID (Heusel et al., 2017) under different sampler settings\n(different guidance scales and sampling steps). We calcu-\nlate the FID on CLIP features as proposed by (Sauer et al.,\n2021). All metrics are evaluated on the COCO-2014 valida-\ntion split (Lin et al., 2014). Full details on the training and\nsampling hyperparameters are provided in Appendix B.3.\n5.1.1. R ESULTS\nWe train each of 61 different formulations on the two\ndatasets.", "start_char_idx": 15533, "end_char_idx": 18741, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c8b2255-e058-4df9-92e0-dad2ddea4bb9": {"__data__": {"id_": "7c8b2255-e058-4df9-92e0-dad2ddea4bb9", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e994813-dccd-4900-a4a1-62866edcb020", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "487faef632419ae531efefbb94273d4a394c09a8ce2bda314ac14afd9bcf6d7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc800e59-e0fb-4a83-a51c-395bc166a0e1", "node_type": "1", "metadata": {}, "hash": "08099fa8e18599979f465db8e7556d8b55248a24dcc0e1efe3b3b9f282d42a98", "class_name": "RelatedNodeInfo"}}, "text": "We calcu-\nlate the FID on CLIP features as proposed by (Sauer et al.,\n2021). All metrics are evaluated on the COCO-2014 valida-\ntion split (Lin et al., 2014). Full details on the training and\nsampling hyperparameters are provided in Appendix B.3.\n5.1.1. R ESULTS\nWe train each of 61 different formulations on the two\ndatasets. We include the following variants from Section 3:\n\u2022Both \u03f5- and v-prediction loss with linear\n(eps/linear ,v/linear ) and cosine ( eps/cos ,\nv/cos ) schedule.\n\u2022RF loss with \u03c0mode(t;s)(rf/mode(s) ) with 7 val-\nues for schosen uniformly between \u22121and1.75, and\n5\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nrank averaged over\nvariant all 5 steps 50 steps\nrf/lognorm(0.00, 1.00) 1.54 1.25 1.50\nrf/lognorm(1.00, 0.60) 2.08 3.50 2.00\nrf/lognorm(0.50, 0.60) 2.71 8.50 1.00\nrf/mode(1.29) 2.75 3.25 3.00\nrf/lognorm(0.50, 1.00) 2.83 1.50 2.50\neps/linear 2.88 4.25 2.75\nrf/mode(1.75) 3.33 2.75 2.75\nrf/cosmap 4.13 3.75 4.00\nedm(0.00, 0.60) 5.63 13.25 3.25\nrf 5.67 6.50 5.75\nv/linear 6.83 5.75 7.75\nedm(0.60, 1.20) 9.00 13.00 9.00\nv/cos 9.17 12.25 8.75\nedm/cos 11.04 14.25 11.25\nedm/rf 13.04 15.25 13.25\nedm(-1.20, 1.20) 15.58 20.25 15.00\nTable 1. Global ranking of variants. For this ranking, we apply\nnon-dominated sorting averaged over EMA and non-EMA weights,\ntwo datasets and different sampling settings.\nImageNet CC12M\nvariant CLIP FID CLIP FID\nrf 0.247 49.70 0.217 94.90\nedm(-1.20, 1.20) 0.236 63.12 0.200 116.60\neps/linear 0.245 48.42 0.222 90.34\nv/cos 0.244 50.74 0.209 97.87\nv/linear 0.246 51.68 0.217 100.76\nrf/lognorm(0.50, 0.60) 0.256 80.41 0.233 120.84\nrf/mode(1.75) 0.253 44.39 0.218 94.06\nrf/lognorm(1.00, 0.60) 0.254 114.26 0.234 147.69\nrf/lognorm(-0.50, 1.00) 0.248 45.64 0.219 89.70\nrf/lognorm(0.00, 1.00) 0.250 45.78 0.224 89.91\nTable 2. Metrics for different variants. FID and CLIP scores of\ndifferent variants with 25 sampling steps. We highlight the best,\nsecond best , and third best entries.\nadditionally for s= 1.0ands= 0which corresponds\nto uniform timestep sampling ( rf/mode ).\n\u2022RF loss with \u03c0ln(t;m, s)(rf/lognorm(m, s) )\nwith 30 values for (m, s)in the grid with muniform\nbetween \u22121and1, andsuniform between 0.2and2.2.\n\u2022 RF loss with \u03c0CosMap (t)(rf/cosmap ).", "start_char_idx": 18415, "end_char_idx": 20646, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc800e59-e0fb-4a83-a51c-395bc166a0e1": {"__data__": {"id_": "fc800e59-e0fb-4a83-a51c-395bc166a0e1", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c8b2255-e058-4df9-92e0-dad2ddea4bb9", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "f7168d67e205fe4385f883fe0f4e35bef0687aa7b168ce3cd17ec99a5d339cea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fe02bce-2d9b-4b42-9962-5b96a155f989", "node_type": "1", "metadata": {}, "hash": "3cd4b97b783899906e0e6fb3b4c5ade90ff7a7b9f05f5bf359333118f762276b", "class_name": "RelatedNodeInfo"}}, "text": "Metrics for different variants. FID and CLIP scores of\ndifferent variants with 25 sampling steps. We highlight the best,\nsecond best , and third best entries.\nadditionally for s= 1.0ands= 0which corresponds\nto uniform timestep sampling ( rf/mode ).\n\u2022RF loss with \u03c0ln(t;m, s)(rf/lognorm(m, s) )\nwith 30 values for (m, s)in the grid with muniform\nbetween \u22121and1, andsuniform between 0.2and2.2.\n\u2022 RF loss with \u03c0CosMap (t)(rf/cosmap ).\n\u2022EDM ( edm( Pm, Ps)) with 15 values for Pmchosen\nuniformly between \u22121.2and1.2andPsuniform be-\ntween 0.6and1.8. Note that Pm, Ps= (\u22121.2,1.2)\ncorresponds to the parameters in (Karras et al., 2022).\n\u2022EDM with a schedule such that it matches the log-SNR\nweighting of rf(edm/rf ) and one that matches the\nlog-SNR weighting of v/cos (edm/cos ).\nFor each run, we select the step with minimal validation loss\nwhen evaluated with EMA weights and then collect CLIP\nscores and FID obtained with 6 different sampler settingsboth with and without EMA weights.\nFor all 24 combinations of sampler settings, EMA weights,\nand dataset choice, we rank the different formulations using\na non-dominated sorting algorithm. For this, we repeatedly\ncompute the variants that are Pareto optimal according to\nCLIP and FID scores, assign those variants the current iter-\nation index, remove those variants, and continue with the\nremaining ones until all variants get ranked. Finally, we\naverage those ranks over the 24 different control settings.\nWe present the results in Tab. 1, where we only show the\ntwo best-performing variants for those variants that were\nevaluated with different hyperparameters. We also show\nranks where we restrict the averaging over sampler settings\nwith 5 steps and with 50 steps.\nWe observe that rf/lognorm(0.00, 1.00) consis-\ntently achieves a good rank. It outperforms a rectified\nflow formulation with uniform timestep sampling ( rf) and\nthus confirms our hypothesis that intermediate timesteps are\nmore important. Among all the variants, only rectified flow\nformulations with modified timestep sampling perform bet-\nter than the LDM-Linear (Rombach et al., 2022) formulation\n(eps/linear ) used previously.\nWe also observe that some variants perform well in some\nsettings but worse in others, e.g.rf/lognorm(0.50,\n0.60) is the best-performing variant with 50 sampling\nsteps but much worse (average rank 8.5) with 5 sampling\nsteps. We observe a similar behavior with respect to the\ntwo metrics in Tab. 2. The first group shows representa-\ntive variants and their metrics on both datasets with 25\nsampling steps. The next group shows the variants that\nachieve the best CLIP and FID scores. With the exception\nofrf/mode(1.75) , these variants typically perform very\nwell in one metric but relatively badly in the other. In con-\ntrast, we once again observe that rf/lognorm(0.00,\n1.00) achieves good performance across metrics and\ndatasets, where it obtains the third-best scores two out of\nfour times and once the second-best performance.\nFinally, we illustrate the qualitative behavior of different\nformulations in Figure 3, where we use different colors\nfor different groups of formulations ( edm,rf,eps and\nv). Rectified flow formulations generally perform well and,\ncompared to other formulations, their performance degrades\nless when reducing the number of sampling steps.\n5.2. Improving Modality Specific Representations\nHaving found a formulation in the previous section that\nallows rectified flow models to not only compete with estab-\nlished diffusion formulations such as LDM-Linear (Rom-\nbach et al., 2022) or EDM (Karras et al., 2022), but even\noutperforms them, we now turn to the application of our\nformulation to high-resolution text-to-image synthesis.", "start_char_idx": 20215, "end_char_idx": 23917, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1fe02bce-2d9b-4b42-9962-5b96a155f989": {"__data__": {"id_": "1fe02bce-2d9b-4b42-9962-5b96a155f989", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc800e59-e0fb-4a83-a51c-395bc166a0e1", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "ec70e209ded7de388da962482f6564c027f5ca78a4b47e584dca720d774f85b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f277803-acb7-4984-b710-677e4dbbd209", "node_type": "1", "metadata": {}, "hash": "42af553dbfc7cd409030071c6ca618e1807883260272454831f794aea4215750", "class_name": "RelatedNodeInfo"}}, "text": "Finally, we illustrate the qualitative behavior of different\nformulations in Figure 3, where we use different colors\nfor different groups of formulations ( edm,rf,eps and\nv). Rectified flow formulations generally perform well and,\ncompared to other formulations, their performance degrades\nless when reducing the number of sampling steps.\n5.2. Improving Modality Specific Representations\nHaving found a formulation in the previous section that\nallows rectified flow models to not only compete with estab-\nlished diffusion formulations such as LDM-Linear (Rom-\nbach et al., 2022) or EDM (Karras et al., 2022), but even\noutperforms them, we now turn to the application of our\nformulation to high-resolution text-to-image synthesis. Ac-\n6\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\n10 20 30 40 506080100120140\nedm(-1.20, 1.20)\neps/linear\nrf/lognorm(0.00, 1.00)\nrf\nv/cos\nv/linear\nnumber of sampling stepsFID\nFigure 3. Rectified flows are sample efficient. Rectified Flows\nperform better then other formulations when sampling fewer steps.\nFor 25 and more steps, only rf/lognorm(0.00, 1.00) re-\nmains competitive to eps/linear .\nMetric 4 chn 8 chn 16 chn\nFID (\u2193) 2.41 1.56 1.06\nPerceptual Similarity ( \u2193) 0.85 0.68 0.45\nSSIM ( \u2191) 0.75 0.79 0.86\nPSNR ( \u2191) 25.12 26.40 28.62\nTable 3. Improved Autoencoders. Reconstruction performance\nmetrics for different channel configurations. The downsampling\nfactor for all models is f= 8.\ncordingly, the final performance of our algorithm depends\nnot only on the training formulation, but also on the parame-\nterization via a neural network and the quality of the image\nand text representations we use. In the following sections,\nwe describe how we improve all these components before\nscaling our final method in Section 5.3.\n5.2.1. I MPROVED AUTOENCODERS\nLatent diffusion models achieve high efficiency by operating\nin the latent space of a pretrained autoencoder (Rombach\net al., 2022), which maps an input RGB X\u2208RH\u00d7W\u00d73into\na lower-dimensional space x=E(X)\u2208Rh\u00d7w\u00d7d. The\nreconstruction quality of this autoencoder provides an upper\nbound on the achievable image quality after latent diffusion\ntraining. Similar to Dai et al. (2023), we find that increasing\nthe number of latent channels dsignificantly boosts recon-\nstruction performance, see Table 3. Intuitively, predicting\nlatents with higher dis a more difficult task, and thus mod-\nels with increased capacity should be able to perform better\nfor larger d, ultimately achieving higher image quality. We\nconfirm this hypothesis in Figure 10, where we see that the\nd= 16 autoencoder exhibits better scaling performance in\nterms of sample FID. For the remainder of this paper, we\nthus choose d= 16 .\n5.2.2. I MPROVED CAPTIONS\nBetker et al. (2023) demonstrated that synthetically gen-\nerated captions can greatly improve text-to-image models\ntrained at scale. This is due to the oftentimes simplisticOriginal Captions 50/50 Mix\nsuccess rate [%] success rate [%]\nColor Attribution 11.75 24.75\nColors 71.54 68.09\nPosition 6.50 18.00\nCounting 33.44 41.56\nSingle Object 95.00 93.75\nTwo Objects 41.41 52.53\nOverall score 43.27 49.78\nTable 4. Improved Captions . Using a 50/50 mixing ratio of\nsynthetic (via CogVLM (Wang et al., 2023)) and original cap-\ntions improves text-to-image performance. Assessed via the\nGenEval (Ghosh et al., 2023) benchmark.\nnature of the human-generated captions that come with\nlarge-scale image datasets, which overly focus on the image\nsubject and usually omit details describing the background\nor composition of the scene, or, if applicable, displayed\ntext (Betker et al., 2023).", "start_char_idx": 23188, "end_char_idx": 26804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f277803-acb7-4984-b710-677e4dbbd209": {"__data__": {"id_": "9f277803-acb7-4984-b710-677e4dbbd209", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fe02bce-2d9b-4b42-9962-5b96a155f989", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "67172051b31db45c80620bdd3ee3cc0a6ee83ce6925e4d189c2687da1986d6a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06df1f32-e75d-4d2e-b0d2-d9aed2263ec6", "node_type": "1", "metadata": {}, "hash": "0e7920ad63bdecc7b1061e06f4fc547aaf3778e037148a3c60a5e45b47434047", "class_name": "RelatedNodeInfo"}}, "text": "Improved Captions . Using a 50/50 mixing ratio of\nsynthetic (via CogVLM (Wang et al., 2023)) and original cap-\ntions improves text-to-image performance. Assessed via the\nGenEval (Ghosh et al., 2023) benchmark.\nnature of the human-generated captions that come with\nlarge-scale image datasets, which overly focus on the image\nsubject and usually omit details describing the background\nor composition of the scene, or, if applicable, displayed\ntext (Betker et al., 2023). We follow their approach and\nuse an off-the-shelf, state-of-the-art vision-language model,\nCogVLM (Wang et al., 2023), to create synthetic annotations\nfor our large-scale image dataset. As synthetic captions may\ncause a text-to-image model to forget about certain concepts\nnot present in the VLM\u2019s knowledge corpus, we use a ratio\nof 50 % original and 50 % synthetic captions.\nTo assess the effect of training on this caption mix, we train\ntwod= 15 MM-DiT models for 250k steps, one on only\noriginal captions and the other on the 50/50 mix. We evalu-\nate the trained models using the GenEval benchmark (Ghosh\net al., 2023) in Table 4. The results demonstrate that the\nmodel trained with the addition of synthetic captions clearly\noutperforms the model that only utilizes original captions.\nWe thus use the 50/50 synthetic/original caption mix for the\nremainder of this work.\n5.2.3. I MPROVED TEXT-TO-IMAGE BACKBONES\nIn this section, we compare the performance of existing\ntransformer-based diffusion backbones with our novel mul-\ntimodal transformer-based diffusion backbone, MM-DiT , as\nintroduced in Section 4. MM-DiT is specifically designed to\nhandle different domains, here text and image tokens, using\n(two) different sets of trainable model weights. More specif-\nically, we follow the experimental setup from Section 5.1\nand compare text-to-image performance on CC12M of DiT,\nCrossDiT (DiT but with cross-attending to the text tokens\ninstead of sequence-wise concatenation (Chen et al., 2023))\nand our MM-DiT . For MM-DiT , we compare models with\ntwo sets of weights and three sets of weights, where the lat-\nter handles the CLIP (Radford et al., 2021) and T5 (Raffel\net al., 2019) tokens ( c.f. Section 4) separately. Note that DiT\n(w/ concatenation of text and image tokens as in Section 4)\ncan be interpreted as a special case of MM-DiT with one\n7\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\na space elevator,\ncinematic scifi art\nA cheeseburger with juicy\nbeef patties and melted\ncheese sits on top of a toilet\nthat looks like a throne and\nstands in the middle of the\nroyal chamber.\na hole in the floor of my\nbathroom with small\ngremlins living in it\na small office made out of car\nparts\nThis dreamlike digital art\ncaptures a vibrant,\nkaleidoscopic bird in a lush\nrainforest.\nhuman life depicted entirely\nout of fractals\nan origami pig on fire\nin the middle of a\ndark room with a\npentagram on the\nfloor\nan old rusted robot wearing pants and a jacket riding skis in a supermarket.\n smiling cartoon dog sits at a table, coffee mug on hand, as a room goes up in flames. \u201cThis is fine,\u201d the\ndog assures himself.\nA whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus. This imaginative creature features the distinctive, bulky body of a hippo, but with a texture and\nappearance resembling a golden-brown, crispy waffle. The creature might have elements like waffle squares across its skin and a syrup-like sheen. It\u2019s set in a surreal environment that playfully combines a\nnatural water habitat of a hippo with elements of a breakfast table setting, possibly including oversized utensils or plates in the background. The image should evoke a sense of playful absurdity and culinary\nfantasy.\n8\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nFigure 4. Training dynamics of model architectures.", "start_char_idx": 26336, "end_char_idx": 30203, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06df1f32-e75d-4d2e-b0d2-d9aed2263ec6": {"__data__": {"id_": "06df1f32-e75d-4d2e-b0d2-d9aed2263ec6", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f277803-acb7-4984-b710-677e4dbbd209", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "daebe9ca6bba8f85c0d7890a50bced1676e5df6f0e05700e101fd918a76c0582", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fabc6f2b-20bf-4367-a65f-63e6abe5e6ff", "node_type": "1", "metadata": {}, "hash": "0d34d472866393a5ecfa6efd77720e7d5d68b674eabca74d6326503edda31592", "class_name": "RelatedNodeInfo"}}, "text": "smiling cartoon dog sits at a table, coffee mug on hand, as a room goes up in flames. \u201cThis is fine,\u201d the\ndog assures himself.\nA whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus. This imaginative creature features the distinctive, bulky body of a hippo, but with a texture and\nappearance resembling a golden-brown, crispy waffle. The creature might have elements like waffle squares across its skin and a syrup-like sheen. It\u2019s set in a surreal environment that playfully combines a\nnatural water habitat of a hippo with elements of a breakfast table setting, possibly including oversized utensils or plates in the background. The image should evoke a sense of playful absurdity and culinary\nfantasy.\n8\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nFigure 4. Training dynamics of model architectures. Compara-\ntive analysis of DiT,CrossDiT ,UViT , and MM-DiT on CC12M,\nfocusing on validation loss, CLIP score, and FID. Our proposed\nMM-DiT performs favorably across all metrics.\nshared set of weights for all modalities. Finally, we consider\nthe UViT (Hoogeboom et al., 2023) architecture as a hybrid\nbetween the widely used UNets and transformer variants.\nWe analyze the convergence behavior of these architectures\nin Figure 4: Vanilla DiT underperforms UViT. The cross-\nattention DiT variant CrossDiT achieves better performance\nthan UViT, although UViT seems to learn much faster ini-\ntially. Our MM-DiT variant significantly outperforms the\ncross-attention and vanilla variants. We observe only a small\ngain when using three parameter sets instead of two (at the\ncost of increased parameter count and VRAM usage), and\nthus opt for the former option for the remainder of this work.\n5.3. Training at Scale\nBefore scaling up, we filter and preencode our data to ensure\nsafe and efficient pretraining. Then, all previous consider-\nations of diffusion formulations, architectures, and data\nculminate in the last section, where we scale our models up\nto 8B parameters.\n5.3.1. D ATA PREPROCESSING\nPre-Training Mitigations Training data significantly im-\npacts a generative model\u2019s abilities. Consequently, data\nfiltering is effective at constraining undesirable capabili-\nties (Nichol, 2022). Before training at sale, we filter our\ndata for the following categories: (i) Sexual content: We\nuse NSFW-detection models to filter for explicit content.\n(ii) Aesthetics: We remove images for which our rating\nsystems predict a low score. (iii) Regurgitation: We use a\ncluster-based deduplication method to remove perceptual\nand semantic duplicates from the training data; see Ap-\npendix E.2.\nPrecomputing Image and Text Embeddings Our model\nuses the output of multiple pretrained, frozen networks as in-\nputs (autoencoder latents and text encoder representations).\nSince these outputs are constant during training, we precom-\npute them once for the entire dataset. We provide a detailed\ndiscussion of our approach in Appendix E.1.\nFigure 5. Effects of QK-normalization. Normalizing the Q- and\nK-embeddings before calculating the attention matrix prevents the\nattention-logit growth instability ( left), which causes the attention\nentropy to collapse ( right ) and has been previously reported in the\ndiscriminative ViT literature (Dehghani et al., 2023; Wortsman\net al., 2023). In contrast with these previous works, we observe\nthis instability in the last transformer blocks of our networks. Max-\nimum attention logits and attention entropies are shown averaged\nover the last 5 blocks of a 2B (d=24) model.\n5.3.2. F INETUNING ON HIGHRESOLUTIONS\nQK-Normalization In general, we pretrain all of our mod-\nels on low-resolution images of size 2562pixels. Next, we\nfinetune our models on higher resolutions with mixed as-\npect ratios (see next paragraph for details). We find that,\nwhen moving to high resolutions, mixed precision train-\ning can become unstable and the loss diverges. This can\nbe remedied by switching to full precision training \u2014 but\ncomes with a \u223c2\u00d7performance drop compared to mixed-\nprecision training. A more efficient alternative is reported\nin the (discriminative) ViT literature: Dehghani et al.", "start_char_idx": 29324, "end_char_idx": 33509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fabc6f2b-20bf-4367-a65f-63e6abe5e6ff": {"__data__": {"id_": "fabc6f2b-20bf-4367-a65f-63e6abe5e6ff", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06df1f32-e75d-4d2e-b0d2-d9aed2263ec6", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "cb57a3fe0713aa062e8fe1cc7ef8505d0ce58e3be189ff8caf33b8fe57d09edb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0cec7fa9-c358-476f-8495-f98cc293777e", "node_type": "1", "metadata": {}, "hash": "ac2ff0f54c7cc2a02a0865297efb9cbeb606295f263e4c559dd5d17819e20026", "class_name": "RelatedNodeInfo"}}, "text": "Max-\nimum attention logits and attention entropies are shown averaged\nover the last 5 blocks of a 2B (d=24) model.\n5.3.2. F INETUNING ON HIGHRESOLUTIONS\nQK-Normalization In general, we pretrain all of our mod-\nels on low-resolution images of size 2562pixels. Next, we\nfinetune our models on higher resolutions with mixed as-\npect ratios (see next paragraph for details). We find that,\nwhen moving to high resolutions, mixed precision train-\ning can become unstable and the loss diverges. This can\nbe remedied by switching to full precision training \u2014 but\ncomes with a \u223c2\u00d7performance drop compared to mixed-\nprecision training. A more efficient alternative is reported\nin the (discriminative) ViT literature: Dehghani et al. (2023)\nobserve that the training of large vision transformer models\ndiverges because the attention entropy grows uncontrollably.\nTo avoid this, Dehghani et al. (2023) propose to normalize\nQ and K before the attention operation. We follow this\napproach and use RMSNorm (Zhang & Sennrich, 2019)\nwith learnable scale in both streams of our MMDiT archi-\ntecture for our models, see Figure 2. As demonstrated in\nFigure 5, the additional normalization prevents the attention\nlogit growth instability, confirming findings by Dehghani\net al. (2023) and Wortsman et al. (2023) and enables efficient\ntraining at bf16-mixed (Chen et al., 2019) precision when\ncombined with \u03f5= 10\u221215in the AdamW (Loshchilov &\nHutter, 2017) optimizer. This technique can also be applied\non pretrained models that have not used qk-normalization\nduring pretraining: The model quickly adapts to the addi-\ntional normalization layers and trains more stably. Finally,\nwe would like to point out that although this method can\ngenerally help to stabilize the training of large models, it is\nnot a universal recipe and may need to be adapted depending\non the exact training setup.\nPositional Encodings for Varying Aspect Ratios After\ntraining on a fixed 256\u00d7256resolution we aim to (i) in-\ncrease the resolution and resolution and (ii) enable inference\nwith flexible aspect ratios. Since we use 2d positional fre-\n9\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nFigure 6. Timestep shifting at higher resolutions. Top right: Hu-\nman quality preference rating when applying the shifting based\non Equation (23). Bottom row: A5122model trained and sam-\npled withp\nm/n = 1.0(top) andp\nm/n = 3.0(bottom ). See\nSection 5.3.2.\nquency embeddings we have to adapt them based on the\nresolution. In the multi-aspect ratio setting, a direct inter-\npolation of the embeddings as in (Dosovitskiy et al., 2020)\nwould not reflect the side lengths correctly. Instead we use\na combination of extended and interpolated position grids\nwhich are subsequently frequency embedded.\nFor a target resolution of S2pixels, we use bucketed sam-\npling (NovelAI, 2022; Podell et al., 2023) such that that each\nbatch consists of images of a homogeneous size H\u00d7W,\nwhere H\u00b7W\u2248S2. For the maximum and minimum\ntraining aspect ratios, this results in the maximum values for\nwidth, Wmax, and height, Hmax, that will be encountered. Let\nhmax=Hmax/16, wmax=Wmax/16ands=S/16be the\ncorresponding sizes in latent space (a factor 8) after patching\n(a factor 2). Based on these values, we construct a vertical\nposition grid with the values ((p\u2212hmax\u2212s\n2)\u00b7256\nS)hmax\u22121\np=0 and\ncorrespondingly for the horizontal positions. We then center-\ncrop from the resulting positional 2d grid before embedding\nit.\nResolution-dependent shifting of timestep schedules In-\ntuitively, since higher resolutions have more pixels, we need\nmore noise to destroy their signal. Assume we are working\nin a resolution with n=H\u00b7Wpixels. Now, consider a\n\u201dconstant\u201d image, i.e.", "start_char_idx": 32786, "end_char_idx": 36500, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0cec7fa9-c358-476f-8495-f98cc293777e": {"__data__": {"id_": "0cec7fa9-c358-476f-8495-f98cc293777e", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fabc6f2b-20bf-4367-a65f-63e6abe5e6ff", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "abd19473266f7fd041d384abd699eab006be2d98bd5115be74f4b5eba0c06c4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d0e5eaf-c9ea-43a3-a15b-2ee649a044d5", "node_type": "1", "metadata": {}, "hash": "78551b1fb3d416f8cc07d0c692d8ceba4857682c3633a133c37cf217127a72ad", "class_name": "RelatedNodeInfo"}}, "text": "Let\nhmax=Hmax/16, wmax=Wmax/16ands=S/16be the\ncorresponding sizes in latent space (a factor 8) after patching\n(a factor 2). Based on these values, we construct a vertical\nposition grid with the values ((p\u2212hmax\u2212s\n2)\u00b7256\nS)hmax\u22121\np=0 and\ncorrespondingly for the horizontal positions. We then center-\ncrop from the resulting positional 2d grid before embedding\nit.\nResolution-dependent shifting of timestep schedules In-\ntuitively, since higher resolutions have more pixels, we need\nmore noise to destroy their signal. Assume we are working\nin a resolution with n=H\u00b7Wpixels. Now, consider a\n\u201dconstant\u201d image, i.e. one where every pixel has the value\nc. The forward process produces zt= (1\u2212t)c 1+t\u03f5,\nwhere both 1and\u03f5\u2208Rn. Thus, ztprovides nobservations\nof the random variable Y= (1\u2212t)c+t\u03b7withcand\u03b7\ninR, and \u03b7follows a standard normal distribution. Thus,\nE(Y) = (1 \u2212t)cand\u03c3(Y) =t. We can therefore recover\ncviac=1\n1\u2212tE(Y), and the error between cand its sam-\nple estimate \u02c6c=1\n1\u2212tPn\ni=1zt,ihas a standard deviation of\nFigure 7. Human Preference Evaluation against currrent\nclosed and open SOTA generative image models. Our 8B model\ncompares favorable against current state-of-the-art text-to-image\nmodels when evaluated on the parti-prompts (Yu et al., 2022)\nacross the categories visual quality ,prompt following andtypogra-\nphy generation .\n\u03c3(t, n) =t\n1\u2212tq\n1\nn(because the standard error of the mean\nforYhas deviationt\u221an). So if one already knows that the\nimage z0was constant across its pixels, \u03c3(t, n)represents\nthe degree of uncertainty about z0. For example, we imme-\ndiately see that doubling the width and height leads to half\nthe uncertainty at any given time 0< t < 1. But, we can\nnow map a timestep tnat resolution nto a timestep tmat\nresolution mthat results in the same degree of uncertainty\nvia the ansatz \u03c3(tn, n) =\u03c3(tm, m). Solving for tmgives\ntm=pm\nntn\n1 + (pm\nn\u22121)tn(23)\nWe visualize this shifting function in Figure 6. Note that the\nassumption of constant images is not realistic. To find good\nvalues for the shift value \u03b1:=pm\nnduring inference, we\napply them to the sampling steps of a model trained at reso-\nlution 1024\u00d71024 and run a human preference study. The\nresults in Figure 6 show a strong preference for samples with\nshifts greater than 1.5but less drastic differences among the\nhigher shift values. In our subsequent experiments, we thus\nuse a shift value of \u03b1= 3.0both during training and sam-\npling at resolution 1024\u00d71024 . A qualitative comparison\nbetween samples after 8k training steps with and without\nsuch a shift can be found in Figure 6. Finally, note that\nEquation 23 implies a log-SNR shift of logn\nmsimilar to\n(Hoogeboom et al., 2023):\n\u03bbtm= 2 log1\u2212tnpm\nntn(24)\n=\u03bbtn\u22122 log\u03b1=\u03bbtn\u2212logm\nn. (25)\n10\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nAfter the shifted training at resolution 1024\u00d71024 , we align\nthe model using Direct Preference Optimization (DPO) as\ndescribed in Appendix C.\n5.3.3. R ESULTS\nIn Figure 8, we examine the effect of training our MM-DiT\nat scale. For images, we conduct a large scaling study and\ntrain models with different numbers of parameters for 500k\nsteps on 2562pixels resolution using preencoded data, c.f.\nAppendix E.1, with a batch size of 4096. We train on 2\u00d72\npatches (Peebles & Xie, 2023), and report validation losses\non the CoCo dataset (Lin et al., 2014) every 50k steps.", "start_char_idx": 35890, "end_char_idx": 39258, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d0e5eaf-c9ea-43a3-a15b-2ee649a044d5": {"__data__": {"id_": "1d0e5eaf-c9ea-43a3-a15b-2ee649a044d5", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0cec7fa9-c358-476f-8495-f98cc293777e", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "7c59a724ea4d8f696dcdb39f04a039daef7d54412aad903d3df5235ee983d94d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a81bd2d8-339d-4124-b17e-96cfd43be695", "node_type": "1", "metadata": {}, "hash": "6a99aa917ededd61549d3fa3011036a4cbcef57edd766484b07ab05679b1276e", "class_name": "RelatedNodeInfo"}}, "text": "(25)\n10\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nAfter the shifted training at resolution 1024\u00d71024 , we align\nthe model using Direct Preference Optimization (DPO) as\ndescribed in Appendix C.\n5.3.3. R ESULTS\nIn Figure 8, we examine the effect of training our MM-DiT\nat scale. For images, we conduct a large scaling study and\ntrain models with different numbers of parameters for 500k\nsteps on 2562pixels resolution using preencoded data, c.f.\nAppendix E.1, with a batch size of 4096. We train on 2\u00d72\npatches (Peebles & Xie, 2023), and report validation losses\non the CoCo dataset (Lin et al., 2014) every 50k steps. In\nparticular, to reduce noise in the validation loss signal, we\nsample loss levels equidistant in t\u2208(0,1)and compute\nvalidation loss for each level separately. We then average\nthe loss across all but the last ( t= 1) levels.\nSimilarly, we conduct a preliminary scaling study of our\nMM-DiT on videos. To this end we start from the pretrained\nimage weights and additionally use a 2x temporal patching.\nWe follow Blattmann et al. (2023b) and feed data to the\npretrained model by collapsing the temporal into the batch\naxis. In each attention layer we rearrange the representation\nin the visual stream and add a full attention over all spatio-\ntemporal tokens after the spatial attention operation before\nthe final feedforward layer. Our video models are trained for\n140k steps with a batch size of 512 on videos comprising\n16 frames with 2562pixels. We report validation losses on\nthe Kinetics dataset (Carreira & Zisserman, 2018) every 5k\nsteps. Note that our reported FLOPs for video training in\nFigure 8 are only FLOPs from video training and do not\ninclude the FLOPs from image pretraining.\nFor both the image and video domains, we observe a smooth\ndecrease in the validation loss when increasing model size\nand training steps. We find the validation loss to be highly\ncorrelated to comprehensive evaluation metrics (Comp-\nBench (Huang et al., 2023), GenEval (Ghosh et al., 2023))\nand to human preference. These results support the valida-\ntion loss as a simple and general measure of model perfor-\nmance. Our results do not show saturation neither for image\nnot for video models.\nFigure 12 illustrates how training a larger model for longer\nimpacts sample quality. Tab. 5 shows the results of GenEval\nin full. When applying the methods presented in Sec-\ntion 5.3.2 and increasing training image resolution, our\nbiggest model excels in most categories and outperforms\nDALLE 3 (Betker et al., 2023), the current state of the art in\nprompt comprehension, in overall score.", "start_char_idx": 38612, "end_char_idx": 41231, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a81bd2d8-339d-4124-b17e-96cfd43be695": {"__data__": {"id_": "a81bd2d8-339d-4124-b17e-96cfd43be695", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d0e5eaf-c9ea-43a3-a15b-2ee649a044d5", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b8083d7ad52ee4ba55ea4e65ce4413e271a0851f4677a44dd75e7b6de6d57e10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80998fca-7d7f-4e33-bc97-02456a449d2c", "node_type": "1", "metadata": {}, "hash": "22a7c4ff94134ed7f1afe60154f5eb5a5a76b7be2c7516d49715ad444563ff82", "class_name": "RelatedNodeInfo"}}, "text": "We find the validation loss to be highly\ncorrelated to comprehensive evaluation metrics (Comp-\nBench (Huang et al., 2023), GenEval (Ghosh et al., 2023))\nand to human preference. These results support the valida-\ntion loss as a simple and general measure of model perfor-\nmance. Our results do not show saturation neither for image\nnot for video models.\nFigure 12 illustrates how training a larger model for longer\nimpacts sample quality. Tab. 5 shows the results of GenEval\nin full. When applying the methods presented in Sec-\ntion 5.3.2 and increasing training image resolution, our\nbiggest model excels in most categories and outperforms\nDALLE 3 (Betker et al., 2023), the current state of the art in\nprompt comprehension, in overall score.\nOurd= 38 model outperforms current proprietary (Betker\net al., 2023; ide, 2024) and open (Sauer et al., 2023; pla,\n2024; Chen et al., 2023; Pernias et al., 2023) SOTA gener-\native image models in human preference evaluation on theModel OverallObjects\nCounting Colors PositionColor\nSingle Two Attribution\nminDALL-E 0.23 0.73 0.11 0.12 0.37 0.02 0.01\nSD v1.5 0.43 0.97 0.38 0.35 0.76 0.04 0.06\nPixArt-alpha 0.48 0.98 0.50 0.44 0.80 0.08 0.07\nSD v2.1 0.50 0.98 0.51 0.44 0.85 0.07 0.17\nDALL-E 2 0.52 0.94 0.66 0.49 0.77 0.10 0.19\nSDXL 0.55 0.98 0.74 0.39 0.85 0.15 0.23\nSDXL Turbo 0.55 1.00 0.72 0.49 0.80 0.10 0.18\nIF-XL 0.61 0.97 0.74 0.66 0.81 0.13 0.35\nDALL-E 3 0.67 0.96 0.87 0.47 0.83 0.43 0.45\nOurs (depth=18), 51220.58 0.97 0.72 0.52 0.78 0.16 0.34\nOurs (depth=24), 51220.62 0.98 0.74 0.63 0.67 0.34 0.36\nOurs (depth=30), 51220.64 0.96 0.80 0.65 0.73 0.33 0.37\nOurs (depth=38), 51220.68 0.98 0.84 0.66 0.74 0.40 0.43\nOurs (depth=38), 5122w/DPO 0.71 0.98 0.89 0.73 0.83 0.34 0.47\nOurs (depth=38), 10242w/DPO 0.74 0.99 0.94 0.72 0.89 0.33 0.60\nTable 5. GenEval comparisons . Our largest model (depth=38)\noutperforms all current open models and DALLE-3 (Betker et al.,\n2023) on GenEval (Ghosh et al., 2023). We highlight the best,\nsecond best , and third best entries. For DPO, see Appendix C.\nrelative CLIP score decrease [%]\n5/50 steps 10/50 steps 20/50 steps path length\ndepth=15 4.30 0.86 0.21 191.13\ndepth=30 3.59 0.70 0.24 187.96\ndepth=38 2.71 0.14 0.08 185.96\nTable 6. Impact of model size on sampling efficiency. The table\nshows the relative performance decrease relative to CLIP scores\nevaluated using 50 sampling steps at a fixed seed.", "start_char_idx": 40489, "end_char_idx": 42879, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80998fca-7d7f-4e33-bc97-02456a449d2c": {"__data__": {"id_": "80998fca-7d7f-4e33-bc97-02456a449d2c", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a81bd2d8-339d-4124-b17e-96cfd43be695", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "e72bcab966d31bcb6c2beb9c292fbb3f5ec05cb72e261f93a4993f602dbebdc2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8071f620-4c3b-41e9-ae64-a5a953a57d3e", "node_type": "1", "metadata": {}, "hash": "ecf81db05ad0916d9d445cc43c77e9b9e977fd86c182f49c3eee30ecce747571", "class_name": "RelatedNodeInfo"}}, "text": "GenEval comparisons . Our largest model (depth=38)\noutperforms all current open models and DALLE-3 (Betker et al.,\n2023) on GenEval (Ghosh et al., 2023). We highlight the best,\nsecond best , and third best entries. For DPO, see Appendix C.\nrelative CLIP score decrease [%]\n5/50 steps 10/50 steps 20/50 steps path length\ndepth=15 4.30 0.86 0.21 191.13\ndepth=30 3.59 0.70 0.24 187.96\ndepth=38 2.71 0.14 0.08 185.96\nTable 6. Impact of model size on sampling efficiency. The table\nshows the relative performance decrease relative to CLIP scores\nevaluated using 50 sampling steps at a fixed seed. Larger models\ncan be sampled using fewer steps, which we attribute to increased\nrobustness and better fitting the straight-path objective of rectified\nflow models, resulting in shorter path lengths. Path length is\ncalculated by summing up \u2225v\u03b8\u00b7dt\u2225over 50 steps.\nParti-prompts benchmark (Yu et al., 2022) in the categories\nvisual aesthetics ,prompt following andtypography gener-\nation ,c.f. Figure 7. For evaluating human preference in\nthese categories, raters were shown pairwise outputs from\ntwo models, and asked to answer the following questions:\nPrompt following: Which image looks more representative\nto the text shown above and faithfully follows it?\nVisual aesthetics: Given the prompt, which image is of\nhigher-quality andaesthetically more pleasing ?\nTypography: Which image more accurately shows/displays\nthe text specified in the above description? More accurate\nspelling is preferred! Ignore other aspects.\nLastly, Table 6 highlights an intriguing result: not only do\nbigger models perform better, they also require fewer steps\nto reach their peak performance.\nFlexible Text Encoders While the main motivation for\nusing multiple text-encoders is boosting the overall model\nperformance (Balaji et al., 2022), we now show that this\nchoice additionally increases the flexibility of our MM-DiT -\nbased rectified flow during inference. As described in Ap-\npendix B.3 we train our model with three text encoders, with\nan individual drop-out rate of 46.3%. Hence, at inference\n11\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nFigure 8. Quantitative effects of scaling. We analyze the impact of model size on performance, maintaining consistent training\nhyperparameters throughout. An exception is depth=38, where learning rate adjustments at 3\u00d7105steps were necessary to prevent\ndivergence. (Top) Validation loss smoothly decreases as a function of both model size and training steps for both image (columns 1 and 2)\nand video models (columns 3 and 4). (Bottom) Validation loss is a strong predictor of overall model performance . There is a marked\ncorrelation between validation loss and holistic image evaluation metrics, including GenEval (Ghosh et al., 2023), column 1, human\npreference, column 2, and T2I-CompBench (Huang et al., 2023), column 3. For video models we observe a similar correlation between\nvalidation loss and human preference, column 4. .\nAll text-encoders w/o T5 (Raffel et al., 2019)\n\u201cA burger patty, with the bottom bun and lettuce and tomatoes. \u201dCOFFEE\u201d written on it in mustard\u201d\n\u201cA monkey holding a sign reading \u201dScaling transformer models is awesome!\u201d\n\u201cA mischievous ferret with a playful grin squeezes itself into a large glass jar, surrounded by\ncolorful candy. The jar sits on a wooden table in a cozy kitchen, and warm sunlight filters\nthrough a nearby window\u201d\nFigure 9. Impact of T5. We observe T5 to be important for\ncomplex prompts e.g. such involving a high degree of detail or\nlonger spelled text (rows 2 and 3). For most prompts, however, we\nfind that removing T5 at inference time still achieves competitive\nperformance.\ntime, we can use an arbitrary subset of all three text encoders.\nThis offers means for trading off model performance for im-\nproved memory efficiency, which is particularly relevant\nfor the 4.7B parameters of T5-XXL (Raffel et al., 2019)\nthat require significant amounts of VRAM.", "start_char_idx": 42288, "end_char_idx": 46249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8071f620-4c3b-41e9-ae64-a5a953a57d3e": {"__data__": {"id_": "8071f620-4c3b-41e9-ae64-a5a953a57d3e", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80998fca-7d7f-4e33-bc97-02456a449d2c", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "57eaf9c1586749287b735611f9d7cad4dad6e71b44c72ee4d60acb1acc9c61b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "704047b1-a36a-4da0-a97c-513c568cc5ca", "node_type": "1", "metadata": {}, "hash": "a9d5c43afa9c438ad7e6e34b66ad5e9805eb99477bdf478cd8ccc7a47678ac54", "class_name": "RelatedNodeInfo"}}, "text": "The jar sits on a wooden table in a cozy kitchen, and warm sunlight filters\nthrough a nearby window\u201d\nFigure 9. Impact of T5. We observe T5 to be important for\ncomplex prompts e.g. such involving a high degree of detail or\nlonger spelled text (rows 2 and 3). For most prompts, however, we\nfind that removing T5 at inference time still achieves competitive\nperformance.\ntime, we can use an arbitrary subset of all three text encoders.\nThis offers means for trading off model performance for im-\nproved memory efficiency, which is particularly relevant\nfor the 4.7B parameters of T5-XXL (Raffel et al., 2019)\nthat require significant amounts of VRAM. Interestingly, we\nobserve limited performance drops when using only the two\nCLIP-based text-encoders for the text prompts and replac-\ning the T5 embeddings by zeros. We provide a qualitative\nvisualization in Figure 9. Only for complex prompts involv-ing either highly detailed descriptions of a scene or larger\namounts of written text do we find significant performance\ngains when using all three text-encoders. These observa-\ntions are also verified in the human preference evaluation\nresults in Figure 7 ( Ours w/o T5 ). Removing T5 has no\neffect on aesthetic quality ratings ( 50% win rate), and only a\nsmall impact on prompt adherence ( 46% win rate), whereas\nits contribution to the capabilities of generating written text\nare more significant ( 38% win rate).\n6. Conclusion\nIn this work, we presented a scaling analysis of rectified\nflow models for text-to-image synthesis. We proposed a\nnovel timestep sampling for rectified flow training that im-\nproves over previous diffusion training formulations for\nlatent diffusion models and retains the favourable proper-\nties of rectified flows in the few-step sampling regime. We\nalso demonstrated the advantages of our transformer-based\nMM-DiT architecture that takes the multi-modal nature of\nthe text-to-image task into account. Finally, we performed\na scaling study of this combination up to a model size of\n8B parameters and 5\u00d71022training FLOPs. We showed\nthat validation loss improvements correlate with both exist-\ning text-to-image benchmarks as well as human preference\nevaluations. This, in combination with our improvements in\ngenerative modeling and scalable, multimodal architectures\nachieves performance that is competitive with state-of-the-\nart proprietary models. The scaling trend shows no signs of\nsaturation, which makes us optimistic that we can continue\nto improve the performance of our models in the future.\n12\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nBroader Impact\nThis paper presents work whose goal is to advance the field\nof machine learning in general and image synthesis in par-\nticular. There are many potential societal consequences\nof our work, none of which we feel must be specifically\nhighlighted here. For an extensive discussion of the gen-\neral ramifications of diffusion models, we point interested\nreaders towards (Po et al., 2023).\nReferences\nIdeogram v1.0 announcement, 2024. URL https://ab\nout.ideogram.ai/1.0 .\nPlayground v2.5 announcement, 2024. URL https://bl\nog.playgroundai.com/playground-v2-5/ .\nAlbergo, M. S. and Vanden-Eijnden, E. Building normaliz-\ning flows with stochastic interpolants, 2022.\nAtchison, J. and Shen, S. M. Logistic-normal distributions:\nSome properties and uses. Biometrika , 67(2):261\u2013272,\n1980.\nautofaiss. autofaiss, 2023. URL https://github.c\nom/criteo/autofaiss .\nBalaji, Y ., Nah, S., Huang, X., Vahdat, A., Song, J., Zhang,\nQ., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro,\nB., Karras, T., and Liu, M.-Y . ediff-i: Text-to-image\ndiffusion models with an ensemble of expert denoisers,\n2022.\nBetker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L.,\nOuyang, L., Zhuang, J., Lee, J., Guo, Y ., et al. Improving\nimage generation with better captions. Computer Science.\nhttps://cdn. openai.", "start_char_idx": 45602, "end_char_idx": 49510, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "704047b1-a36a-4da0-a97c-513c568cc5ca": {"__data__": {"id_": "704047b1-a36a-4da0-a97c-513c568cc5ca", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8071f620-4c3b-41e9-ae64-a5a953a57d3e", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "b0426196872a6ce0115342c5fa25e32d63264de6f3e0603aaeb0e72b2059f4eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8da4888-0c49-4b85-8b23-57e608cf4c58", "node_type": "1", "metadata": {}, "hash": "cad1347828a40b367e749708d34608e80f0b0fd375a23d04a80284d3e8d36799", "class_name": "RelatedNodeInfo"}}, "text": "autofaiss, 2023. URL https://github.c\nom/criteo/autofaiss .\nBalaji, Y ., Nah, S., Huang, X., Vahdat, A., Song, J., Zhang,\nQ., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro,\nB., Karras, T., and Liu, M.-Y . ediff-i: Text-to-image\ndiffusion models with an ensemble of expert denoisers,\n2022.\nBetker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L.,\nOuyang, L., Zhuang, J., Lee, J., Guo, Y ., et al. Improving\nimage generation with better captions. Computer Science.\nhttps://cdn. openai. com/papers/dall-e-3. pdf , 2(3), 2023.\nBlattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D.,\nKilian, M., Lorenz, D., Levi, Y ., English, Z., V oleti, V .,\nLetts, A., et al. Stable video diffusion: Scaling latent\nvideo diffusion models to large datasets. arXiv preprint\narXiv:2311.15127 , 2023a.\nBlattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim,\nS. W., Fidler, S., and Kreis, K. Align your latents: High-\nresolution video synthesis with latent diffusion models,\n2023b.\nBrooks, T., Holynski, A., and Efros, A. A. Instructpix2pix:\nLearning to follow image editing instructions. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pp. 18392\u201318402, 2023.\nCarlini, N., Hayes, J., Nasr, M., Jagielski, M., Sehwag,\nV ., Tramer, F., Balle, B., Ippolito, D., and Wallace, E.\nExtracting training data from diffusion models. In 32ndUSENIX Security Symposium (USENIX Security 23) , pp.\n5253\u20135270, 2023.\nCarreira, J. and Zisserman, A. Quo vadis, action recogni-\ntion? a new model and the kinetics dataset, 2018.\nChangpinyo, S., Sharma, P. K., Ding, N., and Soricut,\nR. Conceptual 12m: Pushing web-scale image-text pre-\ntraining to recognize long-tail visual concepts. 2021\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR) , pp. 3557\u20133567, 2021. URL\nhttps://api.semanticscholar.org/Corp\nusID:231951742 .\nChen, D., Chou, C., Xu, Y ., and Hseu, J. Bfloat16: The\nsecret to high performance on cloud tpus, 2019. URL\nhttps://cloud.google.com/blog/produc\nts/ai-machine-learning/bfloat16-the-s\necret-to-high-performance-on-cloud-t\npus?hl=en .\nChen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y ., Wang,\nZ., Kwok, J., Luo, P., Lu, H., and Li, Z. Pixart-a: Fast\ntraining of diffusion transformer for photorealistic text-\nto-image synthesis, 2023.\nChen, T. Q., Rubanova, Y ., Bettencourt, J., and Duvenaud,\nD. K. Neural ordinary differential equations. In Neural\nInformation Processing Systems , 2018. URL https:\n//api.semanticscholar.org/CorpusID:49\n310446 .\nCherti, M., Beaumont, R., Wightman, R., Wortsman, M.,\nIlharco, G., Gordon, C., Schuhmann, C., Schmidt, L.,\nand Jitsev, J. Reproducible scaling laws for contrastive\nlanguage-image learning. In 2023 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) .\nIEEE, 2023. doi: 10.1109/cvpr52729.2023.00276.", "start_char_idx": 49014, "end_char_idx": 51831, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8da4888-0c49-4b85-8b23-57e608cf4c58": {"__data__": {"id_": "e8da4888-0c49-4b85-8b23-57e608cf4c58", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "704047b1-a36a-4da0-a97c-513c568cc5ca", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "27b1a99349e83786ab54c0e3b3feca46ff82756fb84d4eaa915b40c6cf4a7296", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78b30729-344e-4468-b07a-ee14486603ca", "node_type": "1", "metadata": {}, "hash": "e8a3869f3f171c8b27fd9d2e8ad5d1f3a43c80859480e29017626684ee7ddaa5", "class_name": "RelatedNodeInfo"}}, "text": "Chen, T. Q., Rubanova, Y ., Bettencourt, J., and Duvenaud,\nD. K. Neural ordinary differential equations. In Neural\nInformation Processing Systems , 2018. URL https:\n//api.semanticscholar.org/CorpusID:49\n310446 .\nCherti, M., Beaumont, R., Wightman, R., Wortsman, M.,\nIlharco, G., Gordon, C., Schuhmann, C., Schmidt, L.,\nand Jitsev, J. Reproducible scaling laws for contrastive\nlanguage-image learning. In 2023 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) .\nIEEE, 2023. doi: 10.1109/cvpr52729.2023.00276. URL\nhttp://dx.doi.org/10.1109/CVPR52729.2\n023.00276 .\nDai, X., Hou, J., Ma, C.-Y ., Tsai, S., Wang, J., Wang, R.,\nZhang, P., Vandenhende, S., Wang, X., Dubey, A., Yu, M.,\nKadian, A., Radenovic, F., Mahajan, D., Li, K., Zhao, Y .,\nPetrovic, V ., Singh, M. K., Motwani, S., Wen, Y ., Song,\nY ., Sumbaly, R., Ramanathan, V ., He, Z., Vajda, P., and\nParikh, D. Emu: Enhancing image generation models\nusing photogenic needles in a haystack, 2023.\nDao, Q., Phung, H., Nguyen, B., and Tran, A. Flow match-\ning in latent space, 2023.\nDehghani, M., Djolonga, J., Mustafa, B., Padlewski, P.,\nHeek, J., Gilmer, J., Steiner, A., Caron, M., Geirhos, R.,\nAlabdulmohsin, I., Jenatton, R., Beyer, L., Tschannen,\nM., Arnab, A., Wang, X., Riquelme, C., Minderer, M.,\nPuigcerver, J., Evci, U., Kumar, M., van Steenkiste, S.,\n13\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nElsayed, G. F., Mahendran, A., Yu, F., Oliver, A., Huot,\nF., Bastings, J., Collier, M. P., Gritsenko, A., Birodkar,\nV ., Vasconcelos, C., Tay, Y ., Mensink, T., Kolesnikov,\nA., Paveti \u00b4c, F., Tran, D., Kipf, T., Lu \u02c7ci\u00b4c, M., Zhai, X.,\nKeysers, D., Harmsen, J., and Houlsby, N. Scaling vision\ntransformers to 22 billion parameters, 2023.\nDhariwal, P. and Nichol, A. Diffusion models beat gans on\nimage synthesis, 2021.\nDockhorn, T., Vahdat, A., and Kreis, K. Score-based gener-\native modeling with critically-damped langevin diffusion.\narXiv preprint arXiv:2112.07068 , 2021.\nDockhorn, T., Vahdat, A., and Kreis, K. Genie: Higher-\norder denoising diffusion solvers, 2022.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. ICLR ,\n2020.\nEsser, P., Chiu, J., Atighehchian, P., Granskog, J., and Ger-\nmanidis, A. Structure and content-guided video synthesis\nwith diffusion models, 2023.\nEuler, L. Institutionum calculi integralis . Number Bd. 1 in\nInstitutionum calculi integralis. imp. Acad. imp. Sa `ent.,\n1768. URL https://books.google.de/book\ns?id=Vg8OAAAAQAAJ .", "start_char_idx": 51304, "end_char_idx": 53948, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78b30729-344e-4468-b07a-ee14486603ca": {"__data__": {"id_": "78b30729-344e-4468-b07a-ee14486603ca", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8da4888-0c49-4b85-8b23-57e608cf4c58", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "fa046ea3da1039a388e2b2ea65fe9a07946373f9256e3c90a952b80431f6acee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4ac1324-33eb-4290-b37e-3ec45952c912", "node_type": "1", "metadata": {}, "hash": "eb0e253caef0e13a334d485dfe77bd6376dcd7d11650449eb3d13675e665eb40", "class_name": "RelatedNodeInfo"}}, "text": "An image is worth 16x16\nwords: Transformers for image recognition at scale. ICLR ,\n2020.\nEsser, P., Chiu, J., Atighehchian, P., Granskog, J., and Ger-\nmanidis, A. Structure and content-guided video synthesis\nwith diffusion models, 2023.\nEuler, L. Institutionum calculi integralis . Number Bd. 1 in\nInstitutionum calculi integralis. imp. Acad. imp. Sa `ent.,\n1768. URL https://books.google.de/book\ns?id=Vg8OAAAAQAAJ .\nFischer, J. S., Gui, M., Ma, P., Stracke, N., Baumann, S. A.,\nand Ommer, B. Boosting latent diffusion with flow match-\ning. arXiv preprint arXiv:2312.07360 , 2023.\nGhosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An\nobject-focused framework for evaluating text-to-image\nalignment. arXiv preprint arXiv:2310.11513 , 2023.\nGupta, A., Yu, L., Sohn, K., Gu, X., Hahn, M., Fei-Fei, L.,\nEssa, I., Jiang, L., and Lezama, J. Photorealistic video\ngeneration with diffusion models, 2023.\nHessel, J., Holtzman, A., Forbes, M., Le Bras, R., and\nChoi, Y . Clipscore: A reference-free evaluation metric for\nimage captioning. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing .\nAssociation for Computational Linguistics, 2021. doi:\n10.18653/v1/2021.emnlp-main.595. URL http://dx\n.doi.org/10.18653/v1/2021.emnlp-main.\n595.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium, 2017.\nHo, J. and Salimans, T. Classifier-free diffusion guidance,\n2022.Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models, 2020.\nHo, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko,\nA., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J.,\nand Salimans, T. Imagen video: High definition video\ngeneration with diffusion models, 2022.\nHoogeboom, E., Heek, J., and Salimans, T. Simple diffusion:\nEnd-to-end diffusion for high resolution images, 2023.\nHuang, K., Sun, K., Xie, E., Li, Z., and Liu, X. T2i-\ncompbench: A comprehensive benchmark for open-world\ncompositional text-to-image generation. arXiv preprint\narXiv:2307.06350 , 2023.\nHyv\u00a8arinen, A. Estimation of non-normalized statistical\nmodels by score matching. J. Mach. Learn. Res. , 6:695\u2013\n709, 2005. URL https://api.semanticschola\nr.org/CorpusID:1152227 .\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models,\n2020.\nKarras, T., Aittala, M., Aila, T., and Laine, S. Elucidating\nthe design space of diffusion-based generative models.\nArXiv , abs/2206.00364, 2022. URL https://api.se\nmanticscholar.org/CorpusID:249240415 .\nKarras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila,\nT., and Laine, S. Analyzing and improving the train-\ning dynamics of diffusion models. arXiv preprint\narXiv:2312.02696 , 2023.", "start_char_idx": 53532, "end_char_idx": 56363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4ac1324-33eb-4290-b37e-3ec45952c912": {"__data__": {"id_": "c4ac1324-33eb-4290-b37e-3ec45952c912", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78b30729-344e-4468-b07a-ee14486603ca", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "4b807d0d5a907eb81a749347b10b2adfcb0c222d6512568f67c747be3c2d7280", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f02a42c7-5a6f-4088-bff2-f8b2bd12ccb5", "node_type": "1", "metadata": {}, "hash": "a09400ab693bfe3d1b409e68d37c1a7ccf0903afaa42780a41e3d81469c17943", "class_name": "RelatedNodeInfo"}}, "text": "Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating\nthe design space of diffusion-based generative models.\nArXiv , abs/2206.00364, 2022. URL https://api.se\nmanticscholar.org/CorpusID:249240415 .\nKarras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila,\nT., and Laine, S. Analyzing and improving the train-\ning dynamics of diffusion models. arXiv preprint\narXiv:2312.02696 , 2023.\nKingma, D. P. and Gao, R. Understanding diffusion ob-\njectives as the elbo with simple data augmentation. In\nThirty-seventh Conference on Neural Information Pro-\ncessing Systems , 2023.\nLee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D.,\nCallison-Burch, C., and Carlini, N. Deduplicating train-\ning data makes language models better. arXiv preprint\narXiv:2107.06499 , 2021.\nLee, S., Kim, B., and Ye, J. C. Minimizing trajectory curva-\nture of ode-based generative models, 2023.\nLin, S., Liu, B., Li, J., and Yang, X. Common diffusion noise\nschedules and sample steps are flawed. In Proceedings\nof the IEEE/CVF Winter Conference on Applications of\nComputer Vision , pp. 5404\u20135411, 2024.\nLin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\nmanan, D., Doll \u00b4ar, P., and Zitnick, C. L. Microsoft COCO:\nCommon Objects in Context , pp. 740\u2013755. Springer In-\nternational Publishing, 2014. ISBN 9783319106021. doi:\n14\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\n10.1007/978-3-319-10602-1 48. URL http://dx.d\noi.org/10.1007/978-3-319-10602-1_48 .\nLipman, Y ., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and\nLe, M. Flow matching for generative modeling. In The\nEleventh International Conference on Learning Repre-\nsentations , 2023. URL https://openreview.net\n/forum?id=PqvMRDCJT9t .\nLiu, X., Gong, C., and Liu, Q. Flow straight and fast:\nLearning to generate and transfer data with rectified flow,\n2022.\nLiu, X., Zhang, X., Ma, J., Peng, J., and Liu, Q. Instaflow:\nOne step is enough for high-quality diffusion-based text-\nto-image generation, 2023.\nLoshchilov, I. and Hutter, F. Fixing weight decay regular-\nization in adam. ArXiv , abs/1711.05101, 2017. URL\nhttps://api.semanticscholar.org/Corp\nusID:3312944 .\nLu, C., Zhou, Y ., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-\nsolver++: Fast solver for guided sampling of diffusion\nprobabilistic models, 2023.\nMa, N., Goldstein, M., Albergo, M. S., Boffi, N. M., Vanden-\nEijnden, E., and Xie, S. Sit: Exploring flow and diffusion-\nbased generative models with scalable interpolant trans-\nformers, 2024.\nNichol, A. Dall-e 2 pre-training mitigations. https:\n//openai.com/research/dall-e-2-pre-t\nraining-mitigations , 2022.\nNichol, A. and Dhariwal, P. Improved denoising diffusion\nprobabilistic models, 2021.\nNovelAI. Novelai improvements on stable diffusion, 2022.\nURL https://blog.novelai.net/novelai\n-improvements-on-stable-diffusion-e10\nd38db82ac .\nPeebles, W. and Xie, S. Scalable diffusion models with\ntransformers. In 2023 IEEE/CVF International Con-\nference on Computer Vision (ICCV) .", "start_char_idx": 55974, "end_char_idx": 58936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f02a42c7-5a6f-4088-bff2-f8b2bd12ccb5": {"__data__": {"id_": "f02a42c7-5a6f-4088-bff2-f8b2bd12ccb5", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4ac1324-33eb-4290-b37e-3ec45952c912", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "7fa4601a5b34aec34e95b93fe9a3ec4add4c9d0b9100cd45cde1dc91d65198f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6cec9fc-118a-4b28-a706-fa7355bfdc54", "node_type": "1", "metadata": {}, "hash": "f1b84cdef90a39b3e248f3de4e1fb6419a35a3db869cc6d42eda414a1c8a8743", "class_name": "RelatedNodeInfo"}}, "text": "Nichol, A. Dall-e 2 pre-training mitigations. https:\n//openai.com/research/dall-e-2-pre-t\nraining-mitigations , 2022.\nNichol, A. and Dhariwal, P. Improved denoising diffusion\nprobabilistic models, 2021.\nNovelAI. Novelai improvements on stable diffusion, 2022.\nURL https://blog.novelai.net/novelai\n-improvements-on-stable-diffusion-e10\nd38db82ac .\nPeebles, W. and Xie, S. Scalable diffusion models with\ntransformers. In 2023 IEEE/CVF International Con-\nference on Computer Vision (ICCV) . IEEE, 2023. doi:\n10.1109/iccv51070.2023.00387. URL http://dx.d\noi.org/10.1109/ICCV51070.2023.00387 .\nPernias, P., Rampas, D., Richter, M. L., Pal, C. J., and\nAubreville, M. Wuerstchen: An efficient architecture for\nlarge-scale text-to-image diffusion models, 2023.\nPizzi, E., Roy, S. D., Ravindra, S. N., Goyal, P., and Douze,\nM. A self-supervised descriptor for image copy detection.\nInProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pp. 14532\u201314542,\n2022.Po, R., Yifan, W., Golyanik, V ., Aberman, K., Barron, J. T.,\nBermano, A. H., Chan, E. R., Dekel, T., Holynski, A.,\nKanazawa, A., et al. State of the art on diffusion models\nfor visual computing. arXiv preprint arXiv:2310.07204 ,\n2023.\nPodell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn,\nT., M \u00a8uller, J., Penna, J., and Rombach, R. Sdxl: Im-\nproving latent diffusion models for high-resolution image\nsynthesis, 2023.\nPooladian, A.-A., Ben-Hamu, H., Domingo-Enrich, C.,\nAmos, B., Lipman, Y ., and Chen, R. T. Q. Multisam-\nple flow matching: Straightening flows with minibatch\ncouplings, 2023.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\nJ., Krueger, G., and Sutskever, I. Learning transferable\nvisual models from natural language supervision, 2021.\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Man-\nning, C. D., and Finn, C. Direct Preference Optimiza-\ntion: Your Language Model is Secretly a Reward Model.\narXiv:2305.18290 , 2023.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer, 2019.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,\nM. Hierarchical text-conditional image generation with\nclip latents, 2022.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. In 2022 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) . IEEE,\n2022. doi: 10.1109/cvpr52688.2022.01042. URL\nhttp://dx.doi.org/10.1109/CVPR52688.2\n022.01042 .\nRonneberger, O., Fischer, P., and Brox, T. U-Net: Convolu-\ntional Networks for Biomedical Image Segmentation , pp.\n234\u2013241. Springer International Publishing, 2015.", "start_char_idx": 58449, "end_char_idx": 61249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6cec9fc-118a-4b28-a706-fa7355bfdc54": {"__data__": {"id_": "a6cec9fc-118a-4b28-a706-fa7355bfdc54", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f02a42c7-5a6f-4088-bff2-f8b2bd12ccb5", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "140165ba7d8fb2f89445d7901ad0634c570e6a46e1dc8807201b110bfaecfc4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39a40bea-67f5-48e9-8b8a-575e98e7563c", "node_type": "1", "metadata": {}, "hash": "9e8464b4061942a39e3f0ee04a00426d9c0cdf4a50962d617ee4f6ad42b8ec94", "class_name": "RelatedNodeInfo"}}, "text": "Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. In 2022 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) . IEEE,\n2022. doi: 10.1109/cvpr52688.2022.01042. URL\nhttp://dx.doi.org/10.1109/CVPR52688.2\n022.01042 .\nRonneberger, O., Fischer, P., and Brox, T. U-Net: Convolu-\ntional Networks for Biomedical Image Segmentation , pp.\n234\u2013241. Springer International Publishing, 2015. ISBN\n9783319245744. doi: 10.1007/978-3-319-24574-4 28.\nURL http://dx.doi.org/10.1007/978-3-3\n19-24574-4_28 .\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,\nMa, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\nM. S., Berg, A. C., and Fei-Fei, L. Imagenet large scale\nvisual recognition challenge. International Journal of\nComputer Vision , 115:211 \u2013 252, 2014. URL https:\n//api.semanticscholar.org/CorpusID:29\n30547 .\n15\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nSaharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans,\nT., Fleet, D., and Norouzi, M. Palette: Image-to-image\ndiffusion models. In ACM SIGGRAPH 2022 Conference\nProceedings , pp. 1\u201310, 2022a.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Den-\nton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi,\nS. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J.,\nand Norouzi, M. Photorealistic text-to-image diffusion\nmodels with deep language understanding, 2022b.\nSaharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D. J.,\nand Norouzi, M. Image super-resolution via iterative\nrefinement. IEEE Transactions on Pattern Analysis and\nMachine Intelligence , 45(4):4713\u20134726, 2022c.\nSauer, A., Chitta, K., M \u00a8uller, J., and Geiger, A. Projected\ngans converge faster. Advances in Neural Information\nProcessing Systems , 2021.\nSauer, A., Lorenz, D., Blattmann, A., and Rombach,\nR. Adversarial diffusion distillation. arXiv preprint\narXiv:2311.17042 , 2023.\nSheynin, S., Polyak, A., Singer, U., Kirstain, Y ., Zohar, A.,\nAshual, O., Parikh, D., and Taigman, Y . Emu edit: Precise\nimage editing via recognition and generation tasks. arXiv\npreprint arXiv:2311.10089 , 2023.\nSinger, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang,\nS., Hu, Q., Yang, H., Ashual, O., Gafni, O., Parikh, D.,\nGupta, S., and Taigman, Y . Make-a-video: Text-to-video\ngeneration without text-video data, 2022.\nSohl-Dickstein, J. N., Weiss, E. A., Maheswaranathan,\nN., and Ganguli, S. Deep unsupervised learning using\nnonequilibrium thermodynamics. ArXiv , abs/1503.03585,\n2015. URL https://api.semanticscholar.\norg/CorpusID:14888175 .\nSomepalli, G., Singla, V ., Goldblum, M., Geiping, J., and\nGoldstein, T. Diffusion art or digital forgery? investigat-\ning data replication in diffusion models.", "start_char_idx": 60777, "end_char_idx": 63527, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39a40bea-67f5-48e9-8b8a-575e98e7563c": {"__data__": {"id_": "39a40bea-67f5-48e9-8b8a-575e98e7563c", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6cec9fc-118a-4b28-a706-fa7355bfdc54", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "4f2947a68f35f603b593de26099569b20689c6973ffc68dc5b2eb5fbaab05768", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60ab30f6-200c-444a-98fc-3f616dc39add", "node_type": "1", "metadata": {}, "hash": "c1ea26162e52e83e912c898d9243f71ddcdd6e08b30beaa2f3809665554a5083", "class_name": "RelatedNodeInfo"}}, "text": "Make-a-video: Text-to-video\ngeneration without text-video data, 2022.\nSohl-Dickstein, J. N., Weiss, E. A., Maheswaranathan,\nN., and Ganguli, S. Deep unsupervised learning using\nnonequilibrium thermodynamics. ArXiv , abs/1503.03585,\n2015. URL https://api.semanticscholar.\norg/CorpusID:14888175 .\nSomepalli, G., Singla, V ., Goldblum, M., Geiping, J., and\nGoldstein, T. Diffusion art or digital forgery? investigat-\ning data replication in diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pp. 6048\u20136058, 2023a.\nSomepalli, G., Singla, V ., Goldblum, M., Geiping, J., and\nGoldstein, T. Understanding and mitigating copying\nin diffusion models. arXiv preprint arXiv:2305.20086 ,\n2023b.\nSong, J., Meng, C., and Ermon, S. Denoising diffusion\nimplicit models, 2022.\nSong, Y . and Ermon, S. Generative modeling by estimating\ngradients of the data distribution, 2020.Song, Y ., Sohl-Dickstein, J. N., Kingma, D. P., Kumar,\nA., Ermon, S., and Poole, B. Score-based generative\nmodeling through stochastic differential equations. ArXiv ,\nabs/2011.13456, 2020. URL https://api.semant\nicscholar.org/CorpusID:227209335 .\nTong, A., Malkin, N., Huguet, G., Zhang, Y ., Rector-Brooks,\nJ., Fatras, K., Wolf, G., and Bengio, Y . Improving and\ngeneralizing flow-based generative models with mini-\nbatch optimal transport, 2023.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need, 2017.\nVillani, C. Optimal transport: Old and new. 2008. URL\nhttps://api.semanticscholar.org/Corp\nusID:118347220 .\nVincent, P. A connection between score matching and de-\nnoising autoencoders. Neural Computation , 23:1661\u2013\n1674, 2011. URL https://api.semanticscho\nlar.org/CorpusID:5560643 .\nWallace, B., Dang, M., Rafailov, R., Zhou, L., Lou, A., Pu-\nrushwalkam, S., Ermon, S., Xiong, C., Joty, S., and Naik,\nN. Diffusion Model Alignment Using Direct Preference\nOptimization. arXiv:2311.12908 , 2023.\nWang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y ., Ji,\nJ., Yang, Z., Zhao, L., Song, X., et al. Cogvlm: Visual\nexpert for pretrained language models. arXiv preprint\narXiv:2311.03079 , 2023.\nWortsman, M., Liu, P. J., Xiao, L., Everett, K., Alemi, A.,\nAdlam, B., Co-Reyes, J. D., Gur, I., Kumar, A., Novak,\nR., Pennington, J., Sohl-dickstein, J., Xu, K., Lee, J.,\nGilmer, J., and Kornblith, S. Small-scale proxies for\nlarge-scale transformer training instabilities, 2023.\nYu, J., Xu, Y ., Koh, J. Y ., Luong, T., Baid, G., Wang, Z., Va-\nsudevan, V ., Ku, A., Yang, Y ., Ayan, B. K., et al. Scaling\nAutoregressive Models for Content-Rich Text-to-Image\nGeneration. arXiv:2206.10789 , 2022.\nZhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. Scaling\nvision transformers. In CVPR , pp. 12104\u201312113, 2022.", "start_char_idx": 63072, "end_char_idx": 65874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60ab30f6-200c-444a-98fc-3f616dc39add": {"__data__": {"id_": "60ab30f6-200c-444a-98fc-3f616dc39add", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39a40bea-67f5-48e9-8b8a-575e98e7563c", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "61df971fd15235671f988dc103cbca3bd9f16efa3d3de655ba9967b72e07bb1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "886891de-9050-4153-aafe-558e6c4c9775", "node_type": "1", "metadata": {}, "hash": "98b15e31c3e4606b6e735eb110cbcff620c11b6ed5e5bdb36e0e34a59d98c8d3", "class_name": "RelatedNodeInfo"}}, "text": "Yu, J., Xu, Y ., Koh, J. Y ., Luong, T., Baid, G., Wang, Z., Va-\nsudevan, V ., Ku, A., Yang, Y ., Ayan, B. K., et al. Scaling\nAutoregressive Models for Content-Rich Text-to-Image\nGeneration. arXiv:2206.10789 , 2022.\nZhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. Scaling\nvision transformers. In CVPR , pp. 12104\u201312113, 2022.\nZhang, B. and Sennrich, R. Root mean square layer normal-\nization, 2019.\n16\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nSupplementary\nA. Background\nDiffusion Models (Sohl-Dickstein et al., 2015; Song et al., 2020; Ho et al., 2020) generate data by approximating the\nreverse ODE to a stochastic forward process which transforms data to noise. They have become the standard approach for\ngenerative modeling of images (Dhariwal & Nichol, 2021; Ramesh et al., 2022; Saharia et al., 2022b; Rombach et al., 2022;\nBalaji et al., 2022) and videos (Singer et al., 2022; Ho et al., 2022; Esser et al., 2023; Blattmann et al., 2023b; Gupta et al.,\n2023). Since these models can be derived both via a variational lower bound on the negative likelihood (Sohl-Dickstein et al.,\n2015) and score matching (Hyv \u00a8arinen, 2005; Vincent, 2011; Song & Ermon, 2020), various formulations of forward- and\nreverse processes (Song et al., 2020; Dockhorn et al., 2021), model parameterizations (Ho et al., 2020; Ho & Salimans, 2022;\nKarras et al., 2022), loss weightings (Ho et al., 2020; Karras et al., 2022) and ODE solvers (Song et al., 2022; Lu et al., 2023;\nDockhorn et al., 2022) have led to a large number of different training objectives and sampling procedures. More recently,\nthe seminal works of Kingma & Gao (2023) and Karras et al. (2022) have proposed unified formulations and introduced\nnew theoretical and practical insights for training (Karras et al., 2022; Kingma & Gao, 2023) and inference (Karras et al.,\n2022). However, despite these improvements, the trajectories of common ODEs involve partly significant amounts of\ncurvature (Karras et al., 2022; Liu et al., 2022), which requires increased amounts of solver steps and, thus, renders fast\ninference difficult. To overcome this, we adopt rectified flow models whose formulation allows for learning straight ODE\ntrajectories.\nRectified Flow Models (Liu et al., 2022; Albergo & Vanden-Eijnden, 2022; Lipman et al., 2023) approach generative\nmodeling by constructing a transport map between two distributions through an ordinary differential equation (ODE). This\napproach has close connections to continuous normalizing flows (CNF) (Chen et al., 2018) as well as diffusion models.\nCompared to CNFs, Rectified Flows and Stochastic Interpolants have the advantage that they do not require simulation\nof the ODE during training. Compared to diffusion models, they can result in ODEs that are faster to simulate than the\nprobability flow ODE (Song et al., 2020) associated with diffusion models. Nevertheless, they do not result in optimal\ntransport solutions, and multiple works aim to minimize the trajectory curvature further (Lee et al., 2023; Tong et al., 2023;\nPooladian et al., 2023).", "start_char_idx": 65544, "end_char_idx": 68635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "886891de-9050-4153-aafe-558e6c4c9775": {"__data__": {"id_": "886891de-9050-4153-aafe-558e6c4c9775", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60ab30f6-200c-444a-98fc-3f616dc39add", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "7a0871bf792d728b39cc2e625ffb9f36d7a46cc9efdd3f1ba9b6bc890b42beaa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "faa4de4b-018e-40ef-9903-1bf60f3e1bc2", "node_type": "1", "metadata": {}, "hash": "9c3d6588b3dad09ccafce95032a6fd5c965f8e52731add55ace42e511bb66c64", "class_name": "RelatedNodeInfo"}}, "text": "This\napproach has close connections to continuous normalizing flows (CNF) (Chen et al., 2018) as well as diffusion models.\nCompared to CNFs, Rectified Flows and Stochastic Interpolants have the advantage that they do not require simulation\nof the ODE during training. Compared to diffusion models, they can result in ODEs that are faster to simulate than the\nprobability flow ODE (Song et al., 2020) associated with diffusion models. Nevertheless, they do not result in optimal\ntransport solutions, and multiple works aim to minimize the trajectory curvature further (Lee et al., 2023; Tong et al., 2023;\nPooladian et al., 2023). (Dao et al., 2023; Ma et al., 2024) demonstrate the feasibility of rectified flow formulations for\nclass-conditional image synthesis, (Fischer et al., 2023) for latent-space upsampling, and (Liu et al., 2023) apply the reflow\nprocedure of (Liu et al., 2022) to distill a pretrained text-to-image model (Rombach et al., 2022). Here, we are interested in\nrectified flows as the foundation for text-to-image synthesis with fewer sampling steps. We perform an extensive comparison\nbetween different formulations and loss weightings and propose a new timestep schedule for training of rectified flows with\nimproved performance.\nScaling Diffusion Models The transformer architecture (Vaswani et al., 2017) is well known for its scaling properties in\nNLP (Kaplan et al., 2020) and computer vision tasks (Dosovitskiy et al., 2020; Zhai et al., 2022). For diffusion models,\nU-Net architectures (Ronneberger et al., 2015) have been the dominant choice (Ho et al., 2020; Rombach et al., 2022; Balaji\net al., 2022). While some recent works explore diffusion transformer backbones (Peebles & Xie, 2023; Chen et al., 2023;\nMa et al., 2024), scaling laws for text-to-image diffusion models remain unexplored.\n17\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nDetailed pen and ink drawing of a happy pig butcher selling meat in its shop.\n a massive alien space ship that is shaped like a pretzel.\nA kangaroo holding a beer,\nwearing ski goggles and\npassionately singing silly\nsongs.\nAn entire universe inside a\nbottle sitting on the shelf at\nwalmart on sale.\nA cheesburger surfing the\nvibe wave at night\nA swamp ogre with a pearl\nearring by Johannes Vermeer\nA car made out of vegetables.\n heat death of the universe,\nline art\nA crab made of cheese on a plate\n Dystopia of thousand of workers picking cherries and feeding them into a machine that runs on steam\nand is as large as a skyscraper. Written on the side of the machine: \u201dSD3 Paper\u201d\ntranslucent pig, inside is a smaller pig.\n Film still of a long-legged cute big-eye anthropomorphic cheeseburger wearing sneakers relaxing on\nthe couch in a sparsely decorated living room.\n18\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\ndetailed pen and ink drawing of a massive complex alien space ship above a farm in the middle of\nnowhere.\nphoto of a bear wearing a suit and tophat in a river in the middle of a forest holding a sign that says \u201dI\ncant bear it\u201d.\ntilt shift aerial photo of a cute city made of sushi on a wooden table in the evening.\n dark high contrast render of a psychedelic tree of life illuminating dust in a mystical cave.\nan anthropomorphic fractal person behind the counter at a fractal themed restaurant.\n beautiful oil painting of a steamboat in a river in the afternoon. On the side of the river is a large brick\nbuilding with a sign on top that says \u00a8SD3\u00a8.\nan anthopomorphic pink donut with a mustache and cowboy hat standing by a log cabin in a forest\nwith an old 1970s orange truck in the driveway\nfox sitting in front of a computer in a messy room at night. On the screen is a 3d modeling program\nwith a line render of a zebra.\n19\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nB. On Flow Matching\nB.1.", "start_char_idx": 68006, "end_char_idx": 71872, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "faa4de4b-018e-40ef-9903-1bf60f3e1bc2": {"__data__": {"id_": "faa4de4b-018e-40ef-9903-1bf60f3e1bc2", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "886891de-9050-4153-aafe-558e6c4c9775", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "8874be322d969f7a34980861cdfbc06d356b86826d539bb2913d2e7882282188", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e873e7b1-a0f9-49ba-9e24-7a06cb02b613", "node_type": "1", "metadata": {}, "hash": "0e28095737d2eabac494ace29e3a941f85196dbf1c0414de973207145a1122de", "class_name": "RelatedNodeInfo"}}, "text": "tilt shift aerial photo of a cute city made of sushi on a wooden table in the evening.\n dark high contrast render of a psychedelic tree of life illuminating dust in a mystical cave.\nan anthropomorphic fractal person behind the counter at a fractal themed restaurant.\n beautiful oil painting of a steamboat in a river in the afternoon. On the side of the river is a large brick\nbuilding with a sign on top that says \u00a8SD3\u00a8.\nan anthopomorphic pink donut with a mustache and cowboy hat standing by a log cabin in a forest\nwith an old 1970s orange truck in the driveway\nfox sitting in front of a computer in a messy room at night. On the screen is a 3d modeling program\nwith a line render of a zebra.\n19\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nB. On Flow Matching\nB.1. Details on Simulation-Free Training of Flows\nFollowing (Lipman et al., 2023), to see that ut(z)generates pt, we note that the continuity equation provides a necessary\nand sufficient condition (Villani, 2008):\nd\ndtpt(x) +\u2207 \u00b7[pt(x)vt(x)] = 0 \u2194vtgenerates probability density path pt. (26)\nTherefore it suffices to show that\n\u2212\u2207 \u00b7 [ut(z)pt(z)] =\u2212\u2207 \u00b7 [E\u03f5\u223cN(0,I)ut(z|\u03f5)pt(z|\u03f5)\npt(z)pt(z)] (27)\n=E\u03f5\u223cN(0,I)\u2212 \u2207 \u00b7 [ut(z|\u03f5)pt(z|\u03f5)] (28)\n=E\u03f5\u223cN(0,I)d\ndtpt(z|\u03f5) =d\ndtpt(z), (29)\nwhere we used the continuity equation Equation (26) for ut(z|\u03f5)in line Equation (28) to Equation (29) since ut(z|\u03f5)\ngenerates pt(z|\u03f5)and the definition of Equation (6) in line Equation (27)\nThe equivalence of objectives LFM\u21cbLCFM (Lipman et al., 2023) follows from\nLFM(\u0398) = Et,pt(z)||v\u0398(z, t)\u2212ut(z)||2\n2 (30)\n=Et,pt(z)||v\u0398(z, t)||2\n2\u22122Et,pt(z)\u27e8v\u0398(z, t)|ut(z)\u27e9+c (31)\n=Et,pt(z)||v\u0398(z, t)||2\n2\u22122Et,pt(z|\u03f5),p(\u03f5)\u27e8v\u0398(z, t)|ut(z|\u03f5)\u27e9+c (32)\n=Et,pt(z|\u03f5),p(\u03f5)||v\u0398(z, t)\u2212ut(z|\u03f5)||2\n2+c\u2032=LCFM(\u0398) + c\u2032(33)\nwhere c, c\u2032do not depend on \u0398and line Equation (31) to line Equation (32) follows from:\nEpt(z|\u03f5),p(\u03f5)\u27e8v\u0398(z, t)|ut(z|\u03f5)\u27e9=Z\ndzZ\nd\u03f5pt(z|\u03f5)p(\u03f5)\u27e8v\u0398(z, t)|ut(z|\u03f5)\u27e9 (34)\n=Z\ndzpt(z)\u27e8v\u0398(z, t)|Z\nd\u03f5pt(z|\u03f5)\npt(z)p(\u03f5)ut(z|\u03f5)\u27e9 (35)\n=Z\ndzpt(z)\u27e8v\u0398(z, t)|ut(z)\u27e9=Ept(z)\u27e8v\u0398(z, t)|ut(z)\u27e9 (36)\nwhere we extended withpt(z)\npt(z)in line Equation (35) and used the definition of Equation (6) in line Equation (35) to\nEquation (36).\nB.2. Details on Image and Text Representations\nLatent Image Representation We follow LDM (Rombach et al., 2022) and use a pretrained autoencoder to represent RGB\nimages X\u2208RH\u00d7W\u00d73in a smaller latent space x=E(X)\u2208Rh\u00d7w\u00d7d. We use a spatial downsampling factor of 8, such\nthath=H\n8andw=W\n8, and experiment with different values for din Section 5.2.1.", "start_char_idx": 71077, "end_char_idx": 73583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e873e7b1-a0f9-49ba-9e24-7a06cb02b613": {"__data__": {"id_": "e873e7b1-a0f9-49ba-9e24-7a06cb02b613", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "faa4de4b-018e-40ef-9903-1bf60f3e1bc2", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "cfb45ea9fd3ce3f9ba8fea6b0a25c5605677673d1fa5142e6356a4a9a2d3d34b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f594f6e8-e041-4263-85de-82bce159084b", "node_type": "1", "metadata": {}, "hash": "56dd51ebde4950d44c3c190f8032a754fa5e9d9649b65505ca132cc13a1ae4df", "class_name": "RelatedNodeInfo"}}, "text": "B.2. Details on Image and Text Representations\nLatent Image Representation We follow LDM (Rombach et al., 2022) and use a pretrained autoencoder to represent RGB\nimages X\u2208RH\u00d7W\u00d73in a smaller latent space x=E(X)\u2208Rh\u00d7w\u00d7d. We use a spatial downsampling factor of 8, such\nthath=H\n8andw=W\n8, and experiment with different values for din Section 5.2.1. We always apply the forward process\nfrom Equation 2 in the latent space, and when sampling a representation xvia Equation 1, we decode it back into pixel\nspace X=D(x)via the decoder D. We follow Rombach et al. (2022) and normalize the latents by their mean and standard\ndeviation, which are globally computed over a subset of the training data. Figure 10 shows how generative model training\nfor different devolves as a function of model capacity, as discussed in Section 5.2.1.\nText Representation Similar to the encoding of images to latent representations, we also follow previous approaches\n(Saharia et al., 2022b; Balaji et al., 2022) and encode the text conditioning cusing pretrained, frozen text models. In\nparticular, for all experiments, we use a combination of CLIP (Radford et al., 2021) models and a encoder-decoder text model.\nSpecifically, we encode cwith the text encoders of both a CLIP L/14 model of Radford et al. (2021) as well as an OpenCLIP\nbigG/14 model of Cherti et al. (2023). We concatenate the pooled outputs, of sizes 768and1280 respectively, to obtain\na vector conditioning cvec\u2208R2048. We also concatenate the penultimate hidden representations channel-wise to a CLIP\n20\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nFigure 10. FID scores after training flow models with different sizes (parameterized via their depth) on the latent space of different\nautoencoders (4 latent channels, 8 channels and 16 channels) as discussed in Section 5.2.1. As expected, the flow model trained on the\n16-channel autoencoder space needs more model capacity to achieve similar performance. At depth d= 22 , the gap between 8-chn and\n16-chn becomes negligible. We opt for the 16-chn model as we ultimately aim to scale to much larger model sizes.\ncontext conditioning cCLIP\nctxt\u2208R77\u00d72048. Next, we encode calso to the final hidden representation, cT5\nctxt\u2208R77\u00d74096, of the\nencoder of a T5-v1.1-XXL model (Raffel et al., 2019). Finally, we zero-pad cCLIP\nctxtalong the channel axis to 4096 dimensions\nto match the T5 representation and concatenate it along the sequence axis with cT5\nctxtto obtain the final context representation\ncctxt\u2208R154\u00d74096. These two caption representations, cvecandcctxt, are used in two different ways as described in Section 4.\nB.3. Preliminaries for the Experiments in Section 5.1.\nDatasets We use two datasets to account for the missing of a standard text-to-image benchmark. As a widely used dataset,\nwe convert the ImageNet dataset (Russakovsky et al., 2014) into a dataset suitable for text-to-image models by adding\ncaptions of the form \u201ca photo of a \u2329class name \u232a\u201d to images, where \u2329class name \u232ais randomly chosen from one\nof the provided names for the image\u2019s class label. As a more realistic text-to-image dataset, we use the CC12M dataset\n(Changpinyo et al., 2021) for training.\nOptimization In this experiment, we train all models using a global batch size of 1024 using the AdamW optimizer\n(Loshchilov & Hutter, 2017) with a learning rate of 10\u22124and 1000 linear warmup steps. We use mixed-precision training\nand keep a copy of the model weights which gets updated every 100 training batches with an exponential moving average\n(EMA) using a decay factor of 0.99.", "start_char_idx": 73239, "end_char_idx": 76820, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f594f6e8-e041-4263-85de-82bce159084b": {"__data__": {"id_": "f594f6e8-e041-4263-85de-82bce159084b", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e873e7b1-a0f9-49ba-9e24-7a06cb02b613", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "ffd1a38db7b978664a8c530dfe5a8613528875deada4caeb99fedda504451ae0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59ba10f2-f256-4413-995b-5450da642bff", "node_type": "1", "metadata": {}, "hash": "7186e3eef8b37796640c51b4d7d6ff20c4449f40254a7a74fa5bb422833706b3", "class_name": "RelatedNodeInfo"}}, "text": "As a more realistic text-to-image dataset, we use the CC12M dataset\n(Changpinyo et al., 2021) for training.\nOptimization In this experiment, we train all models using a global batch size of 1024 using the AdamW optimizer\n(Loshchilov & Hutter, 2017) with a learning rate of 10\u22124and 1000 linear warmup steps. We use mixed-precision training\nand keep a copy of the model weights which gets updated every 100 training batches with an exponential moving average\n(EMA) using a decay factor of 0.99. For unconditional diffusion guidance (Ho & Salimans, 2022), we set the outputs of each\nof the three text encoders independently to zero with a probability of 46.4%, such that we roughly train an unconditional\nmodel in 10% of all steps.\nEvaluation As described in Section 5.1, we use CLIP scores, FID and validation losses to evaluate our models regularly\nduring training on the COCO-2014 validation split (Lin et al., 2014).\nAs the loss values differ widely in magnitude and variance for different timesteps, we evaluate them in a stratified way on\neight equally spaced values in the time interval [0,1].\nTo analyze how different approaches behave under different sampler settings, we produce 1000 samples for each of the\nsamplers which differ in guidance scales as well as number of sampling steps. We evaluate these samples with CLIP scores\nusing CLIP L/14 (Radford et al., 2021) and also compute FID between CLIP L/14 image features of these samples and the\nimages of the validation set. For sampling, we always use a Euler discretization (Euler, 1768) of Equation 1 and six different\nsettings: 50 steps with classifier-free-guidance scales 1.0, 2.5, 5.0, and 5, 10, 25 steps with classifier-free-guidance scale 5.0.\nB.4. Improving SNR Samplers for Rectified Flow Models\nAs described in Section 2, we introduce novel densities \u03c0(t)for the timesteps that we use to train our rectified flow models.\nFigure 11 visualizes the distributions of the logit-normal sampler and the mode sampler introduced in Section 3.1. Notably,\nas we demonstrate in Section 5.1, the logit-normal sampler outperforms the classic uniform rectified flow formulation (Liu\net al., 2022) and established diffusion baselines such as EDM (Karras et al., 2022) and LDM-Linear (Rombach et al., 2022).\n21\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nFigure 11. The mode (left) and logit-normal (right) distributions that we explore for biasing the sampling of training timesteps.\n\u201cA raccoon wearing formal clothes, wearing a\ntophat and holding a cane. The raccoon is\nholding a garbage bag. Oil painting in the style\nof abstract cubism.\u201d\u201cA bowl of soup that looks like a monster made\nout of plasticine\u201d\u201cTwo cups of coffee, one with latte art of a\nheart. The other has latte art of stars.\u201d\u201cA smiling sloth is wearing a leather jacket, a\ncowboy hat, a kilt and a bowtie. The sloth is\nholding a quarterstaff and a big book. The sloth\nis standing on grass a few feet in front of a\nshiny VW van with flowers painted on it.\nwide-angle lens from below.\u201d\nFigure 12. Qualitative effects of scaling. Displayed are examples demonstrating the impact of scaling training steps (left to right: 50k,\n200k, 350k, 500k) and model sizes (top to bottom: depth=15, 30, 38) on PartiPrompts, highlighting the influence of training duration and\nmodel complexity.\n22\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nC. Direct Preference Optimization\n\u201ca peaceful lakeside landscape with\nmigrating herd of sauropods\u201d\u201ca book with the words \u2018Don\u2019t Panic\u00a1,\nwritten on it\u201d2B base\n 2B w/ DPO\n 8b base\n 8b w/ DPO\nFigure 13. Comparison between base models and DPO-finetuned models. DPO-finetuning generally results in more aesthetically pleasing\nsamples with better spelling.", "start_char_idx": 76328, "end_char_idx": 80080, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59ba10f2-f256-4413-995b-5450da642bff": {"__data__": {"id_": "59ba10f2-f256-4413-995b-5450da642bff", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f594f6e8-e041-4263-85de-82bce159084b", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "1264b52bc3d0e9a44da2e9f06bc56c3580863fbaf7dff738537b9ba034a2f410", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "264c2ab9-95f1-4ad7-a464-05d748cb6e21", "node_type": "1", "metadata": {}, "hash": "c45dca033d39a1782b2c4035cadfdc27ca7b452845e08f0757b4dc35b6ed7fcd", "class_name": "RelatedNodeInfo"}}, "text": "wide-angle lens from below.\u201d\nFigure 12. Qualitative effects of scaling. Displayed are examples demonstrating the impact of scaling training steps (left to right: 50k,\n200k, 350k, 500k) and model sizes (top to bottom: depth=15, 30, 38) on PartiPrompts, highlighting the influence of training duration and\nmodel complexity.\n22\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nC. Direct Preference Optimization\n\u201ca peaceful lakeside landscape with\nmigrating herd of sauropods\u201d\u201ca book with the words \u2018Don\u2019t Panic\u00a1,\nwritten on it\u201d2B base\n 2B w/ DPO\n 8b base\n 8b w/ DPO\nFigure 13. Comparison between base models and DPO-finetuned models. DPO-finetuning generally results in more aesthetically pleasing\nsamples with better spelling.\nDirect Preference Optimization (DPO) (Rafailov et al., 2023) is a technique to finetune LLMs with preference data. Recently,\nthis method has been adapted to preference finetuning of text-to-image diffusion models (Wallace et al., 2023). In this\nsection, we verify that our model is also amenable to preference optimization. In particular, we apply the method introduced\nin Wallace et al. (2023) to our 2B and 8B parameter base model. Rather than finetuning the entire model, we introduce\nlearnable Low-Rank Adaptation (LoRA) matrices (of rank 128) for all linear layers as is common practice. We finetune\nthese new parameters for 4k and 2k iteration for the 2B and 8B base model, respectively. We then evaluate the resulting\nmodel in a human preference study using a subset of 128 captions from the Partiprompts set (Yu et al., 2022) (roughly three\nvoter per prompt and comparison). Figure 14 shows that our base models can be effectively tuned for human preference.\nFigure 13 shows samples of the respective base models and DPO-finetuned models.\nD. Finetuning for instruction-based image editing\nA common approach for training instruction based image editing and general image-to-image diffusion models is to\nconcatenate the latents of the input image to the noised latents of the diffusion target along the channel dimension before\nfeeding the input into a U-Net (Brooks et al., 2023; Sheynin et al., 2023; Saharia et al., 2022a;c). We follow the same\napproach, concatenating input and target along the channels before patching, and demonstrate that the same method is\napplicable to our proposed architecture. We finetune the 2B parameter base model on a dataset consisting of image-to-image\nediting tasks similar to the distribution of the InstructPix2Pix dataset (Brooks et al., 2023) as well as inpainting, segmentation,\ncolorization, deblurring and controlnet tasks similar to Emu Edit and Palette (Sheynin et al., 2023; Saharia et al., 2022a).\nAs shown in Fig 15 we observe that the resulting 2B Edit model has the capability to manipulate text in a given image, even\n23\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nPrompt Quality0102030405060Human Preference [ % ]depth=24 (2B)\nbase\nw/ DPO\nPrompt Qualitydepth=38 (8B)\nbase\nw/ DPO\nFigure 14. Human preference evaluation between base models and DPO-finetuned models. Human evaluators prefer DPO-finetuned\nmodels for both prompt following and general quality.\nModel Mem [GB] FP [ms] Storage [kB] Delta [%]\nV AE (Enc) 0.14 2.45 65.5 13.8\nCLIP-L 0.49 0.45 121.3 2.6\nCLIP-G 2.78 2.77 202.2 15.6\nT5 19.05 17.46 630.7 98.3\nTable 7. Key figures for preencoding frozen input networks. Mem is the memory required to load the model on the GPU. FP [ms] is\nthe time per sample for the forward pass with per-device batch size of 32. Storage is the size to save a single sample.", "start_char_idx": 79333, "end_char_idx": 82928, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "264c2ab9-95f1-4ad7-a464-05d748cb6e21": {"__data__": {"id_": "264c2ab9-95f1-4ad7-a464-05d748cb6e21", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59ba10f2-f256-4413-995b-5450da642bff", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "d8a2f4cbae3e99c00cdbdc349eaa1c9eda276b25679ea5e0b2069e6b4242cfed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62f0efd7-058f-4834-8c4d-9354cb911292", "node_type": "1", "metadata": {}, "hash": "468c4a47e24e748602161af56f3b1632fc8b531cb87ab05ee74a1f1bb6fe8aa0", "class_name": "RelatedNodeInfo"}}, "text": "Human preference evaluation between base models and DPO-finetuned models. Human evaluators prefer DPO-finetuned\nmodels for both prompt following and general quality.\nModel Mem [GB] FP [ms] Storage [kB] Delta [%]\nV AE (Enc) 0.14 2.45 65.5 13.8\nCLIP-L 0.49 0.45 121.3 2.6\nCLIP-G 2.78 2.77 202.2 15.6\nT5 19.05 17.46 630.7 98.3\nTable 7. Key figures for preencoding frozen input networks. Mem is the memory required to load the model on the GPU. FP [ms] is\nthe time per sample for the forward pass with per-device batch size of 32. Storage is the size to save a single sample. Delta [%] is how\nmuch longer a training step takes, when adding this into the loop for the 2B MMDiT-Model (568ms/it).\nthough no text manipulation tasks were included in the training data. We were not able to reproduce similar results when\ntraining a SDXL-based (Podell et al., 2023) editing model on the same data.\nE. Data Preprocessing for Large-Scale Text-to-Image Training\nE.1. Precomputing Image and Text Embeddings\nOur model uses the output of multiple pretrained, frozen networks as inputs (autoencoder latents and text encoder repre-\nsentations). Since these outputs are constant during training, we precompute them once for the entire dataset. This comes\nwith two main advantages: (i) The encoders do not need to be available on the GPU during training, lowering the required\nmemory. (ii) The forward encoding pass is skipped during training, saving time and total needed compute after the first\nepoch, see Tab. 7.\nThis approach has two disadvantages: First, random augmentation for each sample every epoch is not possible and we use\nsquare-center cropping during precomputation of image latents. For finetuning our model at higher resolutions, we specify\na number of aspect ratio buckets, and resize and crop to the closest bucket first and then precompute in that aspect ratio.\nSecond, the dense output of the text encoders is particularly large, creating additional storage cost and longer loading times\nduring training ( c.f. Tab. 7). We save the embeddings of the language models in half precision, as we do not observe a\ndeterioration in performance in practice.\nE.2. Preventing Image Memorization\nIn the context of generative image models memorization of training samples can lead to a number of issues (Somepalli et al.,\n2023a; Carlini et al., 2023; Somepalli et al., 2023b). To avoid verbatim copies of images by our trained models, we carefully\nscan our training dataset for duplicated examples and remove them.\n24\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nInput Output 1 Output 2\nWrite \u201dgo small\ngo home\u201d\ninstead\nGO BIG OR GO UNET\nis written on\nthe blackboard\nchange the\nword to\nUNOT\nmake the\nsign say\nMMDIT rules\nFigure 15. Zero Shot Text manipulation and insertion with the 2B Edit model\nDetails on Deduplication In accordance with the methods outlined by Carlini et al. (2023) and Somepalli et al. (2023a),\nwe opt for SSCD (Pizzi et al., 2022) as the backbone for the deduplication process. The SSCD algorithm is a state-of-the-art\ntechnique for detecting near-duplicate images at scale, and it generates high-quality image embeddings that can be used for\nclustering and other downstream tasks. We also decided to follow Nichol (2022) to decide on a number of clusters N. For\nour experiments, we use N= 16,000.\nWe utilize autofaiss (2023) for clustering. autofaiss (2023) is a library that simplifies the process of using Faiss (Facebook AI\nSimilarity Search) for large-scale clustering tasks. Specifically, leverage FAISS index factory1functionality to train a custom\nindex with predefined number of centroids. This approach allows for efficient and accurate clustering of high-dimensional\ndata, such as image embeddings.\nAlgorithm 1 details our deduplication approach. We ran an experiment to see how much data is removed by different SSCD\nthreshold as shown in Figure 16b.", "start_char_idx": 82357, "end_char_idx": 86256, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62f0efd7-058f-4834-8c4d-9354cb911292": {"__data__": {"id_": "62f0efd7-058f-4834-8c4d-9354cb911292", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "264c2ab9-95f1-4ad7-a464-05d748cb6e21", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "5fcc261b7d5b573c0cbf9b24ce6a7c77bd4b84e6ca3f8f443a90821c03d22ab4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c418a385-7d69-425d-84ff-b3b083c362b8", "node_type": "1", "metadata": {}, "hash": "8d2d5fb1de87965ed25002eb4cc76f0ce5216819e47301095b127c39fed5ea1b", "class_name": "RelatedNodeInfo"}}, "text": "We also decided to follow Nichol (2022) to decide on a number of clusters N. For\nour experiments, we use N= 16,000.\nWe utilize autofaiss (2023) for clustering. autofaiss (2023) is a library that simplifies the process of using Faiss (Facebook AI\nSimilarity Search) for large-scale clustering tasks. Specifically, leverage FAISS index factory1functionality to train a custom\nindex with predefined number of centroids. This approach allows for efficient and accurate clustering of high-dimensional\ndata, such as image embeddings.\nAlgorithm 1 details our deduplication approach. We ran an experiment to see how much data is removed by different SSCD\nthreshold as shown in Figure 16b. Based on these results we selected four thresholds for the final run Figure 16a.\n1https://github.com/facebookresearch/faiss/wiki/The-index-factory\n25\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nE.3. Assessing the Efficacy of our Deduplication Efforts\nCarlini et al. (2023) devise a two-stage data extraction attack that generates images using standard approaches, and flags\nthose that exceed certain membership inference scoring criteria. Carlini et al. (2023) bias their search towards duplicated\ntraining examples because these are orders of magnitude more likely to be memorized than non-duplicated examples\n(Somepalli et al., 2023a;a; Lee et al., 2021).\nTo assess how well our SSCD-based deduplication works, we follow Carlini et al. (2023) to extract memorized samples from\nsmall, specifically for this purpose trained models and compare them before and after deduplication. Two main step of the\nmentioned procedure include: 1) Generate many examples using the diffusion model in the standard sampling manner and\nwith the known prompts. 2) Perform membership inference to separate the model\u2019s novel generations from those generations\nwhich are memorized training examples. Algorithm 2 shows the steps to find the memorized samples based on Carlini et al.\n(2023). Note that we run this techniques two times; one for SD-2.1 model with only exact dedup removal as baseline, and\nfor a model with the SD2.1 architecture but trained on removed exact duplication and near-duplication using SSCD (Pizzi\net al., 2022).\nWe select the 350,000 most-duplicated examples from the training dataset based on SSCD (Pizzi et al., 2022) with threshold\nof 0.5, and generate 500 candidate images for each text prompt to increase the likelihood of finding memorization. The\nintuition is that for diffusion models, with high probability Gen(p;r1)\u2248dGen(p;r2)for two different random initial seeds\nr1,r2. On the other hand, if Gen(p;r1)\u2248dGen(p;r2)under some distance measure d, it is likely that these generated\nsamples are memorized examples. To compute the distance measure dbetween two images, we use a modified Euclidean\nl2distance. In particular, we found that many generations were often spuriously similar according to l2distance (e.g.,\nthey all had gray backgrounds). We therefore instead divide each image into 16 non-overlapping 128 \u00d7128 tiles and\nmeasure the maximum of the l2distance between any pair of image tiles between the two images. Figure 17 shows the\ncomparison between number of memorized samples, before and after using SSCD with the threshold of 0.5 to remove\nnear-duplicated samples. Carlini et al. (2023) mark images within clique size of 10 as memorized samples. Here we\nalso explore different sizes for cliques. For all clique thresholds, SSCD is able to significantly reduce the number of\nmemorized samples. Specifically, when the clique size is 10, trained SD models on the deduplicated training samples cut off\nat SSCD = 0.5show a 5\u00d7reduction in potentially memorized examples.", "start_char_idx": 85576, "end_char_idx": 89273, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c418a385-7d69-425d-84ff-b3b083c362b8": {"__data__": {"id_": "c418a385-7d69-425d-84ff-b3b083c362b8", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": "4", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "775dc617073cb8f243295451f55f498f3e4e426afdca11cf790b96df877027f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62f0efd7-058f-4834-8c4d-9354cb911292", "node_type": "1", "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "hash": "a6550c3908ce77fe4ab81391796f1ba3a143b27be96cd1608ffb507cb1cb1465", "class_name": "RelatedNodeInfo"}}, "text": "We therefore instead divide each image into 16 non-overlapping 128 \u00d7128 tiles and\nmeasure the maximum of the l2distance between any pair of image tiles between the two images. Figure 17 shows the\ncomparison between number of memorized samples, before and after using SSCD with the threshold of 0.5 to remove\nnear-duplicated samples. Carlini et al. (2023) mark images within clique size of 10 as memorized samples. Here we\nalso explore different sizes for cliques. For all clique thresholds, SSCD is able to significantly reduce the number of\nmemorized samples. Specifically, when the clique size is 10, trained SD models on the deduplicated training samples cut off\nat SSCD = 0.5show a 5\u00d7reduction in potentially memorized examples.\nAlgorithm 1 Finding Duplicate Items in a Cluster\nRequire: vecs \u2013 List of vectors in a single cluster, items \u2013 List of item IDs corresponding to vecs, index \u2013 FAISS index\nfor similarity search within the cluster, thresh \u2013 Threshold for determining duplicates\nOutput: dups \u2013 Set of duplicate item IDs\n1:dups\u2190new set ()\n2:fori\u21900tolength( vecs)\u22121do\n3: qs\u2190vecs[i]{Current vector }\n4: qid\u2190items [i]{Current item ID }\n5: lims, D, I\u2190index .range search( qs,thresh )\n6: ifqid\u2208dups then\n7: continue\n8: end if\n9: start \u2190lims[0]\n10: end\u2190lims[1]\n11: duplicate indices \u2190I[start :end]\n12: duplicate ids\u2190new list()\n13: forjinduplicate indices do\n14: ifitems [j]\u0338=qidthen\n15: duplicate ids.append( items [j])\n16: end if\n17: end for\n18: dups.update( duplicate ids)\n19:end for\n20:Return dups{Final set of duplicate IDs }\n26\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\n(a) Final result of SSCD deduplication over the entire dataset\n (b) Result of SSCD deduplication with various thresholds over 1000\nrandom clusters\nFigure 16. Results of deduplicating our training datasets for various filtering thresholds.\nAlgorithm 2 Detecting Memorization in Generated Images\nRequire: Set of prompts P, Number of generations per prompt N, Similarity threshold \u03f5= 0.15, Memorization threshold\nT\nEnsure: Detection of memorized images in generated samples\n1:Initialize Dto the set of most-duplicated examples\n2:foreach prompt p\u2208Pdo\n3: fori= 1toNdo\n4: Generate image Gen(p;ri)with random seed ri\n5: end for\n6:end for\n7:foreach pair of generated images xi, xjdo\n8: ifdistance d(xi, xj)< \u03f5then\n9: Connect xiandxjin graph G\n10: end if\n11:end for\n12:foreach node in Gdo\n13: Find largest clique containing the node\n14: ifsize of clique \u2265Tthen\n15: Mark images in the clique as memorized\n16: end if\n17:end for\n27\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nFigure 17. SSCD-based deduplication prevents memorization. To assess how well our SSCD-based deduplication works, we extract\nmemorized samples from small, specifically for this purpose trained models and compare them before and after deduplication. We plot a\ncomparison between number of memorized samples, before and after using SSCD with the threshold of 0.5 to remove near-duplicated\nsamples. Carlini et al. (2023) mark images within clique size of 10 as memorized samples. Here we also explore different sizes for cliques.\nFor all clique thresholds, SSCD is able to significantly reduce the number of memorized samples. Specifically, when the clique size is 10,\nmodels on the deduplicated training samples cut off at SSCD = 0.5show a 5\u00d7reduction in potentially memorized examples.\n28", "start_char_idx": 88541, "end_char_idx": 91930, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b00647ef-3325-460c-ba49-6f9721f9549b": {"__data__": {"id_": "b00647ef-3325-460c-ba49-6f9721f9549b", "embedding": null, "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The text provides detailed technical reports on the development of a highly capable language model called phi-3-mini, focusing on its architecture, training methodology, data optimization, post-training processes, and performance benchmarks compared to other models like GPT-3.5 and Mixtral. It also discusses the implementation of safety measures in accordance with Microsoft's responsible AI principles, including safety alignment in post-training, red-teaming, automated testing, and evaluations across various RAI harm categories. The text covers the challenges faced by language models like phi-3-mini and the strategies employed to mitigate issues such as factual inaccuracies, biases, inappropriate content generation, and safety concerns. Additionally, it delves into the development and evaluation of highly capable language models, the training of large language models, and the scaling of neural language models, along with topics like measuring how models mimic human falsehoods, open book question answering datasets, and the evaluation of foundation models.\n\nSome questions that this text can answer include:\n- Solve for x:(\u22121/3)(\u22124\u22123x)=1/2\n- Which body cavity contains the pituitary gland?\n- Where was the most famous site of the mystery cults in Greece?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c623a196-b15f-4618-831b-a4d7d42cbf5b": {"__data__": {"id_": "c623a196-b15f-4618-831b-a4d7d42cbf5b", "embedding": null, "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The text discusses the development and implementation of Infini-Transformers, a novel approach that enhances Transformer-based Large Language Models (LLMs) to efficiently process infinitely long inputs with bounded memory and computation. It introduces the concept of Infini-attention, a mechanism that combines local causal attention and long-term linear attention in a single Transformer block, enabling continual pre-training and fine-tuning for long-context adaptation. The text compares Infini-Transformers with other memory models and encoder-decoder models, highlighting its advantages in terms of memory complexity, compression ratio, and performance in tasks like book summarization. The text also explores various strategies for optimizing transformer models for long-context tasks, such as setting learning rates, using specific optimizers, implementing warmup and decay techniques, applying gradient checkpointing, and determining batch sizes.\n\nSome questions that this text can answer include:\n- How can Transformer LLMs be scaled to process infinitely long inputs with bounded memory and computation?\n- What is the role of the Infini-attention mechanism in enhancing the capabilities of Transformer models?\n- What are the benefits of using Infini-attention for long-context language modeling tasks compared to baseline models?\n- What experiments were conducted to evaluate the performance of Infini-Transformers in tasks like passkey retrieval and book summarization?\n- How does Infini-Transformers perform in terms of summarization compared to other models?\n- What are some efficient attention techniques mentioned in the text to improve the dot-product attention?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "600d818b-aa3e-465d-9e14-1aa896fd173d": {"__data__": {"id_": "600d818b-aa3e-465d-9e14-1aa896fd173d", "embedding": null, "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The text discusses novel approaches in transformer-based language models, specifically focusing on the concept of Mixture-of-Depths (MoD) for dynamically allocating compute resources. It explores how transformers can learn to distribute compute efficiently by making per-token decisions on compute allocation within the model. The text also delves into routing schemes that involve token-choice and expert-choice routing to optimize compute allocation and reduce computational costs while maintaining performance. Additionally, it highlights the advantages of MoD transformers over traditional models and their potential integration with Mixture-of-Experts (MoE) models to create MoDE models for improved performance. \n\nSome questions that this text can answer include:\n- How can transformers dynamically allocate compute resources within a model?\n- What is the significance of conditional computation in optimizing compute efficiency in language models?\n- How do MoD transformers differ from traditional transformer models and Mixture of Experts (MoE) transformers?\n- What are the benefits of using a static compute budget in transformer models?\n- How can routing schemes be utilized to optimize compute allocation in transformer-based language models?\n- What are the differences between token-choice and expert-choice routing in terms of load balancing and computational efficiency?\n- How do different routing schemes impact the downstream performance of transformer models?\n- What advantages do MoD transformers offer over isoFLOP-optimal baselines in terms of performance and computational efficiency?\n- How can MoD and MoE models be combined to create MoDE models, and what benefits does this integration provide?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57e6e1e6-8244-4476-a9d2-91bf029192ce": {"__data__": {"id_": "57e6e1e6-8244-4476-a9d2-91bf029192ce", "embedding": null, "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "attention_is_all_you_need", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The text discusses the Transformer model, a neural network architecture that relies on attention mechanisms for sequence transduction tasks like machine translation. It explains the components of the Transformer, training details, and results achieved in translation tasks. The text also covers the success of the Transformer model in various tasks, variations in its architecture, and the impact of attention mechanisms on performance. Some questions that this text can answer include the motivation behind the Transformer model, its key components, training techniques, achieved results in machine translation, success in different tasks, variations in architecture, and the benefits of using attention mechanisms in neural networks.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71b6e97e-ce67-4bbd-a1a6-04735127a41c": {"__data__": {"id_": "71b6e97e-ce67-4bbd-a1a6-04735127a41c", "embedding": null, "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The provided text discusses the development and evaluation of a novel approach for high-resolution image synthesis using rectified flow transformers. It introduces improvements in noise sampling techniques for training rectified flow models, particularly biased towards perceptually relevant scales. Additionally, a new transformer-based architecture for text-to-image generation is presented, allowing bidirectional flow of information between image and text tokens to enhance text comprehension and image synthesis quality. The text covers various aspects of scaling rectified flow transformers for high-resolution image synthesis, including different formulations, model architectures, training dynamics, data preprocessing, and validation processes. It also delves into advancements in text-to-image synthesis, image generation with better captions, and stable video diffusion using rectified flow transformers and diffusion models. The text addresses questions related to training normalizing flows for image synthesis, model performance evaluation metrics, the impact of noise sampling distributions, text-to-image model architectures, and improvements in scaling rectified flow transformers for high-resolution image synthesis.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone": {"node_ids": ["a302a713-ac42-4509-a9a2-f2bd388f1af8", "4a0cd810-3214-4b9e-ae98-b09055cfd986", "32379a21-97eb-4b9c-8ac0-7c21302a7315", "5874e3c8-e266-4293-8921-4d32ad651c11", "71ad8bf0-092e-40e2-a87f-87d6554e3432", "e3a2b94c-ecc3-40fd-aac9-17f6dff3a2fe", "5e03aabd-d0dd-4efa-bab5-94c9b8d518c1", "c2ab354c-d7f3-4668-83e7-455a01bdb1ea", "902caf10-2e3b-4105-895e-8d080ceb145c", "ae0591ff-ca0a-4c02-966e-017fe1fef75c", "f9606dfa-42bd-48f3-bb6a-771d9c74274f", "b00647ef-3325-460c-ba49-6f9721f9549b"], "metadata": {"file_path": "txt/phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_name": "phi_3_technical_report_a_highly_capable_language_model_locally_on_your_phone.txt", "file_type": "text/plain", "file_size": 27227, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}}, "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention": {"node_ids": ["f98839d3-cec4-4fe6-a6d9-04a5e7f25875", "34264931-bbb0-479c-9b81-e643fdeabce6", "088f9899-8dba-4ada-bee4-7c8fc4ccb19f", "fce8519d-3e34-4885-a7a6-c4b2514bab06", "0f0476ca-26bc-4dc0-ac95-b80d959e0b60", "f3f9f3fb-b8e0-471b-9430-886ad9ddfa16", "a2d44ff4-3e3c-4523-9f9d-fe6a3d692572", "3a96d7f4-1a22-4c2f-9cea-7b46be1a7674", "e5f96973-ca2b-4ace-b207-ee5e6c7b4a9d", "ea090aea-2a3c-408b-8190-945e25e739c6", "5250accc-3425-42a3-906f-902217ca1440", "55e2c282-a495-4863-8042-234f76ab1f2e", "cd94529f-38e1-4590-b98c-29b46afc1e3e", "cb44d17a-a68d-40e0-8ef7-d555bea3b23c", "c623a196-b15f-4618-831b-a4d7d42cbf5b"], "metadata": {"file_path": "txt/leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_name": "leave_no_context_behind_efficient_infinite_context_transformers_with_infini_attention.txt", "file_type": "text/plain", "file_size": 40147, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}}, "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models": {"node_ids": ["313cf5ae-84a6-4bb7-8779-e640ba4f8bde", "ab935868-42a1-451c-aae5-ea359f68442b", "f9daad84-c0c2-43c7-a8ec-a9bcb4d18198", "68df46e3-3932-4d86-b538-cf3fa32c80b1", "abe64ea2-e9c7-4afe-89a7-0224c679db57", "d247bf45-ed8f-4e83-98d4-8f5cbb4af2b6", "8f3aff55-9de9-4d75-9a37-2df328337463", "775b688a-1f18-4cc4-bb47-acc5492557e2", "cc093955-24e9-48e7-91bd-134fc32fdf10", "bdf9cd3e-f264-477b-8b6d-722054c50b74", "e3af10ba-05e8-4289-b39f-9c0b5a85bafa", "71b3447b-d1c9-4ebe-bda1-5927740a15e2", "62116594-a9ba-407a-bfb9-6339b47cddc3", "600d818b-aa3e-465d-9e14-1aa896fd173d"], "metadata": {"file_path": "txt/mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_name": "mixture_of_depths_dynamically_allocating_compute_in_transformer_based_language_models.txt", "file_type": "text/plain", "file_size": 42388, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}}, "attention_is_all_you_need": {"node_ids": ["17656648-675c-4e4c-9f74-f8a1898a0e13", "46161918-fde5-404b-a93f-cd7f392f1a8f", "591ef5c2-b978-4a27-91a1-c337d874dd70", "76890a72-4f06-4667-acc0-76843cc527a1", "f24a72f4-a72a-45c6-9a50-8a8d05513a0b", "c5caa6f1-858b-4b0e-9d43-50307d4c33a0", "3d82c73e-304a-4f3a-80c2-3e64487ad704", "2b5a090b-5488-4693-b018-fc85db5e5597", "a6ae20ed-1773-4a80-bc6e-bcc837a7ea87", "6fe8244d-d01a-4367-bef8-6bf6b29496ee", "0296c0e0-2a66-4aed-986c-a3fa62c6a5fd", "437d6809-94e8-4638-82ec-dadd0ac2e706", "010b42cb-6cde-4e2e-b2c1-87a26bfd755d", "57e6e1e6-8244-4476-a9d2-91bf029192ce"], "metadata": {"file_path": "txt/attention_is_all_you_need.txt", "file_name": "attention_is_all_you_need.txt", "file_type": "text/plain", "file_size": 39641, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}}, "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis": {"node_ids": ["5ca433ae-99b9-4a01-9613-f422ed83191c", "45092251-c78b-4dfb-9f4a-4aea28b74238", "abfc959e-5184-4b04-acdb-d1672f33d50a", "07734fec-4445-4ee1-a66a-55c466de346a", "d0111e1a-f9e8-4076-8946-f00e4ccf83ab", "09a30549-613c-4f86-96a2-0e62e5467107", "9e994813-dccd-4900-a4a1-62866edcb020", "7c8b2255-e058-4df9-92e0-dad2ddea4bb9", "fc800e59-e0fb-4a83-a51c-395bc166a0e1", "1fe02bce-2d9b-4b42-9962-5b96a155f989", "9f277803-acb7-4984-b710-677e4dbbd209", "06df1f32-e75d-4d2e-b0d2-d9aed2263ec6", "fabc6f2b-20bf-4367-a65f-63e6abe5e6ff", "0cec7fa9-c358-476f-8495-f98cc293777e", "1d0e5eaf-c9ea-43a3-a15b-2ee649a044d5", "a81bd2d8-339d-4124-b17e-96cfd43be695", "80998fca-7d7f-4e33-bc97-02456a449d2c", "8071f620-4c3b-41e9-ae64-a5a953a57d3e", "704047b1-a36a-4da0-a97c-513c568cc5ca", "e8da4888-0c49-4b85-8b23-57e608cf4c58", "78b30729-344e-4468-b07a-ee14486603ca", "c4ac1324-33eb-4290-b37e-3ec45952c912", "f02a42c7-5a6f-4088-bff2-f8b2bd12ccb5", "a6cec9fc-118a-4b28-a706-fa7355bfdc54", "39a40bea-67f5-48e9-8b8a-575e98e7563c", "60ab30f6-200c-444a-98fc-3f616dc39add", "886891de-9050-4153-aafe-558e6c4c9775", "faa4de4b-018e-40ef-9903-1bf60f3e1bc2", "e873e7b1-a0f9-49ba-9e24-7a06cb02b613", "f594f6e8-e041-4263-85de-82bce159084b", "59ba10f2-f256-4413-995b-5450da642bff", "264c2ab9-95f1-4ad7-a464-05d748cb6e21", "62f0efd7-058f-4834-8c4d-9354cb911292", "c418a385-7d69-425d-84ff-b3b083c362b8", "71b6e97e-ce67-4bbd-a1a6-04735127a41c"], "metadata": {"file_path": "txt/scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_name": "scaling_rectified_flow_transformers_for_high_resolution_image_synthesis.txt", "file_type": "text/plain", "file_size": 92711, "creation_date": "2024-04-24", "last_modified_date": "2024-04-24"}}}}